<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>未来五年预测</title>
      <link href="/2020/08/30/%E6%9C%AA%E6%9D%A5%E4%BA%94%E5%B9%B4%E9%A2%84%E6%B5%8B/"/>
      <url>/2020/08/30/%E6%9C%AA%E6%9D%A5%E4%BA%94%E5%B9%B4%E9%A2%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>30岁, 人生换了一个方向, 预测简单一下未来5年, 这几家公司的未来. 5年后回头再看, 万一能猜对80%的结局, 5年就去创业去.</p><h2 id="预测的三个原则"><a href="#预测的三个原则" class="headerlink" title="预测的三个原则"></a>预测的三个原则</h2><ol><li>垄断才能创造利润.</li><li>每隔10年, 大公司必须在一个万亿美元市场完成局部垄断.</li><li>快销领域变化过快</li></ol><h3 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h3><p>十分看好阿里, 因为在电商领域造成事实上的垄断, 这是他的基本盘.  另外蚂蚁占住了金融这个万亿市场, 撑住了阿里的市值.  这两个是阿里现有的, 未来主要点是阿里云, 这个也是个万亿市场的, 如果一直能保持住国内第一名, 阿里的市值还能涨一半. 另外非常看好菜鸟网络, 他是一个全球的物流体系, 是商业交易的中枢, 目前刚起步, 未来空间很大, 但至少要在2030年左右才能看到成果了. 新零售的模式, 不太看好, 可能也是不懂啊, 零售最重要的是营销和库存管理, 好像没听到阿里对这两方面有什么建树吧.</p><h2 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a>腾讯</h2><p>不看好腾讯未来, 虽然支撑其市值的是社交和游戏,  社交的垄断是不可替代的, 但是游戏还是属于快销领域, 成功与失败都是一眨眼的事情. 而且腾讯一直没有找到未来10年的目标, 腾讯云也不争气. 我有一个大胆的猜测, 腾讯可能会转化为投资公司, 未来致力于投资其他公司, 它来整合流量, 实现共赢. 但投行的想象空间是没有互联网公司大的, 因为投资很难形成垄断, 资本只在乎赚钱.</p><h2 id="百度"><a href="#百度" class="headerlink" title="百度"></a>百度</h2><p>从移动互联网就掉队了, 这个应该不是百度错失移动互联网, 而且移动互联网不太需要搜索, 而是需要内容提供商. 另外”All In AI”的口号也是想找到另外一个万亿市场, 选择了自动驾驶, 但是这个投资周期太长, 到能够实现自动驾驶盈利, 感觉也要2030年了. 不确定百度能不能活到那个时候.</p><h2 id="华为"><a href="#华为" class="headerlink" title="华为"></a>华为</h2><p>华为找到了通信和手机两个万亿市场, 体量应该和腾讯类似. 目前在投入云计算和汽车市场, 但华为能做的汽车市场只有千亿市场, 盘子不够大, 很难靠这个点度过2030年, 所以还是得靠云计算, 云计算的想象空间比较大. 所以最后华为云肯定要和阿里云生死PK, 就像小米和华为手机一下, 一个倒了另外一个就起来了. 但挺怀疑华为云掌舵者的能力,  主要是在这个多风雨的时间段, 能不能争取到各个产品线老大持续给云投入, 这个还是未知的. 华为云今年3岁, 2022年底可知到底能不能撑住华为未来10年. 另外对于芯片的问题, 华为应该还是只是会投入到设计领域, 生产领域不那么赚钱, 很难再回去了. 因此海思未来的定位, 可能还是全球领先的芯片设计公司, 撑不住未来, 只能依附于其他产品线, 例如华为云,消费者终端等.</p><h2 id="字节跳动"><a href="#字节跳动" class="headerlink" title="字节跳动"></a>字节跳动</h2><p>比较不太看好字节跳动, 虽然不知道为什么他的估值那么高. 无论头条还是抖音, 都属于快销产品, 这类产品的特点就是流量大, 但是最大的问题用户的忠诚度不高, 形成不了事实的垄断, 被别人替换的概率比较高. 本来字节也是踩着这些产品上去的. 另外娱乐的变化太快了,  这5年从直播然后火到了短视频, 未来风向变了, 就看字节能不能跟上了.  此外目前字节一直在发力其他的各个领域, 基于找寻下一个热点, 这也符合他们公司的定位. 主要还是看未来的风口能不能被他抓住. 当然战线太长很容易出问题, 不过他们没有上市是个好事, 张一鸣的决策可以不受太大影响.</p><h2 id="快手"><a href="#快手" class="headerlink" title="快手"></a>快手</h2><p>同字节,  而且只做短视频, 未来空间非常小, 虽然现在傲娇的很</p><h2 id="京东"><a href="#京东" class="headerlink" title="京东"></a>京东</h2><p>反正就那样了, 没啥未来了, 一直呆在舒适区不出来了.  </p><h2 id="拼多多"><a href="#拼多多" class="headerlink" title="拼多多"></a>拼多多</h2><p>看好拼多多的下沉市场, 但不是特别看好高端市场, 本来拼多多就是为了盘活农村市场才有了未来, 在高端产品上跟淘宝和京东还是狠点难的, 品牌本身也在那儿. </p><p>所以我觉得拼多多未来可能会和京东差不多的一个体量, 或者再大一个1.5倍左右, 因为农村包围城市也是可能的. 但要想未来取代阿里的地位, 感觉很难. 因为这个市场已经完全饱和了, 容不下那么多企业了. </p><p>但实际上下层市场的空间也很大, 例如粮食交易, 农贸品交易, 化肥交易, 这些也都是一个巨大的市场. 但是看着拼多多好像没往这方向走, 不太了解.</p><h2 id="美团"><a href="#美团" class="headerlink" title="美团"></a>美团</h2><p>比较不看好, 和头条差不多的行业, 另外o2o的战场还没有完全结束, 阿里还跟它打, 现在正在寻找另外一个市场, 战线也拉的比较长. 但在团购领域已经没有竞争者, 所以至少有一个基本盘.  所以我觉得他会不好不坏的活着, 但是5年后不太会有什么起色</p><h2 id="滴滴"><a href="#滴滴" class="headerlink" title="滴滴"></a>滴滴</h2><p>在出行方面, 滴滴已经垄断了, 但受制于政府的安全问题,  变现能力变差了. 目前也到了公司的瓶颈期, 如果想要突破的话, 去做货运行业是非常好的一个选择, 如果自己能做成, 那是个万亿市场. 如果想百度一下做技术支持, 那也是个千亿市场, 可以让他滋润的活到2025年了. 另外自动驾驶也是它发力的方向, 但2030年前就别想了.</p><p>但是我感觉去做外卖那种事情, 完全是为了恶心一些美团. 电单车/租车之类的市场比较小, 感觉盈利困难. </p><p>打车的下沉市场也不多, 但是比PDD更不好做, 因为假货一般是民事纠纷, 但黑车确实能出形式案件.</p><p>总是货运市场做成了, 滴滴未来会挺好的, 不做成就另外一个百度了.</p>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]MPI-Operator介绍</title>
      <link href="/2019/09/22/Kubeflow%E7%B3%BB%E5%88%97-MPI-Operator%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/22/Kubeflow%E7%B3%BB%E5%88%97-MPI-Operator%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>系统里面默认是没有安装<code>mpi-operator</code>, 因此需要自行安装.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubeflow/mpi-operator/blob/master/deploy/mpi-operator.yaml</span><br><span class="line"><span class="comment"># 修改镜像地址</span></span><br><span class="line">kubectl create -f mpi-operator.yaml</span><br></pre></td></tr></table></figure></p><p>在国内永远有镜像替换的烦恼, 在mpi-operator之中需要自行替换这两个镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpioperator/mpi-operator:latest</span><br><span class="line">mpioperator/kubectl-delivery:latest</span><br></pre></td></tr></table></figure><p>你可以直接在<a href="https://hub.docker.com/u/mpioperator" target="_blank" rel="noopener">DockerHub</a>上找到这两个镜像或者自行编译: <a href="https://github.com/kubeflow/mpi-operator/blob/master/Dockerfile" target="_blank" rel="noopener">operator地址</a> <a href="https://github.com/kubeflow/mpi-operator/blob/master/cmd/kubectl-delivery/Dockerfile" target="_blank" rel="noopener">delivery地址</a></p><p>根据以下命令查看是否安装成功(crd已经创建 &amp;&amp; pod为running)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get crd|grep mpi</span></span><br><span class="line">mpijobs.kubeflow.org                   2019-09-18T07:44:48Z</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pod -n mpi-operator|grep mpi</span></span><br><span class="line">mpi-operator-584466c4f6-frw4x          1/1       Running     1          3d</span><br></pre></td></tr></table></figure><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>根据官网的介绍创建<code>tensorflow-benchmarks</code>的例子.</p><p> <a href="https://github.com/kubeflow/mpi-operator/blob/master/examples/v1alpha2/tensorflow-benchmarks.yaml" target="_blank" rel="noopener">文件地址</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeflow.org/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MPIJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tensorflow-benchmarks</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  slotsPerWorker:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  cleanPodPolicy:</span> <span class="string">Running</span></span><br><span class="line"><span class="attr">  mpiReplicaSpecs:</span></span><br><span class="line"><span class="attr">    Launcher:</span></span><br><span class="line"><span class="attr">      replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      template:</span></span><br><span class="line"><span class="attr">         spec:</span></span><br><span class="line"><span class="attr">           containers:</span></span><br><span class="line"><span class="attr">           - image:</span> <span class="string">mpioperator/tensorflow-benchmarks:latest</span></span><br><span class="line"><span class="attr">             name:</span> <span class="string">tensorflow-benchmarks</span></span><br><span class="line"><span class="attr">             command:</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">mpirun</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">--allow-run-as-root</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-np</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">"2"</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-bind-to</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">none</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-map-by</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">slot</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-x</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">NCCL_DEBUG=INFO</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-x</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">LD_LIBRARY_PATH</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-x</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">PATH</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-mca</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">pml</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">ob1</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">-mca</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">btl</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">^openib</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">python</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">--model=resnet101</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">--batch_size=64</span></span><br><span class="line"><span class="bullet">             -</span> <span class="bullet">--variable_update=horovod</span></span><br><span class="line"><span class="attr">    Worker:</span></span><br><span class="line"><span class="attr">      replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      template:</span></span><br><span class="line"><span class="attr">        spec:</span></span><br><span class="line"><span class="attr">          containers:</span></span><br><span class="line"><span class="attr">          - image:</span> <span class="string">mpioperator/tensorflow-benchmarks:latest</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">tensorflow-benchmarks</span></span><br><span class="line"><span class="attr">            resources:</span></span><br><span class="line"><span class="attr">              limits:</span></span><br><span class="line">                <span class="string">nvidia.com/gpu:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>在这里定义了一个<code>Launcher</code>,2个<code>Worker</code>, Work使用GPU, 因此在<code>Launcher</code>的<code>mpirun</code>参数之中<code>-np</code>必须为2, 说明是使用了两个并发.</p><p>等待两个Worker启动之后, Launcher开始刷日志:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -n mpi-operator</span></span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">mpi-operator-584466c4f6-frw4x          1/1       Running   1          3d</span><br><span class="line">tensorflow-benchmarks-launcher-vsq8b   1/1       Running   0          40s</span><br><span class="line">tensorflow-benchmarks-worker-0         1/1       Running   0          40s</span><br><span class="line">tensorflow-benchmarks-worker-1         1/1       Running   0          40s</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl logs -f tensorflow-benchmarks-launcher-vsq8b -n mpi-operator</span></span><br><span class="line">Done warm up</span><br><span class="line">Step    Img/sec total_loss</span><br><span class="line">Done warm up</span><br><span class="line">Step    Img/sec total_loss</span><br><span class="line">1       images/sec: 109.6 +/- 0.0 (jitter = 0.0)        9.181</span><br><span class="line">1       images/sec: 109.9 +/- 0.0 (jitter = 0.0)        9.110</span><br><span class="line">10      images/sec: 108.1 +/- 0.7 (jitter = 1.3)        8.864</span><br><span class="line">10      images/sec: 108.1 +/- 0.7 (jitter = 1.5)        9.184</span><br><span class="line">20      images/sec: 107.9 +/- 0.8 (jitter = 1.1)        9.246</span><br><span class="line">20      images/sec: 107.9 +/- 0.8 (jitter = 1.2)        9.073</span><br><span class="line">30      images/sec: 107.7 +/- 0.6 (jitter = 1.5)        9.147</span><br><span class="line">30      images/sec: 107.7 +/- 0.6 (jitter = 1.5)        9.096</span><br><span class="line">40      images/sec: 107.9 +/- 0.4 (jitter = 1.8)        9.069</span><br><span class="line">40      images/sec: 107.9 +/- 0.4 (jitter = 1.8)        9.194</span><br><span class="line">50      images/sec: 108.3 +/- 0.4 (jitter = 2.2)        9.206</span><br><span class="line">50      images/sec: 108.3 +/- 0.4 (jitter = 2.0)        9.485</span><br><span class="line">60      images/sec: 108.3 +/- 0.3 (jitter = 2.1)        9.139</span><br><span class="line">60      images/sec: 108.3 +/- 0.3 (jitter = 2.0)        9.237</span><br><span class="line">70      images/sec: 107.8 +/- 0.5 (jitter = 2.2)        9.132</span><br><span class="line">70      images/sec: 107.8 +/- 0.5 (jitter = 2.2)        9.045</span><br><span class="line">80      images/sec: 107.8 +/- 0.4 (jitter = 2.3)        9.092</span><br><span class="line">80      images/sec: 107.8 +/- 0.4 (jitter = 2.2)        9.098</span><br><span class="line">90      images/sec: 107.7 +/- 0.4 (jitter = 2.2)        9.205</span><br><span class="line">90      images/sec: 107.7 +/- 0.4 (jitter = 2.3)        9.145</span><br><span class="line">100     images/sec: 107.7 +/- 0.4 (jitter = 2.1)        9.050</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">total images/sec: 215.33</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">100     images/sec: 107.7 +/- 0.4 (jitter = 2.1)        9.013</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">total images/sec: 215.32</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure><blockquote><p>这里用到了<a href="https://hub.docker.com/r/mpioperator/tensorflow-benchmarks" target="_blank" rel="noopener">mpioperator/tensorflow-benchmarks:latest</a>镜像, 需要说明的是, 最新的latest版本是基于cuda10的, 华为云的K8s环境目前只支持cuda9, 因此使用<code>0.2.0</code>的tag版本, 能够完成任务.</p></blockquote><h2 id="实现简析"><a href="#实现简析" class="headerlink" title="实现简析"></a>实现简析</h2><p>在<code>Launcher</code>的日志上, 首先出现的是分发命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">POD_NAME=tensorflow-benchmarks-worker-1</span><br><span class="line"><span class="built_in">shift</span></span><br><span class="line">/opt/kube/kubectl <span class="built_in">exec</span> tensorflow-benchmarks-worker-1 -- /bin/sh -c     PATH=/usr/<span class="built_in">local</span>/bin:<span class="variable">$PATH</span> ; <span class="built_in">export</span> PATH ; LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/lib:<span class="variable">$LD_LIBRARY_PATH</span> ; <span class="built_in">export</span> LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/lib:<span class="variable">$DYLD_LIBRARY_PATH</span> ; <span class="built_in">export</span> DYLD_LIBRARY_PATH ;   /usr/<span class="built_in">local</span>/bin/orted -mca ess <span class="string">"env"</span> -mca ess_base_jobid <span class="string">"2828730368"</span> -mca ess_base_vpid 2 -mca ess_base_num_procs <span class="string">"3"</span> -mca orte_node_regex <span class="string">"tensorflow-benchmarks-launcher-[1:5]l8hm,tensorflow-benchmarks-worker-[1:0-1]@0(3)"</span> -mca orte_hnp_uri <span class="string">"2828730368.0;tcp://172.16.0.86:37557"</span> -mca pml <span class="string">"ob1"</span> -mca btl <span class="string">"^openib"</span> -mca plm <span class="string">"rsh"</span> -mca plm_rsh_agent <span class="string">"/etc/mpi/kubexec.sh"</span> -mca orte_default_hostfile <span class="string">"/etc/mpi/hostfile"</span> -mca hwloc_base_binding_policy <span class="string">"none"</span> -mca rmaps_base_mapping_policy <span class="string">"slot"</span> -mca pmix <span class="string">"^s1,s2,cray,isolated"</span></span><br><span class="line">POD_NAME=tensorflow-benchmarks-worker-0</span><br><span class="line"><span class="built_in">shift</span></span><br><span class="line">/opt/kube/kubectl <span class="built_in">exec</span> tensorflow-benchmarks-worker-0 -- /bin/sh -c     PATH=/usr/<span class="built_in">local</span>/bin:<span class="variable">$PATH</span> ; <span class="built_in">export</span> PATH ; LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/lib:<span class="variable">$LD_LIBRARY_PATH</span> ; <span class="built_in">export</span> LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/lib:<span class="variable">$DYLD_LIBRARY_PATH</span> ; <span class="built_in">export</span> DYLD_LIBRARY_PATH ;   /usr/<span class="built_in">local</span>/bin/orted -mca ess <span class="string">"env"</span> -mca ess_base_jobid <span class="string">"2828730368"</span> -mca ess_base_vpid 1 -mca ess_base_num_procs <span class="string">"3"</span> -mca orte_node_regex <span class="string">"tensorflow-benchmarks-launcher-[1:5]l8hm,tensorflow-benchmarks-worker-[1:0-1]@0(3)"</span> -mca orte_hnp_uri <span class="string">"2828730368.0;tcp://172.16.0.86:37557"</span> -mca pml <span class="string">"ob1"</span> -mca btl <span class="string">"^openib"</span> -mca plm <span class="string">"rsh"</span> -mca plm_rsh_agent <span class="string">"/etc/mpi/kubexec.sh"</span> -mca orte_default_hostfile <span class="string">"/etc/mpi/hostfile"</span> -mca hwloc_base_binding_policy <span class="string">"none"</span> -mca rmaps_base_mapping_policy <span class="string">"slot"</span> -mca pmix <span class="string">"^s1,s2,cray,isolated"</span></span><br></pre></td></tr></table></figure><p>通过<code>/opt/kube/kubectl exec</code>的命令方式, 将执行的真实命令发送到<code>worker</code>上, 这两个命令唯一的差别为<code>-mca ess_base_vpid</code>的序号.</p><blockquote><p><code>worker</code>的启动命令为<code>sleep 365d</code>, 除了接受<code>Launcher</code>的命令之外, 不做其他任何的东西.</p></blockquote><p>那么<code>Launcher</code>是如何实现命令的封装的呢?</p><p>通过mpi-job启动的时候, 将<code>hostfile</code>和<code>kubexec.sh</code>封装在<code>configmap</code>之中.</p><p><code>hostfile</code>通过<code>name-worker-id</code> 的方式组装, 而<code>kubexec.sh</code>应该每个都一样</p><blockquote><p>因此, 每个mpi-job都会有对应的config</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get configmap tensorflow-benchmarks-config -o yaml -n mpi-operator</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  hostfile: |</span><br><span class="line">    tensorflow-benchmarks-worker-0 slots=1</span><br><span class="line">    tensorflow-benchmarks-worker-1 slots=1</span><br><span class="line">  kubexec.sh: |</span><br><span class="line">    <span class="comment">#!/bin/sh</span></span><br><span class="line">    <span class="built_in">set</span> -x</span><br><span class="line">    POD_NAME=<span class="variable">$1</span></span><br><span class="line">    <span class="built_in">shift</span></span><br><span class="line">    /opt/kube/kubectl <span class="built_in">exec</span> <span class="variable">$&#123;POD_NAME&#125;</span> -- /bin/sh -c <span class="string">"$*"</span></span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-09-22T04:07:36Z</span><br><span class="line">  labels:</span><br><span class="line">    app: tensorflow-benchmarks</span><br><span class="line">  name: tensorflow-benchmarks-config</span><br><span class="line">  namespace: mpi-operator</span><br><span class="line">  ownerReferences:</span><br><span class="line">  - apiVersion: kubeflow.org/v1alpha2</span><br><span class="line">    blockOwnerDeletion: <span class="literal">true</span></span><br><span class="line">    controller: <span class="literal">true</span></span><br><span class="line">    kind: MPIJob</span><br><span class="line">    name: tensorflow-benchmarks</span><br><span class="line">    uid: 82cd05da-dcee-11e9-ac58-fa163e3a1ebd</span><br><span class="line">  resourceVersion: <span class="string">"39344012"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/mpi-operator/configmaps/tensorflow-benchmarks-config</span><br><span class="line">  uid: 82cef441-dcee-11e9-860a-fa163e132ef9</span><br></pre></td></tr></table></figure><p>那么这两个文件是如何注入到mpi的流程之中呢?</p><p>应该是通过环境变量<code>OMPI_MCA_plm_rsh_agent</code>和<code>OMPI_MCA_orte_default_hostfile</code>, 这两个应该会像回调函数一样, mpirun的过程之中被执行(这部分是猜测的).</p><h2 id="MPI到底是什么"><a href="#MPI到底是什么" class="headerlink" title="MPI到底是什么?"></a>MPI到底是什么?</h2><p>说了这么久的Kubeflow的MPI Operator, 但对于MPI陌生的人, 应该是完全陌生的领域.</p><p>在这里例子之中, 最后有个参数是<code>--variable_update=horovod</code>, <a href="https://github.com/horovod/horovod" target="_blank" rel="noopener">Horovod</a>就是一种基于MPI架构实现分布式训练框架, 我准备再开个番外篇专门介绍一下<code>Horovod</code>, 在其中学习一下MPI.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JupyterLab插件整理</title>
      <link href="/2019/09/05/JupyterLab%E6%8F%92%E4%BB%B6%E6%95%B4%E7%90%86/"/>
      <url>/2019/09/05/JupyterLab%E6%8F%92%E4%BB%B6%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Jupyter架构"><a href="#Jupyter架构" class="headerlink" title="Jupyter架构"></a>Jupyter架构</h2><p><a href="https://jupyter.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">文档</a></p><p>交互图:</p><p><img src="https://jupyter.readthedocs.io/en/latest/_images/notebook_components.png" alt></p><p>组件图:</p><p><img src="https://jupyter.readthedocs.io/en/latest/_images/repos_map.png" alt></p><h2 id="插件列表"><a href="#插件列表" class="headerlink" title="插件列表"></a>插件列表</h2><p><a href="https://github.com/markusschanta/awesome-jupyter" target="_blank" rel="noopener">awesome-jupyter</a></p><p><a href="https://github.com/mauhai/awesome-jupyterlab" target="_blank" rel="noopener">awesome-jupyterlab</a></p><h3 id="jupyterlab-toc"><a href="#jupyterlab-toc" class="headerlink" title="jupyterlab-toc)"></a><a href="[jupyterlab-toc](https://github.com/ian-r-rose/jupyterlab-toc">jupyterlab-toc</a>)</h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @jupyterlab/toc</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/ian-r-rose/jupyterlab-toc/raw/master/toc.gif" alt></p><h3 id="jupyterlab-tensorboard"><a href="#jupyterlab-tensorboard" class="headerlink" title="jupyterlab_tensorboard"></a><a href="https://github.com/chaoleili/jupyterlab_tensorboard" target="_blank" rel="noopener">jupyterlab_tensorboard</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install jupyterlab_tensorboard</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/chaoleili/jupyterlab_tensorboard/raw/master/image/launcher.png" alt></p><p><img src="https://github.com/chaoleili/jupyterlab_tensorboard/raw/master/image/commands-input.png" alt></p><h3 id="JupyterLab-drawio"><a href="#JupyterLab-drawio" class="headerlink" title="JupyterLab drawio"></a><a href="https://github.com/QuantStack/jupyterlab-drawio" target="_blank" rel="noopener">JupyterLab drawio</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install jupyterlab-drawio</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/QuantStack/jupyterlab-drawio/raw/master/drawio.gif" alt></p><h3 id="fasta-extension"><a href="#fasta-extension" class="headerlink" title="fasta-extension"></a><a href="https://github.com/jupyterlab/jupyter-renderers/tree/master/packages/fasta-extension" target="_blank" rel="noopener">fasta-extension</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @jupyterlab/fasta-extension</span><br></pre></td></tr></table></figure><p><img src="https://camo.githubusercontent.com/6aa00b126595f41bc5bee84a6696234b63036fda/687474703a2f2f672e7265636f726469742e636f2f74656d697a6a616539582e676966" alt></p><h3 id="jupyterlab-git"><a href="#jupyterlab-git" class="headerlink" title="jupyterlab-git"></a><a href="https://github.com/jupyterlab/jupyterlab-git" target="_blank" rel="noopener">jupyterlab-git</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @jupyterlab/git</span><br><span class="line">pip install --upgrade jupyterlab-git</span><br><span class="line">jupyter serverextension <span class="built_in">enable</span> --py jupyterlab_git</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://camo.githubusercontent.com/8e2f2b6abdaff6b180bf6aee95b288a7af0fde4d/687474703a2f2f672e7265636f726469742e636f2f4e39496b7a62796b38502e676966" alt></p><h3 id="jupyterlab-variableInspector"><a href="#jupyterlab-variableInspector" class="headerlink" title="jupyterlab-variableInspector"></a><a href="https://github.com/lckr/jupyterlab-variableInspector" target="_blank" rel="noopener">jupyterlab-variableInspector</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @lckr/jupyterlab_variableinspector</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/lckr/jupyterlab-variableInspector/raw/master/early_demo.gif" alt></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JupyterLab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]KNative介绍</title>
      <link href="/2019/08/24/Kubeflow%E7%B3%BB%E5%88%97-KNative%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/08/24/Kubeflow%E7%B3%BB%E5%88%97-KNative%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>KNative目前的一些介绍文章:</p><ol><li><a href="https://knative.dev/" target="_blank" rel="noopener">官网</a></li><li><a href="https://www.servicemesher.com/getting-started-with-knative/installing-knative.html" target="_blank" rel="noopener">ServiceMesher介绍</a></li><li><a href="https://zhuanlan.zhihu.com/p/53597915" target="_blank" rel="noopener">蚂蚁金服的布道师</a></li><li><a href="https://www.infoq.cn/article/PEOIcPk4lZRg-fAwry8H" target="_blank" rel="noopener">华为云介绍系列</a></li></ol></blockquote><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>第一次关注到KNative这个项目是在Info上<a href="https://www.infoq.cn/article/6TzPqrVwv-YJYGQpKyzk" target="_blank" rel="noopener">Kubernetes 上领先的开源 Serverless 解决方案有哪些</a>的文章上, 文章里面对比了常见的Serverless框架, KNative是其中的一个, 又由于这个项目是Google推出的, 因此当时觉得这个项目应该是最有希望的.</p><blockquote><p>参考Service Mesh之争, 2018年年中的时候,  还有一些框架例如Istio和Linkred再竞争, 但是到2019年中基础上Istio已经变成事实标准了. 具体可以看<a href="https://www.infoq.cn/article/DtxylyFwlyl7K5Jte*WI" target="_blank" rel="noopener">这盘文章</a></p></blockquote><p>目前KNative已经迭代到0.8版本, 按照google对版本命名的方式, 应该离正式版本近了.</p><p>这次随着KFServing依赖KNative组件, 因此安装了一下KNative体验了一下, 就顺便记录一下吧.</p><h2 id="KNative介绍"><a href="#KNative介绍" class="headerlink" title="KNative介绍"></a>KNative介绍</h2><p>KNative是Google在2018年7月份发布的, 定位为基于 Kubernetes 的 Serverless 解决方案，旨在标准化 Serverless，简化其学习成本.</p><p>Serverless 大体上可以分为两种类型：“Backend as a Service” 和 “Functions as a Service”</p><p>BaaS(Backend as a Service) 后端即服务，服务商为客户 (开发者) 提供整合云后端的服务，如提供文件存储、数据存储、推送服务、身份验证服务等功能，以帮助开发者快速开发应用。</p><p>FaaS(Function as a Service) 函数即服务，服务商提供一个平台，允许客户开发、运行和管理应用程序功能，而无需构建和维护基础架构。按照此模型构建应用程序是实现“无服务器”体系结构的一种方式，通常在构建微服务应用程序时使用。</p><p>而现在看KNative算是FAAS的一员,  FAAS必须要解决的三个问题以及KNative对应方案为:</p><ol><li>函数程序如何编译, 因此KNative有对应的Builder模块, 解决代码转化为镜像的问题</li><li>任务如何触发, 对应KNative的Event模块,  使用一些消息中间件来缓存/发送请求, 例如Kafka</li><li>函数如何服务化, 对应的是KNative的Serving模块, 将用户的代码服务化, 并有服务该有的能力, 例如流量控制,自动伸缩等</li></ol><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/knative/three-compomnent.jpg" alt></p><p>这个三组件是KNative的核心, 整个流程都按照三个模块来构建, 下面就看一下每个模块之中的架构概念.</p><blockquote><p>目前是0.8版本, 现在Build模块已经被移除, 链接到了另外一个开源库.</p><p>就整个架构而言,  Build应该算是整个框架的接口, 如果这个Build不在自己做的话, 无法完成平台的闭环</p></blockquote><h2 id="KNative架构"><a href="#KNative架构" class="headerlink" title="KNative架构"></a>KNative架构</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/knative/knative-audience.svg" alt></p><p>首先看一下KNative的顶层架构:</p><ul><li><p>KNative依赖于Kubernetes, 三大组件的物理实现都是Kubernetes的CRD</p></li><li><p>而对外接口又依赖于Istio,  将一些服务治理网关路由的工作交由Istio完成</p></li><li><p>而KNative自己致力于完成开发者服务开发的工作</p></li></ul><blockquote><p>分工很明确, 比较看好这种方式</p></blockquote><h3 id="KNative-Serving模块"><a href="#KNative-Serving模块" class="headerlink" title="KNative Serving模块"></a>KNative Serving模块</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/knative/serving.png" alt></p><p>Serving主要是服务管理的能力, KNative创造了这几个概念:</p><ol><li><p>Service: 工作负载的底层概念, 管理整个生命周期</p></li><li><p>Route: 用于流量控制, 将不同的请求转发到不同的Revision</p></li><li><p>Configuration: 用于维护Service的配置</p></li><li><p>Revision: 每次代码或者配置修改, 都会生成一个Revision, 一个Revision会对应一个K8s的Deployment</p></li></ol><p>KNative的Serving的概念,与Istio有部分的重复, 但是比Istio更好理解.</p><p>下面看一个真实的<a href="https://knative.dev/docs/serving/samples/hello-world/helloworld-go/index.html" target="_blank" rel="noopener">例子</a>:</p><p>代码和docker的构建略过, 只看如何部署Service</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">serving.knative.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span> <span class="comment"># KNative的Serving定义</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">helloworld-go</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">docker.io/&#123;username&#125;/helloworld-go</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">TARGET</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"Go Sample v1"</span></span><br></pre></td></tr></table></figure><p>创建完成这个步骤, 会依次创建Service/Route/Configuration/Revision以及真实的Deployment.</p><p>该Deployment会注入Istio-proxy, 最后访问由Istio路由来决定, 最后访问该Service也是通过Istio的网关</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@gpu-01-client ~]<span class="comment">#kubectl get svc istio-ingressgateway --namespace istio-system</span></span><br><span class="line">NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGE</span><br><span class="line">istio-ingressgateway   LoadBalancer   10.247.103.192   100.95.144.30   80:31651/TCP,443:30856/TCP   9d</span><br></pre></td></tr></table></figure><p>获取KNative的serviceURL</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@gpu-01-client ~]<span class="comment">#kubectl get ksvc helloworld-go  --output=custom-columns=NAME:.metadata.name,URL:.status.url</span></span><br><span class="line">NAME            URL</span><br><span class="line">helloworld-go   http://helloworld-go.default.example.com</span><br></pre></td></tr></table></figure><p>测试<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@gpu-01-client ~]<span class="comment"># curl -H "Host:helloworld-go.default.example.com" http://100.95.144.30</span></span><br><span class="line">Hello Go Sample v1!</span><br></pre></td></tr></table></figure></p><blockquote><p>KNative能够Scala-to-Zero, 这样也有一个坏处, 当一个新请求到来的时候, 第一次启动会相对较长的时间(需要启动一个工作的容器)</p></blockquote><h3 id="KNative-Event模块"><a href="#KNative-Event模块" class="headerlink" title="KNative Event模块"></a>KNative Event模块</h3><blockquote><p>这部分待续, 还没有看完</p></blockquote><h2 id="KNative总结"><a href="#KNative总结" class="headerlink" title="KNative总结"></a>KNative总结</h2><p>目前KNative的版本还在0.8版本, 还不算正式release, 后续可能会有不少变动.</p><p>而且Build模块的去除, 感觉已经缺少了一个对于开发者的Interface, 不知道他们未来打算拿什么来弥补.</p><p>而对于Serverless框架来说, 我感觉近2年之内应该无法决出真正的事实标准框架, 所以对KNative来说, 还有时间.</p><p>对我们来说, 在1.0版本release之前, 不要将KNative纳入到系统的构架之中, 后续在持续关注社区后期的动向.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[转]MySQL与PostgreSQL对比</title>
      <link href="/2019/08/24/%E8%BD%AC-MySQL%E4%B8%8EPostgreSQL%E5%AF%B9%E6%AF%94/"/>
      <url>/2019/08/24/%E8%BD%AC-MySQL%E4%B8%8EPostgreSQL%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>文章出在<a href="https://www.biaodianfu.com/mysql-vs-postgresql.html" target="_blank" rel="noopener">此处</a>, 这篇文章就像文章摘要一样, 里面的结论也无法保证正确, 需要等后续实践之中来验证.</p><p>今后还会收集其他的一些文章补充这篇文章的内容</p></blockquote><p>最近项目之中, 再选型关系型数据库的类型, 之前团队里一直都是用MySQL + MyBatis的方案, 但是其他项目组选型了PostgreSQL, 现在正在考虑是否要迁移数据库系统.</p><h2 id="MySQL的优势项"><a href="#MySQL的优势项" class="headerlink" title="MySQL的优势项"></a>MySQL的优势项</h2><ol><li><strong>流行度高</strong>, 因此相应的第三方工具会更加齐全</li><li><strong>回滚更好</strong>, PG需要定时触发VACUUM, 否则数据可能膨胀</li><li><strong>Windows支持好</strong></li><li><strong>线程模式</strong>, 相比PG资源利用率高</li><li><strong>用户权限更加完善</strong>, PG只有表级权限, 而MySQL支持列权限</li><li><strong>存储引擎插件化</strong>, innodb适合事务处理场景外, myisam适合静态数据的查询场景</li><li><strong>24*7小时运行</strong></li><li><strong>支持堆表和索引表</strong>, PG只支持堆表</li></ol><h2 id="PostgreSQL的优势项"><a href="#PostgreSQL的优势项" class="headerlink" title="PostgreSQL的优势项"></a>PostgreSQL的优势项</h2><ol><li><strong>Json和Array格式支持</strong>, MySQL5.7版本之后, 也支持JSon, 但是能力略为落后</li><li><strong>GIS支持</strong>, 一般都用PG</li><li><strong>PostgREST提供API能力</strong></li><li><strong>支持树状结构</strong>, 例如R-Tree</li><li><strong>SQL编程</strong>, 使用各种语言来编程, 对标的是MySQL的存储过程</li><li><strong>支持外部数据源</strong>, 这儿估计一堆的限制</li><li><strong>Text</strong>没有长度限制, MySQL需要区分small text, middle text, large text, 但实际上我觉得程序员定义这个长度是比较好的, 就像int和long类型一个意思</li><li><strong>支持图结构数据存储</strong></li><li><strong>支持窗口函数</strong>, OVER语句, 没想到MySQL竟然没有支持这个, SparkSQL都实现了</li><li><strong>更多索引类型</strong>, </li><li><strong>集群支持更好</strong>, 这个点需要存疑, 因为mysql的分布式中间件那么多, 感觉有点不对</li><li><strong>事务隔离做的更好</strong></li><li><strong>对于字符支持更好一些</strong></li><li><strong>对表连接支持较完整</strong>, 真的很难相信MySQL不支持HashJoin和SortMergeJoin, 之前数据库确实用的太简单了</li><li><strong>存储方式支持更大的数据量</strong></li><li><strong>时间精度更高</strong></li><li><strong>优化器的功能较完整</strong></li><li><strong>序列支持更好</strong></li><li><strong>对子查询支持更好</strong></li><li><strong>增加列更加简单</strong></li></ol><h2 id="两者选择规则"><a href="#两者选择规则" class="headerlink" title="两者选择规则"></a>两者选择规则</h2><ol><li>如果是非常简单的场景, 直接使用MySQL</li><li>如果涉及到数据完整性和可靠性的时候, 使用PG</li><li>如果是地理数据, 使用PG</li><li>如果有嵌入式SQL场景, 使用PG</li><li>如果你想学习一个经典的关系型数据库, 使用PG吧</li></ol><h2 id="MyBatis兼容PG和MySQL"><a href="#MyBatis兼容PG和MySQL" class="headerlink" title="MyBatis兼容PG和MySQL"></a>MyBatis兼容PG和MySQL</h2><p>如果上层使用了MyBatis的话, MyBatis有个叫做<code>DatabaseIdProvider</code>的能力可以支持多个不同的数据库.</p><p>在我们的例子里面, 需要支持MySQL/PG/HSQL三种数据库, 就需要在MyBatis的配置文件, 写在如下配置:</p><blockquote><p> HSQL是本地测试常用的数据库</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">databaseIdProvider</span> <span class="attr">type</span>=<span class="string">"DB_VENDOR"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"MySQL"</span> <span class="attr">value</span>=<span class="string">"mysql"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"PostgreSQL"</span> <span class="attr">value</span>=<span class="string">"postgresql"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"HSQL"</span> <span class="attr">value</span>=<span class="string">"hsqldb"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">databaseIdProvider</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在具体的查询语句的配置项文件里面, 可以使用如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">choose</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">when</span> <span class="attr">test</span>=<span class="string">"_databaseId == 'postgresql'"</span>&gt;</span></span><br><span class="line">    LIMIT #&#123;pageSize&#125; OFFSET #&#123;offset&#125;;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">when</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">when</span> <span class="attr">test</span>=<span class="string">"_databaseId == 'mysql'"</span>&gt;</span></span><br><span class="line">    LIMIT #&#123;offset&#125;, #&#123;pageSize&#125;;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">when</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">otherwise</span>&gt;</span></span><br><span class="line">    LIMIT #&#123;offset&#125;, #&#123;pageSize&#125;;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">otherwise</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">choose</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在这个例子里面, PG和MySQL的Limit语法不同, 因此根据不同情况来生成不同的SQL语句.</p><p>在这儿使用的是MyBatis里面的<code>choose-when-otherwise</code>的语法, 这个语法含义和Java中的<code>match</code>含义类似</p><p>在每个<code>when</code>中判断内置的<code>DatabaseIdProvider</code>字段<code>_databaseId</code>是否为PG或者MySQL, 这儿的字符串<code>mysql</code>和<code>postgresql</code>就是在<code>databaseIdProvider</code>定义的那个两个数据库</p>]]></content>
      
      
      <categories>
          
          <category> 转载文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]KFServing介绍</title>
      <link href="/2019/08/23/Kubeflow%E7%B3%BB%E5%88%97-KFServing%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/08/23/Kubeflow%E7%B3%BB%E5%88%97-KFServing%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>之前调研了<a href="https://saintbacchus.github.io/2019/08/11/TensorServing%E4%BE%8B%E5%AD%90/" target="_blank" rel="noopener">TensorFlow Serving</a>的功能, 这周又抽出来一点时间, 调研一下<a href="https://github.com/kubeflow/kfserving.git" target="_blank" rel="noopener">KubeFlow Serving</a></p><h2 id="KFServing的定位"><a href="#KFServing的定位" class="headerlink" title="KFServing的定位"></a>KFServing的定位</h2><p>之前在Kubeflow的<a href="https://saintbacchus.github.io/2019/07/14/Kubeflow%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">介绍文章</a>有提过, Kubeflow支持Training和Serving, 但是如果仔细看Serving的<a href="https://www.kubeflow.org/docs/components/serving/" target="_blank" rel="noopener">官网</a>会发现, 目前这个Serving似乎只是把这种各样的Serving镜像给安装部署起来就好了, 感觉游离在系统之外的.</p><p>所以TFServing的目标就是设计一套接口, 将各个Serving模块抽象化, 变成一套系统.</p><blockquote><p> 但是, 目前TFServing虽然提交活跃度挺高, 但是还没有挂在官网介绍文章之中,  版本非常新, 目前才v0.2版本, 未来接口可能会有很大变化</p></blockquote><h2 id="KFServing架构"><a href="#KFServing架构" class="headerlink" title="KFServing架构"></a>KFServing架构</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kfserving/kfserving.png" alt></p><p>从架构图上可以看出, KFServing在非常高的位置之上,  它底层的服务能力依赖于<a href="https://knative.dev/docs/" target="_blank" rel="noopener">KNative</a>(<em>主要是KNative的Serving模块</em>), 上层支持的框架有:</p><ol><li>TensorFlow: 镜像由于TensorFlow官网提供</li><li>PyTorch:  镜像由KFServing制作, 代码逻辑位于<a href="https://github.com/kubeflow/kfserving/tree/master/python" target="_blank" rel="noopener">此</a></li><li>SKLearn: 同上</li><li>XGBoost: 同上</li><li>TensorRT: 镜像由NVIDIA提供</li></ol><p><strong>基本上主流的一些机器学习框架都已经支持</strong></p><p>下面看一下KFServing的服务能力,  数据流图如下所以: </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kfserving/dataplane.jpg" alt></p><p>这个是一个典型的灰度发布场景,  一个默认环境, 一个灰度环境, 由KNative的来控制流量.</p><blockquote><p>KFServing似乎目前只支持一个灰度实例</p></blockquote><h2 id="KFServing的使用案例"><a href="#KFServing的使用案例" class="headerlink" title="KFServing的使用案例"></a>KFServing的使用案例</h2><p>看个简单的TensorFlow Serving的例子,  所有的样例在这个<a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples" target="_blank" rel="noopener">代码路径</a>之中</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"serving.kubeflow.org/v1alpha1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"KFService"</span> <span class="comment"># `KFService`是KFServing在K8s上定义的CRD.</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"flowers-sample"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  default:</span></span><br><span class="line"><span class="attr">    tensorflow:</span></span><br><span class="line"><span class="attr">      modelUri:</span> <span class="string">"gs://kfserving-samples/models/tensorflow/flowers"</span></span><br></pre></td></tr></table></figure><p>这段YAML的意思为, 创建一个TensorFlow类型的Serving, 模型文件存在<code>Google Cloud</code>的<code>gs://kfserving-samples/models/tensorflow/flowers</code>的路径之中.</p><blockquote><p>TensorFlow Serving并不支持GCS的路径, 这里TF Serving通过一个<a href="https://github.com/kubeflow/kfserving/blob/master/python/model-initializer.Dockerfile" target="_blank" rel="noopener">model-initializer</a>的<code>init-container</code>提前下载到容器内部, TF Serving实例上读取是本地路径,  所以模型不能太大, 因为目前还不支持挂载PVC</p></blockquote><p>当然也可以设置一个灰度服务: </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"serving.kubeflow.org/v1alpha1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"KFService"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"flowers-sample"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  default:</span></span><br><span class="line">   <span class="comment"># 90% of traffic is sent to this model</span></span><br><span class="line"><span class="attr">    tensorflow:</span></span><br><span class="line"><span class="attr">      modelUri:</span> <span class="string">"gs://kfserving-samples/models/tensorflow/flowers"</span></span><br><span class="line"><span class="attr">  canaryTrafficPercent:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  canary:</span></span><br><span class="line">   <span class="comment"># 10% of traffic is sent to this model</span></span><br><span class="line"><span class="attr">    tensorflow:</span></span><br><span class="line"><span class="attr">      modelUri:</span> <span class="string">"gs://kfserving-samples/models/tensorflow/flowers-2"</span></span><br></pre></td></tr></table></figure><p>这个YAML的含义是, 90%走到default服务, 也就是<code>flowers</code>模型, 而剩下的10%的模型走到<code>canary</code>去, 模型为<code>flowers-2</code></p><blockquote><p>这里会生成KNative的资源有: 1个Service, 1个Route, 2个Configuration, 2个Revision</p></blockquote><p>你可以使用KNative的<a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/tensorflow#knative-cli" target="_blank" rel="noopener">客户端</a>, 来完成两个灰度升级回滚等操作.</p><p>访问流量需要获取<code>istio-ingressgateway</code>的地址, 加上定义的url就能访问了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MODEL_NAME=flowers-sample</span><br><span class="line">INPUT_PATH=@./input.json</span><br><span class="line">CLUSTER_IP=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="string">'&#123;.status.loadBalancer.ingress[0].ip&#125;'</span>)</span><br><span class="line">SERVICE_HOSTNAME=$(kubectl get kfservice <span class="variable">$&#123;MODEL_NAME&#125;</span> -o jsonpath=<span class="string">'&#123;.status.url&#125;'</span>)</span><br><span class="line"></span><br><span class="line">curl -v -H <span class="string">"Host: <span class="variable">$&#123;SERVICE_HOSTNAME&#125;</span>"</span> http://<span class="variable">$CLUSTER_IP</span>/v1/models/<span class="variable">$MODEL_NAME</span>:predict -d <span class="variable">$INPUT_PATH</span></span><br></pre></td></tr></table></figure><blockquote><p>YAML的字段定义在<a href="https://github.com/kubeflow/kfserving/blob/master/docs/control-plane.md" target="_blank" rel="noopener">这篇文档</a>之中</p></blockquote><h2 id="KFServing总结"><a href="#KFServing总结" class="headerlink" title="KFServing总结"></a>KFServing总结</h2><p>首先, KFServing目前是一个非常前期的项目, 这就注定了可能很多功能, 它目前都不支持, 但是目前它的目标我们实际上是接受的, 等这个版本再经过几轮迭代之后可以跟进.</p><blockquote><p>模型资产存储在OBS上, Serving获取模型, 自动部署模型推理服务</p></blockquote><p>其次, KFServing的架构依赖实在太多了, 特别依赖是KNative, 目前还不能确定KNative能够在Serverless框架的竞争之中胜出, 而且KNative的版本也比较前期, 问题也挺多, 对于KFServing质量评分造成了不少影响.</p><blockquote><p>实际上灰度发布和流量控制, Istio能力已经完全具备, 不知道为什么必须依赖KNative, 未来能解除KNative的依赖就好了</p></blockquote><p>再次, KFServing有Python的<a href="https://github.com/kubeflow/kfserving/tree/master/python/kfserving" target="_blank" rel="noopener">SDK</a>, 可以使用代码直接部署KFServing的Service, 这对工程人员是一个很方便的能力.</p><p>此外, 它由一个叫做<a href="https://github.com/kubeflow/kfserving/tree/master/tools/tf2openapi" target="_blank" rel="noopener">TF2OpenAPI</a>的小工具, 能够将TF的模型的API描述自动生成, 又是对工程人员的工具.</p><p>最后, KFServing的<a href="https://github.com/kubeflow/kfserving/blob/master/ROADMAP.md" target="_blank" rel="noopener">路标</a>之中提到, KFServing马上会在Kubeflow 0.7版本之中就集成了, 而且9月初就会支持PVC啦,  赶紧迭代起来吧.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu实用工具集</title>
      <link href="/2019/08/18/Ubuntu%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E9%9B%86/"/>
      <url>/2019/08/18/Ubuntu%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h2><ol><li><a href="https://code.visualstudio.com/" target="_blank" rel="noopener">Visual Code</a>应该是最好用的代码编辑器了</li><li><a href="https://typora.io/" target="_blank" rel="noopener">Typora</a>是Markdown编辑器</li><li><a href="https://github.com/notepadqq/notepadqq" target="_blank" rel="noopener">notepadqq</a>Windows上Notepad++的替代者, 没那么好用</li><li><a href="https://github.com/shiftkey/desktop" target="_blank" rel="noopener">GithubDestop</a>图像化管理GitRepository库</li><li><a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener">Idea intellij</a>全家桶</li><li><a href="https://snapcraft.io" target="_blank" rel="noopener">snap</a>Ubuntu上的homebrew, 不过挺难用的</li><li><a href="https://www.getpostman.com/downloads/" target="_blank" rel="noopener">PostMan</a> 图形化API发送工具</li><li><a href="https://docs.conda.io/en/latest/miniconda.html" target="_blank" rel="noopener">MiniConda</a>Python的环境管理工具</li><li><a href="https://zealdocs.org/download.html" target="_blank" rel="noopener">Zeal</a>查看SDK的工具, MacOS上Dash的替代品</li></ol><h2 id="办公工具"><a href="#办公工具" class="headerlink" title="办公工具"></a>办公工具</h2><ol><li><a href="https://www.wps.com/linux" target="_blank" rel="noopener">WPS</a>: 国产的Office工具</li><li><a href="https://github.com/shadowsocks" target="_blank" rel="noopener">Shadowsocks</a>: 翻墙工具</li><li><a href="https://www.jianguoyun.com/s/downloads" target="_blank" rel="noopener">坚果云</a>: 个人云盘, 有2G免费空间</li><li><a href="https://www.google.com/chrome/" target="_blank" rel="noopener">Chrome</a>: Google浏览器</li><li><a href="https://bbs.360.cn/thread-15529293-1-1.html" target="_blank" rel="noopener">360浏览器</a>: 确实没想到360竟然也有linux浏览器</li><li><a href="https://www.xmind.net/download/" target="_blank" rel="noopener">XMind</a>: 思维导图工具</li><li>Shutter截图工具</li><li><a href="https://www.virtualbox.org/wiki/Linux_Downloads" target="_blank" rel="noopener">VisualBox</a>偶尔可以切换一下Windows</li><li><a href="https://www.teamviewer.com/en-us/download/linux/" target="_blank" rel="noopener">TeamViewer</a>可以控制远程的桌面</li></ol><h2 id="娱乐工具"><a href="#娱乐工具" class="headerlink" title="娱乐工具"></a>娱乐工具</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/40419090" target="_blank" rel="noopener">网易云音乐</a>: 官网链接找不到了, 只有其他的安装文档</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[转]OAuth学习</title>
      <link href="/2019/08/18/%E8%BD%AC-OAuth%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/08/18/%E8%BD%AC-OAuth%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p> <a href="http://www.ruanyifeng.com/blog/2019/04/oauth_design.html" target="_blank" rel="noopener">文章转载自此</a></p></blockquote><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>简单说，OAuth 就是一种<strong>授权机制</strong>。数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（token），用来代替密码，供第三方应用使用</p><p>平时我们遇到的使用微信授权登录, 就是OAuth的一个应用. 在国外经常使用github或者google/facebook账号用于登录各种类似的网站.</p><p><img src="https://www.wangbase.com/blogimg/asset/201904/bg2019042101.jpg" alt></p><h2 id="令牌和密码"><a href="#令牌和密码" class="headerlink" title="令牌和密码"></a>令牌和密码</h2><p><code>OAuth</code>的一个特点是使用Token.</p><p>令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。</p><ol><li><p>令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。</p></li><li><p>令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。</p></li><li><p>令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。</p></li></ol><p>上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。</p><p>注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以<strong>令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。</strong>这也是为什么令牌的有效期，一般都设置得很短的原因。</p><h2 id="四种授权方式"><a href="#四种授权方式" class="headerlink" title="四种授权方式"></a>四种授权方式</h2><p>具体描述详见<a href="http://www.ruanyifeng.com/blog/2019/04/oauth-grant-types.html" target="_blank" rel="noopener">文章</a>, 具体的认证流程的就不摘录了, 只摘录特点</p><h3 id="授权码"><a href="#授权码" class="headerlink" title="授权码"></a>授权码</h3><p>这种方式是最常用的流程，安全性也最高，它<strong>适用于那些有后端的 Web 应用</strong>。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。</p><h3 id="隐藏式"><a href="#隐藏式" class="headerlink" title="隐藏式"></a>隐藏式</h3><p>有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。<strong>RFC 6749 就规定了第二种方式，允许直接向前端颁发令牌。这种方式没有授权码这个中间步骤，所以称为（授权码）”隐藏式”（implicit）。</strong></p><p>这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。</p><h3 id="密码式"><a href="#密码式" class="headerlink" title="密码式"></a>密码式</h3><p><strong>如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。</strong></p><p>这种方式需要用户给出自己的用户名/密码，显然风险很大，因此只适用于其他授权方式都无法采用的情况，而且必须是用户高度信任的应用。</p><h3 id="凭证式"><a href="#凭证式" class="headerlink" title="凭证式"></a>凭证式</h3><p><strong>最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。</strong></p><p>这种方式给出的令牌，是针对第三方应用的，而不是针对用户的，即有可能多个用户共享同一个令牌。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>授权码</strong>是最常用的, 其他三个主要是为了针对特定的场景.</p><h2 id="代码案例"><a href="#代码案例" class="headerlink" title="代码案例"></a>代码案例</h2><p>参见这篇<a href="http://www.ruanyifeng.com/blog/2019/04/github-oauth.html" target="_blank" rel="noopener">博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQLFlow深度解析</title>
      <link href="/2019/08/17/SQLFlow%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/08/17/SQLFlow%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="SQLFLow介绍"><a href="#SQLFLow介绍" class="headerlink" title="SQLFLow介绍"></a>SQLFLow介绍</h2><p>SQLFlow是阿里巴巴蚂蚁金服开源的一个AI On SQL的项目, 目标是<strong>SQL 引擎和 AI 引擎连接起来，让用户仅需几行 SQL 代码就能描述整个应用或者产品背后的数据流和 AI 构造</strong></p><p>SQLFlow 最早的初衷，就是希望解决分析师既要操作数据又要使用 AI、往往需要在两个甚至更多的系统之间切换、工作效率低的窘境。</p><p>目前业界已有的AI ON SQL的方案:</p><ul><li>Microsoft SQL Server：Microsoft SQL Server 支持机器学习服务，可以将 R 或 Python 编写的机器学习程序作为外部脚本运行.缺点: 需要编写R或者Python程序代码</li><li>Teradata SQL for DL：Teradata 也提供了 RESTful 服务，可以通过扩展的 SQL SELECT 语法调用. 缺点: 语法耦合了它的Rest服务 </li><li>Google BigQuery：Google BigQuery 通过引入 CREATE MODEL 语句让用 SQL 实现机器学习成为可能. 缺点: 支持的模型有点少, 深度学习还不支持.</li></ul><p>针对以上三个方案的缺点, SQLFlow的定义了自己的三个设计目标:</p><ul><li>这一解决方案应与许多 SQL 引擎都兼容，而不是只能兼容特定版本或类型的 SQL 引擎。</li><li>它应该支持复杂的机器学习模型，包括用于深度学习的 TensorFlow 和用于树模型的 XGBoost。</li><li>能够灵活地配置和运行前沿机器学习算法，包括指定特征交叉，无需在 SQL 语句中嵌入 Python 或 R 代码，以及完全集成超参数估计等。</li></ul><p>目前阶段来说, SQLFlow已经支持MySQL/MaxCompute/Hive, 机器学习框架已经支持TF和XGBoost.</p><p><strong>资源:</strong></p><p><a href="https://www.infoq.cn/article/vlVqC68h2MT-028lh68C" target="_blank" rel="noopener">宣传文章</a></p><p><a href="https://github.com/sql-machine-learning/sqlflow" target="_blank" rel="noopener">Github官网</a></p><p><a href="https://sqlflow.org" target="_blank" rel="noopener">官方文档</a></p><blockquote><p>Spark社区既有MLib这样的机器学习框架, 也有一些基于深度学习的扩展, 例如<a href="https://github.com/horovod/horovod" target="_blank" rel="noopener">Uber的horovod</a>还有<a href="https://github.com/yahoo/TensorFlowOnSpark" target="_blank" rel="noopener">Yahoo的TensorFlowOnSpark</a>, 但是这些框架都是基于的是Spark DataSet的接口联通的, 你可以在DataSet API上使用SQL, 也可以使用AI接口,  你可以认为是<code>AI + SQL</code>的模式, 而不是<code>AI ON SQL</code>的模式</p></blockquote><h2 id="SQLFlow试用"><a href="#SQLFlow试用" class="headerlink" title="SQLFlow试用"></a>SQLFlow试用</h2><p>社区提供了一个官方的试用的Docker镜像, 只要键入以下命令启动容器即可:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 8888:8888 sqlflow/sqlflow:latest</span><br></pre></td></tr></table></figure><p>试用浏览器打开容器机器所在的8888端口, 可以看到一个notebook的页面, 默认已经有一个<code>example.ipynb</code>的样例文件了</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/2.png" alt></p><p>打开这个<code>example.ipynb</code>文件, 这个镜像里面已经安装了<code>mysql</code>, 同时也把部分测试数据导入到数据库里面, 你不需要做任何数据处理的工作, 就可以直接运行Notebook里面的Cell. <em>蚂蚁的开发人员真的很贴心啊</em></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/3.png" alt></p><p>执行推理过程, 使用<code>DNNClassifier</code>算法模型, 并将结果写入到<code>sqlflow_models.my_dnn_model</code>表中, <code>my_dnn_model</code>只有两个字段: ID和BLOB(存放模型序列化后的字节流)</p><p>执行对于测试集(<code>iris.test</code>)进行推理, 并写入到预测结果表之中(<code>iris.predict.class</code>)</p><p>最后展示推理结果集<code>iris.predict</code></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/4.png" alt></p><h2 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h2><p>这个AI ON SQL系统里面, 首先要回到的一个问题是, <strong>AI系统的计算层和SQL系统的计算层是什么关系?</strong></p><p>例如Spark(BigQuery大概率也是, 但是因为闭源不能确定)AI引擎代码是内嵌于SQL计算系统之中的, 并行执行的能力由Spark管理, AI系统就像代码库一样被Spark系统所调用而已.</p><p>但SQLFlow明显不是这种模式的:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/1.png" alt></p><p>从SQLFlow的架构图上看的出来, <strong>AI Engine和SQL Engine之间是独立的</strong>, 两者通过RPC交互<strong>数据和模型</strong></p><ol><li>AI Engine训练或者推理计算的时候, 从SQLEngine获取数据</li><li>AI Engine完成训练过程, 将模型写入到SQL Engine; 推理过程从SQL Engine读取模型</li></ol><p>整个SQLFlow的流程大致如下(图上红色部分为SQLFlow):</p><ol><li>Notebook输入SQL之后, 送入到<code>Parser</code>之内, 这儿的语法解析借用了Hive/MaxCompute等引擎</li><li>解析完SQL语法后, 进行<code>Schema Verification</code></li><li>然后根据SQL语法, 产生对应的Code(根据不同模型和不同引擎产生不同的Code)</li><li>最后执行Code</li></ol><p><a href="https://sql-machine-learning.github.io/sqlflow/doc/syntax.html" target="_blank" rel="noopener">设计文档</a></p><h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><p>最后我们来简单过一遍SQLFlow的代码, 提炼一下找代码框架的思路:</p><blockquote><p>SQLFlow代码量目测在1-1.5万行左右, 半天就能看懂基础流程了</p></blockquote><h3 id="找到入口"><a href="#找到入口" class="headerlink" title="找到入口"></a>找到入口</h3><p>首先, 找到整个项目的Dockerfile, 就在根目录下</p><blockquote><p>如果项目有Docker镜像, Dockerfile就是个个进程的入口, 比<code>main</code>函数还在前面</p></blockquote><figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ADD</span><span class="bash"> scripts/start.sh /</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"bash"</span>, <span class="string">"/start.sh"</span>]</span></span><br></pre></td></tr></table></figure><p>看到最后一行启动的命令为<code>start.sh</code>, 源码文件在<code>scripts/start.sh</code></p><h3 id="找到启动文件"><a href="#找到启动文件" class="headerlink" title="找到启动文件"></a>找到启动文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">print_usage</span></span>() &#123;</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Usage: /bin/bash start.sh [OPTION]\n"</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"\tpopulate-example-dataset-mysql: populate an existing mysql instance with the example dataset."</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"\tmysql: setup the mysql server with the example dataset initialized."</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"\tsqlflow_server: setup the sqlflow gRPC server."</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"\tsqlflow_notebook: setup the Jupyter Notebook server."</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"\tall(default): setup a MySQL server instance, a sqlflow gRPC server and a Jupyter Notebook server sequentially."</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">  ARG=<span class="variable">$&#123;1:-all&#125;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$ARG</span> <span class="keyword">in</span></span><br><span class="line">    all)</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"setup all-in-one"</span></span><br><span class="line">      setup_mysql</span><br><span class="line">      setup_sqlflow_server &amp;</span><br><span class="line">      setup_sqlflow_notebook</span><br><span class="line">      ;;</span><br><span class="line">    *)</span><br><span class="line">      print_usage</span><br><span class="line">      ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到启动文件启动了三个内容: </p><ol><li><p>mysql: 默认的SQL Engine, 已经初始化了数据</p></li><li><p>sqlflow_server: 我们要找的程序, 找到里面真正的执行命令<code>sqlflowserver --datasource=${DS}</code></p></li><li><p>sqlflow_notebook: Notebook交互式界面</p></li></ol><h3 id="找到main函数入口"><a href="#找到main函数入口" class="headerlink" title="找到main函数入口"></a>找到main函数入口</h3><p>全局搜索<code>function main()</code>, 发现只有<code>cmd/sqlflowserver/main.go</code>有.</p><p>在<code>start</code>函数里面找到关键的<code>proto.RegisterSQLFlowServer(s, server.NewServer(sql.Run, nil, modelDir, enableSession))</code>服务启动代码, 其中的<code>sql.Run</code>就整个SQL处理代码.</p><blockquote><p>不太熟悉go语言是怎么出包的</p></blockquote><h3 id="SQL-Run"><a href="#SQL-Run" class="headerlink" title="SQL.Run"></a>SQL.Run</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Run</span><span class="params">(slct <span class="keyword">string</span>, db *DB, modelDir <span class="keyword">string</span>, session *pb.Session)</span> *<span class="title">PipeReader</span></span> &#123;</span><br><span class="line">splittedSQL, err := splitExtendedSQL(slct)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">rd, wr := Pipe()</span><br><span class="line"><span class="comment">// return the lexer error message to client side</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wr.Close()</span><br><span class="line">wr.Write(err)</span><br><span class="line">&#125;()</span><br><span class="line"><span class="keyword">return</span> rd</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(splittedSQL) == <span class="number">2</span> &#123;</span><br><span class="line"><span class="keyword">return</span> runExtendedSQL(slct, db, modelDir, session)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> runStandardSQL(slct, db)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果是SQL语句, 走入到<code>runStandardSQL</code>分支, 如果有训练或者推理语法, 就会走入<code>runExtendedSQL</code>分支</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">runExtendedSQL</span><span class="params">(slct <span class="keyword">string</span>, db *DB, modelDir <span class="keyword">string</span>, session *pb.Session)</span> *<span class="title">PipeReader</span></span> &#123;</span><br><span class="line">rd, wr := Pipe()</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wr.Close()</span><br><span class="line"></span><br><span class="line">err := <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">(startAt time.Time)</span></span> &#123;</span><br><span class="line">log.Debugf(<span class="string">"runExtendedSQL %v finished, elapsed:%v"</span>, slct, time.Since(startAt))</span><br><span class="line">&#125;(time.Now())</span><br><span class="line">pr, e := newParser().Parse(slct) <span class="comment">// 语言解析</span></span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line">cwd, e := ioutil.TempDir(<span class="string">"/tmp"</span>, <span class="string">"sqlflow"</span>)</span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">defer</span> os.RemoveAll(cwd)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> pr.train &#123;</span><br><span class="line">ds, e := newTrainAndValDataset(db, pr.standardSelect.String(), pr.standardSelect.tables[<span class="number">0</span>], <span class="number">0.8</span>)</span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> train(wr, pr, db, cwd, modelDir, slct, ds) <span class="comment">// 调用训练</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> pred(wr, pr, db, cwd, modelDir) <span class="comment">// 调用推理部分</span></span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorf(<span class="string">"runExtendedSQL error:%v"</span>, err)</span><br><span class="line"><span class="keyword">if</span> err != ErrClosedPipe &#123;</span><br><span class="line"><span class="keyword">if</span> err := wr.Write(err); err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorf(<span class="string">"runExtendedSQL error(piping):%v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"><span class="keyword">return</span> rd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到, 先进行<strong>语法解析</strong>, 然后进行<strong>训练或者推理</strong>逻辑, 我们简单看一眼<strong>训练</strong>过程:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">train</span><span class="params">(wr *PipeWriter, tr *extendedSelect, db *DB, cwd <span class="keyword">string</span>, modelDir <span class="keyword">string</span>, slct <span class="keyword">string</span>, ds *trainAndValDataset)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">fts, e := verify(tr, db) <span class="comment">// 语法校验</span></span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> program bytes.Buffer</span><br><span class="line"><span class="keyword">if</span> e := genTF(&amp;program, tr, ds, fts, db); e != <span class="literal">nil</span> &#123; <span class="comment">// 生成代码</span></span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"genTF %v"</span>, e)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cw := &amp;logChanWriter&#123;wr: wr&#125;</span><br><span class="line"><span class="keyword">defer</span> cw.Close()</span><br><span class="line">cmd := tensorflowCmd(cwd, db.driverName) <span class="comment">// 执行命令</span></span><br><span class="line">cmd.Stdin = &amp;program</span><br><span class="line">cmd.Stdout = cw</span><br><span class="line">cmd.Stderr = cw</span><br><span class="line"><span class="keyword">if</span> e := cmd.Run(); e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"training failed %v"</span>, e)</span><br><span class="line">&#125;</span><br><span class="line">m := model&#123;workDir: cwd, TrainSelect: slct&#125;</span><br><span class="line"><span class="keyword">if</span> modelDir != <span class="string">""</span> &#123;</span><br><span class="line"><span class="keyword">return</span> m.saveTar(modelDir, tr.save) <span class="comment">// 保存模型到本地文件夹</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> m.save(db, tr.save) <span class="comment">// 保存模型到数据库</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到, 进入训练过程之后, 先做了<strong>语法校验</strong>, 然后<strong>生成的对应的TF的代码</strong>, 然后调用<code>tensorflowCmd</code><strong>执行命令</strong>, 最后将<strong>模型保存</strong>完毕, 完成训练过程.</p><p><strong>语法校验</strong>先略过, <strong>代码生成</strong>过程比较复杂后面再介绍, 我们先关注于<strong>命令执行</strong>过程:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">tensorflowCmd</span><span class="params">(cwd, driverName <span class="keyword">string</span>)</span> <span class="params">(cmd *exec.Cmd)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> hasPython() &amp;&amp; hasTensorFlow() &amp;&amp; hasDatabaseConnector(driverName) &#123;</span><br><span class="line">log.Printf(<span class="string">"tensorflowCmd: run locally"</span>)</span><br><span class="line">cmd = exec.Command(<span class="string">"python"</span>, <span class="string">"-u"</span>)</span><br><span class="line">cmd.Dir = cwd</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> hasDocker() &#123;</span><br><span class="line">log.Printf(<span class="string">"tensorflowCmd: run in Docker container"</span>)</span><br><span class="line"><span class="keyword">const</span> tfImg = <span class="string">"sqlflow/sqlflow"</span></span><br><span class="line"><span class="keyword">if</span> !hasDockerImage(tfImg) &#123;</span><br><span class="line">log.Printf(<span class="string">"No local Docker image %s.  It will take a long time to pull."</span>, tfImg)</span><br><span class="line">&#125;</span><br><span class="line">cmd = exec.Command(<span class="string">"docker"</span>, <span class="string">"run"</span>, <span class="string">"--rm"</span>,</span><br><span class="line">fmt.Sprintf(<span class="string">"-v%s:/work"</span>, cwd),</span><br><span class="line"><span class="string">"-w/work"</span>, <span class="string">"--network=host"</span>, <span class="string">"-i"</span>, tfImg, <span class="string">"python"</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">"No local TensorFlow or Docker.  No way to run TensorFlow programs"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cmd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>tensorflowCmd</code>有两种执行模式: <strong>本地执行</strong>和<strong>容器执行</strong>, 目前这两种方式都是单机执行模型, 实际上这儿就印证了<strong>AI Engine和SQL Engine分离</strong>的架构</p><p>未来这儿可以很方便的将这个扩展为分布式任务, 例如Kubeflow的TFJob,这个这块需要跟代码生成那儿一起修改.</p><h3 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h3><p>让我们在回到<code>genTF</code>这个函数</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">genTF</span><span class="params">(w io.Writer, pr *extendedSelect, ds *trainAndValDataset, fts fieldTypes, db *DB)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">r, e := newFiller(pr, ds, fts, db) <span class="comment">// 应该是按照字段的过滤吧, 没仔细看, 可能会出错</span></span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> pr.train &#123;</span><br><span class="line"><span class="keyword">return</span> tfTrainTemplate.Execute(w, r) <span class="comment">// 根据训练模板生成code</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> tfPredTemplate.Execute(w, r) <span class="comment">// 根据推理模板生成code</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> tfTrainTemplate = template.Must(template.New(<span class="string">"codegenTfTrain"</span>).Parse(tfTrainTemplateText)) <span class="comment">// 训练模板</span></span><br><span class="line"><span class="keyword">var</span> tfPredTemplate = template.Must(template.New(<span class="string">"codegenTfPred"</span>).Parse(tfPredTemplateText)) <span class="comment">// 推理模板</span></span><br></pre></td></tr></table></figure><p>这儿实际上最关键的是两个训练模板, 这两个模块在<code>template_tf.go</code>里面定义</p><blockquote><p>除了TF的模板, 还有<code>template_alps</code>和<code>template_elasticdl</code>这两个</p></blockquote><p>模板里面就是Python代码了, 截取里面的一部分说明一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># Disable Tensorflow INFO and WARNING logs</span></span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'3'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys, json</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> sqlflow_models <span class="comment"># 注意点1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sqlflow_submitter.db <span class="keyword">import</span> connect, db_generator <span class="comment"># 注意点2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略一部分代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(datasetStr)</span>:</span></span><br><span class="line">    feature_types = []</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> feature_column_names:</span><br><span class="line">        <span class="keyword">if</span> feature_metas[name][<span class="string">"is_sparse"</span>]:</span><br><span class="line">            feature_types.append((tf.int64, tf.int32, tf.int64))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature_types.append(get_dtype(feature_metas[name][<span class="string">"dtype"</span>]))</span><br><span class="line"></span><br><span class="line">    gen = db_generator(driver, conn, datasetStr, feature_column_names, <span class="string">"&#123;&#123;.Y.FeatureName&#125;&#125;"</span>, feature_metas)</span><br><span class="line">    dataset = tf.data.Dataset.from_generator(gen, (tuple(feature_types), tf.&#123;&#123;.Y.Dtype&#125;&#125;)) <span class="comment"># 注意点3</span></span><br><span class="line">    ds_mapper = functools.partial(_parse_sparse_feature, feature_metas=feature_metas)</span><br><span class="line">    <span class="keyword">return</span> dataset.map(ds_mapper)</span><br></pre></td></tr></table></figure><p><strong>注意点1</strong>是<code>sqlflow_models</code>, 这个是定义在同组织的<a href="https://github.com/sql-machine-learning/models" target="_blank" rel="noopener">Model</a>里面,  目前只实现了2个实现<code>dnnclassifier</code>和<code>lstmclassifier</code>, 这里这个<code>dnnclassifier</code>就是试用里面<code>DNNClassifier</code>算法模型的定义所在</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/5.png" alt></p><p><strong>注意点2</strong>是<code>sqlflow_submitter</code>这个定义在<code>sql/python/sqlflow_submmiter</code>包目录下, 在这儿你可以执行本地的Python文件, 也可以定义自己的submit将<code>CodeGen</code>的代码当做客户端代码, 给远程的深度学习服务提交自己的学习任务. 同时这儿也定义了与<code>SQL Engine</code>的交互代码逻辑, 就是里面的<code>connect</code>和<code>db_generator</code></p><p><strong>注意点3</strong>关注于TensorFlow框架是如何读取从数据库里面的数据的, 使用的接口为<code>tf.data.Dataset.from_generator</code></p><p><strong>至此代码分析已经完成, 主流程已经明确了.</strong></p><h2 id="社区动态"><a href="#社区动态" class="headerlink" title="社区动态"></a>社区动态</h2><p>蚂蚁对于这个项目的投入还是很大的, 应该由专门的人在投入这个项目, 更新频率还是相当快的</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/6.png" alt></p><p>但是贡献者还是比较少的, 应该目前看只有21贡献者, 基本上是蚂蚁金服内部员工.</p><p>之前看到他们2019年的<a href="https://github.com/sql-machine-learning/sqlflow/issues/327" target="_blank" rel="noopener">路标</a>, 2019年的目标预定是支持各种框架, 例如<strong>Calcite支持</strong>或者<strong>GPU TF支持</strong></p><p>总体来说2019年主要在完善功能, 但是后来这个ISSUE关闭了, 不确定19年能否完成这些内容.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SQLFlow目前看还处于原型阶段, 整体支持的能力还非常欠缺: GPU的支持, 模型的定义等功能目前好像都不具备.</p><p>就目前整体的设计, 我认为以下三点未来需要加强:</p><ol><li>模型定义接口太复杂了: 1. 实现一个模板;2. 实现一个模型算法;3. 实现一个submitter. 这套逻辑对于工程师可能比较简单(实际上定义地方太多, 也麻烦), 但是对于AI算法的人, 肯定不是用</li><li><code>AI Engine</code>和<code>SQL Engine</code>分离带来的性能问题, 这套架构的问题就是AI系统离数据远了一点, 所有数据都是通过<code>SQL Engine</code>计算而来, 而且是通过RPC获取的数据, 比起Spark这种直接在内存中获取数据, 这里会成为正式商用时候的大瓶颈点</li><li>数据分布式化工程量太大, 这其实是问题二的引申版, 未来肯定要实现数据分布化, AI计算分布式化, 这钟模式我还没有想到如何分布式化(<em>回去再好好想想</em>)</li></ol><p>另外对于这个项目的商业前景如何, 这个确实存疑的, 也许在阿里内部可能有这部分需求, 但对我们来说却不是.</p><p>2018年8月BigQuery出了ML之后, 我们也有计划跟进, 调研了<a href="https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html" target="_blank" rel="noopener">XGBoost On Spark</a>, 但最后还是没决定要做, 最主要的原因是没有客户明确需要这个能力.</p><blockquote><p> 值得表扬的是: 蚂蚁的文档和demo做的是真好, 做技术调研能遇到这样的项目, 确实让我省了不少功夫. </p></blockquote><h2 id="附录-Go语言环境安装"><a href="#附录-Go语言环境安装" class="headerlink" title="附录: Go语言环境安装"></a>附录: Go语言环境安装</h2><blockquote><p>SQLFlow的主编程语言为Go语言, 安装部署也相对方便, 不记录过程了, 只放置一些安装资源链接</p></blockquote><p><a href="https://dl.google.com/go/go1.12.9.windows-amd64.msi" target="_blank" rel="noopener">Go安装包</a></p><p><a href="https://www.jetbrains.com/go/download/#section=windows" target="_blank" rel="noopener">GoLand下载地址</a></p><p><a href="http://idea.lanyus.com/" target="_blank" rel="noopener">GoLand破解</a></p><p><a href="http://blog.lanyus.com/archives/174.html" target="_blank" rel="noopener">IDEA License Server搭建</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]番外:TensorServing例子</title>
      <link href="/2019/08/11/Kubeflow%E7%B3%BB%E5%88%97-%E7%95%AA%E5%A4%96-TensorServing%E4%BE%8B%E5%AD%90/"/>
      <url>/2019/08/11/Kubeflow%E7%B3%BB%E5%88%97-%E7%95%AA%E5%A4%96-TensorServing%E4%BE%8B%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>最近做产品的时候, 由算法团队写完AI算法模型, 由我们产品团队完成对工程化的改造, 然后再把API上线.</p><p>但是这个时候遇到沟通成本很高, AI算法大多是探索式的,  输入是比较少边的, 但是输出可能会多次变化, 这样对产品化人员又很麻烦, 本来就是很简单的代码, 就是需要不停的修改, 用来适配整体的API.</p><blockquote><p>这里有个前提: 尽量让算法的人员减少对产品的代码的感知, 毕竟他们不是专业的</p></blockquote><p>因此, 我查了一下开源社区对这个问题的处理方式, 最后还真查到了<code>TF Serving</code>这个工程, 决定调研一下是否能够解决我的问题.</p><p><code>TF Serving</code>是<a href="https://www.tensorflow.org/tfx" target="_blank" rel="noopener">TensorFlow Extended</a>工程的一个子工程, <code>TFX</code>是一个端到端的AI平台, 简单理解就是:</p><p><strong>他的关注点在于AI机器学习的工程化部分</strong></p><p><code>TFX</code>总共有4个模块:</p><ol><li>TensorFlow Data Validation: <em>TensorFlow Data Validation (TFDV) 能够帮助开发者大规模地理解、验证和监控机器学习数据。Google 每天都使用 TFDV 分析和验证 PB 级的数据，并且在帮助 TFX 用户维护机器学习流水线正常运行方面，TFDV 一贯表现良好</em></li><li>TensorFlow Transform: <em>在将机器学习应用于现实世界的数据集时，需要投入很多精力才能将数据预处理为合适的格式，其中包括在各种格式之间进行转换、对文本进行标记化和词干化、创建词汇表、执行归一化等各种数字操作。您可以使用 tf.Transform 完成所有这些操作</em></li><li>TensorFlow Model Analysis: <em>TensorFlow Model Analysis (TFMA) 让开发者能够计算和可视化模型的评估指标。在部署任何机器学习 (ML) 模型之前，机器学习开发者需要评估模型的性能，以确保其达到特定的质量阈值，并且能够针对所有相关数据切片展示出与预期相符的行为。例如，模型针对整个评估数据集的 AUC 可能是可接受的，但针对特定切片却表现不佳。TFMA 为开发者提供了深入了解其模型性能的工具</em></li><li>TensorFlow Serving: <em>机器学习 (ML) 应用系统必须支持模型版本控制（用于实现包含回滚选项的模型更新）和多个模型（用于实现通过 A/B 测试进行的实验），同时还要确保并发模型能够在硬件加速器（GPU 和 TPU）上以较低的延迟实现较高的吞吐量。TensorFlow Serving 每秒能为 Google 处理数千万次推断</em></li></ol><p>这4个模块都是很有用的点, 但这次我的重点在<code>TensorFlow Serving</code></p><h2 id="TensorFlow-Serving"><a href="#TensorFlow-Serving" class="headerlink" title="TensorFlow Serving"></a>TensorFlow Serving</h2><p>在网上的教程之中, 就一个<strong>提供模型</strong>的<a href="https://www.tensorflow.org/tfx/tutorials/serving/rest_simple" target="_blank" rel="noopener">章节</a>详细介绍了<code>TF Serving</code>的一个例子</p><p>按照Notebook的教程运行一下这个例子(不需要使用GPU, 使用CPU的速度就可以了), 我们简单的分解一下这个过程</p><blockquote><p>直接在本地安装一个Notebook就可以运行, 除了安装<code>tensorflow-model-server</code>可能需要翻墙, 需要手动设置一起, 其他的可以直接运行出来</p></blockquote><p>训练过程:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">  keras.layers.Conv2D(input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>), filters=<span class="number">8</span>, kernel_size=<span class="number">3</span>, </span><br><span class="line">                      strides=<span class="number">2</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'Conv1'</span>),</span><br><span class="line">  keras.layers.Flatten(),</span><br><span class="line">  keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax, name=<span class="string">'Softmax'</span>)</span><br><span class="line">])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">testing = <span class="literal">False</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=tf.train.AdamOptimizer(), </span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">model.fit(train_images, train_labels, epochs=epochs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用测试集推理</span></span><br><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line">print(<span class="string">'\nTest accuracy: &#123;&#125;'</span>.format(test_acc))</span><br></pre></td></tr></table></figure><p>实际上在<code>模型定义</code>的步骤之中, 模型的输入输出实际上已经定了,  但是这里的<strong>输入应该只能是tensor类型的</strong></p><p>使用工具, 查看查看模型输入输出:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!saved_model_cli show --dir &#123;export_path&#125; --all</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">MetaGraphDef with tag-set: &apos;serve&apos; contains the following SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[&apos;serving_default&apos;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&apos;input_image&apos;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 28, 28, 1)</span><br><span class="line">        name: Conv1_input:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&apos;Softmax/Softmax:0&apos;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: Softmax/Softmax:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure><p>创建Serving时候, 参数只要指定模型的路径即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup tensorflow_model_server \</span><br><span class="line">  --rest_api_port=8501 \</span><br><span class="line">  --model_name=fashion_model \</span><br><span class="line">  --model_base_path=<span class="string">"<span class="variable">$&#123;MODEL_DIR&#125;</span>"</span> &gt;server.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>使用客户端, 给<code>8501</code>端口发送请求, 地址为<code>http://localhost:8501/v1/models/fashion_model:predict</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;<span class="string">"content-type"</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">'http://localhost:8501/v1/models/fashion_model:predict'</span>, data=data, headers=headers)</span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">'predictions'</span>]</span><br><span class="line"></span><br><span class="line">show(<span class="number">0</span>, <span class="string">'The model thought this was a &#123;&#125; (class &#123;&#125;), and it was actually a &#123;&#125; (class &#123;&#125;)'</span>.format(</span><br><span class="line">  class_names[np.argmax(predictions[<span class="number">0</span>])], test_labels[<span class="number">0</span>], class_names[np.argmax(predictions[<span class="number">0</span>])], test_labels[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p>除了HTTP请求, <code>TFServing</code>还支持<code>GRPC</code>协议, 端口默认开在<code>8500</code>端口</p><p>GRPC的例子可以看github上的<a href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py" target="_blank" rel="noopener">例子</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从这个例子上来看, <code>TF Serving</code>并不能完全解决上面的问题, 因为模型的输入输出的定义肯定是矩阵模式, 而实际产品之中的输入是非常多样化的, 可以是一个npp文件也可以是一张图片, 显然还需要一个预处理的步骤,才能达到Serving的步骤.</p><p>但<code>TF Serving</code>也是用的:</p><ol><li>不需要写模型推理代码了, 只要统一使用了TensorFlow框架, 推理就简单了</li><li>它有简单的模型版本控制, 用于不同版本之间的升级测试等问题(实际上就是一个简单的版本编号而已)</li></ol><p>那么回过头来, 如果想要解决产品和算法团队之间的GAP呢?</p><p>我想到的方式是, <strong>使用Kubeflow的Pipeline + TFServing</strong>:</p><ol><li>将常见的输入输出算法转化为component, 直接可以使用</li><li>允许算法人员自己定义component, 使用方式和本地变成类似</li><li>Pipeline能够可视化的将各个component连接在一起</li><li>模型推理使用<code>TFServing</code>发布</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装nvidia-docker</title>
      <link href="/2019/08/11/%E5%AE%89%E8%A3%85nvidia-docker/"/>
      <url>/2019/08/11/%E5%AE%89%E8%A3%85nvidia-docker/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>测试环境为: ubuntu == 18.04</p></blockquote><p>官方文档: <a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0" target="_blank" rel="noopener">https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0</a>)</p><blockquote><p>ubuntu18.04版本需要安装的是V2.0版本, 网上文章大多是V1.0的, 在这个版本的ubutnu无法安装</p></blockquote><p>安装命令: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加源</span></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \</span><br><span class="line">  sudo apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | \</span><br><span class="line">  sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">sudo apt-get install nvidia-docker2</span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>启动镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-docker run -p 8888:8888   --dns 8.8.8.8 --dns 8.8.4.4 gcr.io/kubeflow-images-public/tensorflow-1.12.0-notebook-gpu:v0.5.0</span><br></pre></td></tr></table></figure><blockquote><p>—dns 8.8.8.8 —dns 8.8.4.4指定公网DNS, 不然容器里面无法访问网络</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker设置配置代理</title>
      <link href="/2019/08/11/Docker%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86/"/>
      <url>/2019/08/11/Docker%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>测试环境为: ubuntu == 18.04,  docker == 19.03.1</p></blockquote><p>有些docker容器是在Google Cloud上的是, 因此需要下载的时候, 需要配置代理才能访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建docker启动文件夹</span></span><br><span class="line">sudo mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line"><span class="comment">## 创建proxy配置文件</span></span><br><span class="line">sudo vim /etc/systemd/system/docker.service.d/http-proxy.conf</span><br></pre></td></tr></table></figure><p>写入以下配置项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"HTTP_PROXY=http://127.0.0.1:8118/"</span> <span class="string">"HTTPS_PROXY=http://127.0.0.1:8118/"</span> <span class="string">"NO_PROXY=localhost,127.0.0.1,registry.docker-cn.com,hub-mirror.c.163.com"</span></span><br></pre></td></tr></table></figure><blockquote><p><code>http://127.0.0.1:8118</code>是shadowsocks转出来的http端口</p><p>不需要走代理的镜像仓库, 在<code>NO_PROXY</code>里配置</p></blockquote><p>配置生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 刷新配置项</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line"><span class="comment"># 重启docker服务</span></span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"><span class="comment"># 查看docker配置项</span></span><br><span class="line">sudo systemctl show --property=Environment docker</span><br></pre></td></tr></table></figure><p>测试是否生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull gcr.io/kubeflow-images-public/tensorflow-1.12.0-notebook-cpu:v0.5.0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>翻墙代理设置</title>
      <link href="/2019/08/03/%E7%BF%BB%E5%A2%99%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/"/>
      <url>/2019/08/03/%E7%BF%BB%E5%A2%99%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近从同事手中借了一个<code>ShadowSocks</code>的账号, 用来翻墙使用, Windows和Mac使用的时候都比较简单, 因为有图形化的页面, 但是在Ubuntu上遇到不大不小的问题, 特意记录一下.</p><h2 id="安装客户端"><a href="#安装客户端" class="headerlink" title="安装客户端"></a>安装客户端</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果没有安装过pip, 请先安装</span></span><br><span class="line">sudo apt install python-pip</span><br><span class="line">sudo pip install shadowsocks</span><br><span class="line">sudo vim ~/shadowsocks.json</span><br></pre></td></tr></table></figure><p>填入你服务端的配置项:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"server"</span>:<span class="string">"远端ip"</span>,</span><br><span class="line">   <span class="attr">"server_port"</span>:<span class="number">2443</span>,</span><br><span class="line">   <span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line">   <span class="attr">"password"</span>:<span class="string">"密码"</span>,</span><br><span class="line">   <span class="attr">"timeout"</span>:<span class="number">30</span>,</span><br><span class="line">   <span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sslocal -c ~/shadowsocks.json</span><br></pre></td></tr></table></figure><p>这个时候, 如果你的shadowsocks版本是2.8.2的话, 就会出现以下的异常信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undefined symbol EVP_CIPHER_CTX_cleanup</span><br></pre></td></tr></table></figure><p>找到安装目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip list -v|grep shadowsocks</span><br><span class="line"><span class="comment"># shadowsocks       2.8.2             /home/wiiliam/miniconda3/lib/python3.6/site-packages                      pip </span></span><br><span class="line"><span class="built_in">cd</span> /home/wiiliam/miniconda3/lib/python3.6/site-packages/shadowsocks</span><br></pre></td></tr></table></figure><p>进入到安装后执行以下命令替换py的源码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">"s/cleanup/reset/g"</span>  crypto/openssl.py</span><br></pre></td></tr></table></figure><p>重新之后, 看到端口绑定日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">019-08-03 23:01:59 WARNING  warning: your timeout 30 seems too short</span><br><span class="line">2019-08-03 23:01:59 INFO     loading libcrypto from libcrypto.so.1.1</span><br><span class="line">2019-08-03 23:01:59 INFO     starting <span class="built_in">local</span> at 127.0.0.1:1080</span><br></pre></td></tr></table></figure><blockquote><p>注意: 在linux上1080端口监听的类型是<code>socks5</code>, 而如果使用windows版本, 1080监听的是<code>http</code>类型. 这个时候使用<code>lsof -i:1080</code>是看不到socks5监听进程的.</p></blockquote><h2 id="Chrome代理设置"><a href="#Chrome代理设置" class="headerlink" title="Chrome代理设置"></a>Chrome代理设置</h2><p>Chrome有比较好的插件: <code>SwitchyOmega</code></p><p>在配置页面直接导入配置项:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/shadowsocks/1.png" alt></p><p>然后修改<code>FWWed</code>的代理端口已经被正确设置</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/shadowsocks/2.png" alt></p><p>最后在Chrome的插件按钮上切换地址的时候, 使用<code>自动切换</code>即可</p><h2 id="Socks5代理转HTTP代理"><a href="#Socks5代理转HTTP代理" class="headerlink" title="Socks5代理转HTTP代理"></a>Socks5代理转HTTP代理</h2><p>国产的浏览器一般是不支持socks代理的, 所以需要讲sock5代理转为http代理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install privoxy</span><br><span class="line">sudo vi /etc/privoxy/config</span><br></pre></td></tr></table></figure><p>修改以下的配置项:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">isten-address localhost:8118</span><br><span class="line">forward-socks5t / 127.0.0.1:1080 .</span><br></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/privoxy restart</span><br></pre></td></tr></table></figure><h2 id="开机启动"><a href="#开机启动" class="headerlink" title="开机启动"></a>开机启动</h2><blockquote><p>ubuntu18.04不再使用initd管理系统，改用systemd,为了像以前一样，在/etc/rc.local中设置开机启动程序，需要以下几步：systemd默认读取/etc/systemd/system下的配置文件，该目录下的文件会链接/lib/systemd/system/下的文件。一般系统安装完/lib/systemd/system/下会有rc-local.service文件，即我们需要的配置文件</p></blockquote><p>链接服务并修改服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -fs /lib/systemd/system/rc-local.service /etc/systemd/system/rc-local.service</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /etc/systemd/system/</span><br><span class="line">sudo vim rc-local.service</span><br></pre></td></tr></table></figure><p>加入部分设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  SPDX-License-Identifier: LGPL-2.1+</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  This file is part of systemd.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  systemd is free software; you can redistribute it and/or modify it</span></span><br><span class="line"><span class="comment">#  under the terms of the GNU Lesser General Public License as published by</span></span><br><span class="line"><span class="comment">#  the Free Software Foundation; either version 2.1 of the License, or</span></span><br><span class="line"><span class="comment">#  (at your option) any later version.</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># This unit gets pulled automatically into multi-user.target by</span></span><br><span class="line"><span class="comment"># systemd-rc-local-generator if /etc/rc.local is executable.</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=/etc/rc.local Compatibility</span><br><span class="line">Documentation=man:systemd-rc-local-generator(8)</span><br><span class="line">ConditionFileIsExecutable=/etc/rc.local</span><br><span class="line">After=network.target</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/etc/rc.local start</span><br><span class="line">TimeoutSec=0</span><br><span class="line">RemainAfterExit=yes</span><br><span class="line">GuessMainPID=no</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 这部分要新添加</span></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">Alias=rc-local.service</span><br></pre></td></tr></table></figure><p>创建<code>/etc/rc.local</code>文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/rc.local</span><br></pre></td></tr></table></figure><p>填入以下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/sh -e</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># rc.local</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This script is executed at the end of each multiuser runlevel.</span></span><br><span class="line"><span class="comment"># Make sure that the script will "exit 0" on success or any other</span></span><br><span class="line"><span class="comment"># value on error.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In order to enable or disable this script just change the execution</span></span><br><span class="line"><span class="comment"># bits.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default this script does nothing.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 填写脚本区域</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"test"</span> &gt; /var/<span class="built_in">log</span>/ss.log</span><br><span class="line">/home/wiiliam/miniconda3/bin/python /home/wiiliam/miniconda3/bin/sslocal -c /home/wiiliam/shadowsocks.json &gt; /home/wiiliam/shadowsocks.log 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment">### 脚本区域结束</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><blockquote><p>注意这个脚本是以root用户启动的, 所以一定要确保root一定能访问相应的文件</p></blockquote><p>添加执行权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown 755 /etc/rc.local</span><br></pre></td></tr></table></figure><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kionf.com/2016/12/15/errornote-ss/" target="_blank" rel="noopener">SS客户端异常</a></p><p><a href="https://cao0507.github.io/2018/08/21/ubuntu%E9%85%8D%E7%BD%AEshadowsocks%E5%AE%A2%E6%88%B7%E7%AB%AF/" target="_blank" rel="noopener">HTTP代理</a></p><p><a href="https://github.com/FelisCatus/SwitchyOmega/wiki/GFWList" target="_blank" rel="noopener">Chrome设置SwitchyOmega</a></p><p><a href="https://blog.csdn.net/zhengchaooo/article/details/80202599" target="_blank" rel="noopener">开机启动设置</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes知识点</title>
      <link href="/2019/08/01/Kubernetes%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2019/08/01/Kubernetes%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="污点和容忍"><a href="#污点和容忍" class="headerlink" title="污点和容忍"></a>污点和容忍</h2><p>如果你不想在某个节点上, 执行任何Pod, 你可以通过Kubernetes的<strong>Taints</strong>特性实现</p><p>每个污点的组成如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key=value:effect</span><br></pre></td></tr></table></figure><p>每个污点有一个key和value作为污点的标签，其中value可以为空，effect描述污点的作用。当前taint effect支持如下三个选项：</p><ul><li><code>NoSchedule</code>：表示k8s将不会将Pod调度到具有该污点的Node上</li><li><code>PreferNoSchedule</code>：表示k8s将尽量避免将Pod调度到具有该污点的Node上</li><li><code>NoExecute</code>：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去</li></ul><p><strong>污点的设置和去除</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置污点</span></span><br><span class="line">kubectl taint nodes node1 key1=value1:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除污点</span></span><br><span class="line">kubectl taint nodes node1 key1:NoSchedule-</span><br></pre></td></tr></table></figure><p><strong>容忍</strong></p><p>设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。 但我们可以在Pod上设置容忍(Toleration)，意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上。</p><p>通过在Pod的spec中设置tolerations字段，给Pod设置上容忍点Toleration：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"key1"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Equal"</span></span><br><span class="line"><span class="attr">  value:</span> <span class="string">"value1"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoSchedule"</span></span><br><span class="line"><span class="attr">  tolerationSeconds:</span> <span class="number">3600</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"key1"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Equal"</span></span><br><span class="line"><span class="attr">  value:</span> <span class="string">"value1"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"key2"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><p><strong>参考</strong></p><p><a href="https://blog.frognew.com/2018/05/taint-and-toleration.html" target="_blank" rel="noopener">博客</a></p><h2 id="Kubernetes的调度器"><a href="#Kubernetes的调度器" class="headerlink" title="Kubernetes的调度器"></a>Kubernetes的调度器</h2><p>Kubernetes的默认调度器叫做<code>kube-scheduler</code>, 但是你可以自己实现一个调度器, 例如<a href="https://github.com/volcano-sh/volcano" target="_blank" rel="noopener">volcano</a></p><p>创建Pod的时候可以指定<code>spec.schedulerName</code>为自己的调度器</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">annotation-second-scheduler</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">multischeduler-example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  schedulerName:</span> <span class="string">my-scheduler</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">pod-with-second-annotation-container</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">k8s.gcr.io/pause:2.0</span></span><br></pre></td></tr></table></figure><p><strong>参考</strong></p><p><a href="https://k8smeetup.github.io/docs/tasks/administer-cluster/configure-multiple-schedulers/" target="_blank" rel="noopener">配置多个调度器</a></p><p><a href="https://zhuanlan.zhihu.com/p/47047550" target="_blank" rel="noopener">如何实现一个调度器</a></p><h2 id="Kubernetes-Operator入门"><a href="#Kubernetes-Operator入门" class="headerlink" title="Kubernetes Operator入门"></a>Kubernetes Operator入门</h2><p><a href="https://www.qikqiak.com/post/k8s-operator-101/" target="_blank" rel="noopener">参考</a></p><h2 id="强制删除资源"><a href="#强制删除资源" class="headerlink" title="强制删除资源"></a>强制删除资源</h2><p>问题：kubelet delete pod之后总处于Terminating，无法移除</p><p>解决：加参数 —force —grace-period=0，grace-period表示过渡存活期，默认30s，在删除POD之前允许POD慢慢终止其上的容器进程，从而优雅退出，0表示立即终止POD</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete po -n --force --grace-period=0</span><br></pre></td></tr></table></figure><h2 id="kubernetes的ipvs模式和iptables模式"><a href="#kubernetes的ipvs模式和iptables模式" class="headerlink" title="kubernetes的ipvs模式和iptables模式"></a>kubernetes的ipvs模式和iptables模式</h2><h3 id="什么是IPVS？"><a href="#什么是IPVS？" class="headerlink" title="什么是IPVS？"></a>什么是IPVS？</h3><p>IPVS (IP Virtual Server，IP虚拟服务器)是基于Netfilter的、作为linux内核的一部分实现传输层负载均衡的技术，通常称为第4层LAN交换。</p><p>IPVS集成在LVS(Linux Virtual Server)中，它在主机中运行，并在真实服务器集群前充当负载均衡器。IPVS可以将对TCP/UDP服务的请求转发给后端的真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。因此IPVS天然支持Kubernetes Service。</p><h3 id="为什么选择IPVS"><a href="#为什么选择IPVS" class="headerlink" title="为什么选择IPVS"></a>为什么选择IPVS</h3><p>随着kubernetes使用量的增长，其资源的可扩展性变得越来越重要。特别是对于使用kubernetes运行大型工作负载的开发人员或者公司来说，service的可扩展性至关重要。</p><p>kube-proxy是为service构建路由规则的模块，之前依赖iptables来实现主要service类型的支持，比如(ClusterIP和NodePort)。但是iptables很难支持上万级的service，因为iptables纯粹是为防火墙而设计的，并且底层数据结构是内核规则的列表。</p><p>kubernetes早在1.6版本就已经有能力支持5000多节点，这样基于iptables的kube-proxy就成为集群扩容到5000节点的瓶颈。举例来说，如果在一个5000节点的集群，我们创建2000个service，并且每个service有10个pod，那么我们就会在每个节点上有至少20000条iptables规则，这会导致内核非常繁忙。</p><p>基于IPVS的集群内负载均衡就可以完美的解决这个问题。IPVS是专门为负载均衡设计的，并且底层使用哈希表这种非常高效的数据结构，几乎可以允许无限扩容。</p><h3 id="IPVS-vs-IPTABLES区别"><a href="#IPVS-vs-IPTABLES区别" class="headerlink" title="IPVS vs. IPTABLES区别"></a>IPVS vs. IPTABLES区别</h3><p>IPVS模式在Kubernetes v1.8中引入，并在v1.9中进入了beta。 1.11中实现了GA(General Availability)。IPTABLES模式在v1.1中添加，并成为自v1.2以来的默认操作模式。 IPVS和IPTABLES都基于netfilter。 IPVS模式和IPTABLES模式之间的差异如下：</p><ul><li>IPVS为大型集群提供了更好的可扩展性和性能。</li><li>IPVS支持比iptables更复杂的负载平衡算法（最小负载，最少连接，位置，加权等）。</li><li>IPVS支持服务器健康检查和连接重试等。</li></ul><p><a href="https://blog.csdn.net/fanren224/article/details/86548398" target="_blank" rel="noopener">参考1</a></p><p><a href="https://zhuanlan.zhihu.com/p/37230013" target="_blank" rel="noopener">参考2</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华为云上安装helm</title>
      <link href="/2019/08/01/%E5%8D%8E%E4%B8%BA%E4%BA%91%E4%B8%8A%E5%AE%89%E8%A3%85helm/"/>
      <url>/2019/08/01/%E5%8D%8E%E4%B8%BA%E4%BA%91%E4%B8%8A%E5%AE%89%E8%A3%85helm/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Helm是Kubernetes生态系统中的一个软件包管理工具.</p><p>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</p><p>除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。</p><h2 id="Helm概念"><a href="#Helm概念" class="headerlink" title="Helm概念"></a>Helm概念</h2><ul><li>Helm: Kubernetes的应用打包工具，也是命令行工具的名称。</li><li>Tiller: Helm的服务端，部署在Kubernetes集群中，用于处理Helm的相关命令。</li><li>Chart: Helm的打包格式，内部包含了一组相关的kubernetes资源。</li><li>Repoistory: Helm的软件仓库，repository本质上是一个web服务器，该服务器保存了chart软件包以供下载，并有提供一个该repository的chart包的清单文件以供查询。在使用时，Helm可以对接多个不同的Repository。</li><li>Release: 使用Helm install命令在Kubernetes集群中安装的Chart称为Release。</li></ul><p>下图展示这个概念之间的联系:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/helm-architecture.png" alt></p><h2 id="华为云上安装"><a href="#华为云上安装" class="headerlink" title="华为云上安装"></a>华为云上安装</h2><h3 id="Helm安装"><a href="#Helm安装" class="headerlink" title="Helm安装"></a>Helm安装</h3><p>从Github的<a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">发布页</a>或者<a href="https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz" target="_blank" rel="noopener">GoogleCloud</a>下载</p><blockquote><p>helm v2.11版本在K8s v1.11.7测试通过</p></blockquote><p>上传到客户端节点后, 解压并将文件拷贝到系统路径之中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf helm-v2.11.0-linux-amd64.tar.gz</span><br><span class="line">mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/helm</span><br></pre></td></tr></table></figure><h3 id="Helm初始化"><a href="#Helm初始化" class="headerlink" title="Helm初始化"></a>Helm初始化</h3><p>创建一个tiller的service_account: <code>tiller_service_account.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>创建角色</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f tiller_service_account.yaml</span><br></pre></td></tr></table></figure><p>Helm初始化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init --service-account tiller --skip-refresh</span><br><span class="line"><span class="comment">## 更新镜像地址, 最好你从官网下载到tiller镜像, 再将镜像传入到华为云的SWR, 此处填写SWR地址</span></span><br><span class="line">helm init --tiller-image swr.cn-north-1.myhuaweicloud.com/hzw/tiller:v2.11.0 --upgrade</span><br></pre></td></tr></table></figure><blockquote><p>docker tag registry.cn-hangzhou.aliyuncs.com/acs/tiller:v2.11.0 swr.cn-north-1.myhuaweicloud.com/hzw/tiller:v2.11.0</p></blockquote><p>执行<code>helm version</code>查看结果, 发现<code>Error: could not find a ready tiller pod</code>错误</p><p>查找原因</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces|grep tiller</span><br><span class="line"><span class="comment"># 如果没找到pod, 说明前面的步骤还有问题</span></span><br><span class="line">kubectl describe pod <span class="variable">$Pod_Name</span> -n kube-system</span><br></pre></td></tr></table></figure><p>发现问题还是<code>Error: ImagePullBackOff</code></p><p>问题和之前一样, 华为云的k8s拉取镜像需要<code>imagePullSecret</code></p><p>编辑<code>tiller-deploy</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit deploy tiller-deploy -n kube-system</span><br></pre></td></tr></table></figure><p>将以下内容加入到对应的模块之中</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      imagePullSecrets:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">default-secret</span></span><br></pre></td></tr></table></figure><p>过段时间之后,发现tiller已经正常</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@/home]<span class="comment"># kubectl get pod -n kube-system|grep tiller</span></span><br><span class="line">tiller-deploy-759d874f-4dgmv     1/1       Running   0          56m</span><br></pre></td></tr></table></figure><p>Helm安装和初始化工作已经完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@gpu-01-client volcano]<span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.11.0"</span>, GitCommit:<span class="string">"2e55dbe1fdb5fdb96b75ff144a339489417b146b"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.11.0"</span>, GitCommit:<span class="string">"2e55dbe1fdb5fdb96b75ff144a339489417b146b"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="参考说明"><a href="#参考说明" class="headerlink" title="参考说明"></a>参考说明</h2><p><a href="https://blog.csdn.net/wzygis/article/details/84346573" target="_blank" rel="noopener">安装指南</a></p><p><a href="https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0024.html" target="_blank" rel="noopener">华为云官网指南</a></p><p><a href="https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/" target="_blank" rel="noopener">Helm介绍</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu使用Git报错</title>
      <link href="/2019/07/30/Ubuntu%E4%BD%BF%E7%94%A8Git%E6%8A%A5%E9%94%99/"/>
      <url>/2019/07/30/Ubuntu%E4%BD%BF%E7%94%A8Git%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近使用公司的开发机器(ubuntu系统), 执行<code>git clone</code>一直会出现如下错误:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: RPC failed; curl 56 GnuTLS recv error (-110): The TLS connection was non-properly terminated.</span><br></pre></td></tr></table></figure></p><p>据说产生这个错误的原因, ubuntu上使用的https认证组件有问题, 使用如下步骤可以解决该问题.</p><p>大部分的内容参考自<a href="https://zhuanlan.zhihu.com/p/53961303" target="_blank" rel="noopener">这篇博客</a>,但是文章的步骤在公司的机器还是有一些问题,我会注明.</p><p><strong>安装必要依赖</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential fakeroot dpkg-dev</span><br><span class="line">sudo apt-get install libcurl4-openssl-dev</span><br></pre></td></tr></table></figure></p><p><strong>下载源码并重新编译安装</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建临时目录</span></span><br><span class="line">mkdir ~/git-rectify</span><br><span class="line"><span class="built_in">cd</span> ~/git-rectify</span><br><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">apt-get <span class="built_in">source</span> git</span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">apt-get build-dep git</span><br><span class="line"><span class="comment"># 进入源码目录</span></span><br><span class="line"><span class="built_in">cd</span> git-2*</span><br><span class="line"><span class="comment"># 修改配置文件</span></span><br><span class="line">sed -i -- <span class="string">'s/libcurl4-gnutls-dev/libcurl4-openssl-dev/'</span> ./debian/control</span><br><span class="line">sed -i -- <span class="string">'/TEST\s*=\s*test/d'</span> ./debian/rules</span><br><span class="line"><span class="comment"># 重新编译(注意要加入`-uc -us`,不然会出现`secret key not available`错误)</span></span><br><span class="line">dpkg-buildpackage -rfakeroot -b -uc -us</span><br><span class="line"><span class="comment"># 返回上一级,重新安装</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">dpkg -i git_2*.deb</span><br></pre></td></tr></table></figure></p><p><strong>锁住git不更新</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果不锁定, 下次更新git的时候, 会重新覆盖</span></span><br><span class="line">sudo apt-mark hold git</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>蓝绿发布/灰度发布和AB测试</title>
      <link href="/2019/07/30/%E8%93%9D%E7%BB%BF%E5%8F%91%E5%B8%83-%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E5%92%8CAB%E6%B5%8B%E8%AF%95/"/>
      <url>/2019/07/30/%E8%93%9D%E7%BB%BF%E5%8F%91%E5%B8%83-%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E5%92%8CAB%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>之前想做过一个多版本发布的系统,  当时了解了不少微服务部署的概念, 其中就包括蓝绿发布, 灰度发布以及AB测试.</p><p>但是当时对这些概念理会的不是很深, 大致认为他们都是差不多的东西. 这次研究ServiceMesh的时候, 这些概念又出现了, 所以现在写个文章把这些概念理清楚.</p><blockquote><p>内容不少节选自<a href="https://cloud.tencent.com/developer/article/1449209" target="_blank" rel="noopener">这篇文章</a>和<a href="http://www.bewindoweb.com/231.html" target="_blank" rel="noopener">这篇文章</a></p></blockquote><h2 id="蓝绿发布"><a href="#蓝绿发布" class="headerlink" title="蓝绿发布"></a>蓝绿发布</h2><blockquote><p>蓝绿部署中，一共有两套系统：一套是正在提供服务系统，标记为“绿色”；另一套是准备发布的系统，标记为“蓝色”。两套系统都是功能完善的，并且正在运行的系统，只是系统版本和对外服务情况不同。</p></blockquote><p>蓝绿发布最大的特点就是: </p><ol><li><strong>两套运行环境</strong></li><li><strong>流量一把切</strong></li></ol><p>特别要注意的, 两套运行环境只要是指<strong>无状态服务</strong>那个一部分,  对于原数据库或者其他有状态的实例, 始终使用的都是同一套系统.</p><blockquote><p>如果想要将这些<strong>有状态实例</strong>进行切换, 必须保证数据同步的, 这样来就必然会涉及到了CAP理论, 整个系统复杂性就高了.</p></blockquote><h2 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a>灰度发布</h2><blockquote><p>灰度发布又叫做<strong>金丝雀发布</strong>，以前矿工下矿洞前，会放一只金丝雀去试探是否有瓦斯（金丝雀对瓦斯很敏感），映射到这里就是先发布一小部分来试探整体是否能够正常运行，如果能正常运行则进行完全部署的发布方式，目前仍然是不少成长型技术组织的主流发布方式</p></blockquote><p>灰度发布最大特点是:</p><ol><li>流量渐切</li><li>影响可控</li></ol><p>蓝绿发布的时候, <strong>流量一把切</strong>导致变更出现严重bug的时候, 是无法评估影响的, 因为你无法知道变更时间窗口之中, 有多少用户使用了你的服务.</p><p>而灰度发布可以指定新版本的用户, 收集<strong>金丝雀</strong>的反馈意见, 然后进行决策是否放开流程, 或者回退版本.</p><h2 id="A-B测试"><a href="#A-B测试" class="headerlink" title="A/B测试"></a>A/B测试</h2><blockquote><p>A/B测试指的是同时上线V1和V2版本，根据一定条件将流量分别导入V1和V2版本，收集感兴趣的数据，来对比产品功能的效果。</p></blockquote><p>A/B测试经常会和灰度发布在一起说, 因为:</p><ol><li>两者都有多个版本(蓝绿发布一般只有两个版本, 而灰度可以存在多个版本)</li><li>两者都有流量路由控制</li></ol><p>但是A/B测试是一种测试方法, <strong>关注的是旧版本和新版本的效果好坏，比如流量转化率、用户体验等等</strong><br>因此, 两者不是一个维度的事情, 只是一般合起来使用: 应用通过<strong>灰度发布</strong>, 然后进行<strong>A/B测试</strong>,发现问题, 进行改进.</p><p>游戏领域经常有<strong>内测活动</strong>或者<strong>体验服</strong>, 是一个典型的A/B测试</p><blockquote><p>A/B测试存在的问题是<strong>金丝雀可能会死</strong>, 因此还有一种通过<strong>流量回放</strong>技术做的测试方法, 将现网的流程全部回放, 测试新版本的时候将现网的业务全部测试一遍. 但这种技术实现复杂, 因为客户的应用不一定全是幂等的, 所以必须同步备份数据库以及运行环境, 还得有环境版本控制. 总之, 不是那种质量非常高的应用, 一般不会去搞这么一套系统的.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>比较两种发布模式:</p><ol><li>蓝绿发布的实现更加简单, 但是部署资源占用多, 并且影响不可控</li><li>灰度发布的实现确实复杂, 但影响小, 部署也可以采用滚动升级的方式</li></ol><p>现在一般大多采用灰度发布为多.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kube-Debug定位工具</title>
      <link href="/2019/07/28/Kube-Debug%E5%AE%9A%E4%BD%8D%E5%B7%A5%E5%85%B7/"/>
      <url>/2019/07/28/Kube-Debug%E5%AE%9A%E4%BD%8D%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>之前逛技术博客的时候, 看了一个<code>kubectl-debug</code>的<a href="https://aleiwu.com/post/kubectl-debug-intro/" target="_blank" rel="noopener">介绍</a>, 可以使用外部的工具, 来处理容器内部的信息, 这个能力非常有用. </p><p>因为容器制作的时候有个原则就是<strong>容器尽量的小</strong>, 导致基础镜像上连一些基础的命令都没有. 例如<code>vi</code>, <code>ping</code>这些基本的命令, 都可能要自己安装, 但是很多云环境上, 又没法正常的访问互联网, 导致严重浪费时间.</p><h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><p>实现原理说起来很简单, 就是用到容器注入的方式处理,  假设你已经启动了一个<code>TARGET_CONTAINER</code>容器, 它的ID为<code>TARGET_ID</code>, 你可以将<code>busybox</code>的镜像注入到<code>TARGET_CONTAINER</code>里面, 此时容器空间内的命令为<code>busybox</code>镜像所提供的, 但是网络或者进程空间全是<code>TARGET_CONTAINER</code>的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TARGET_ID=666666666</span><br><span class="line"><span class="comment"># 加入目标容器的 network, pid 以及 ipc namespace</span></span><br><span class="line">docker run -it --network=container:<span class="variable">$TARGET_ID</span> --pid=container:<span class="variable">$TARGET_ID</span> --ipc=container:<span class="variable">$TARGET_ID</span> busybox</span><br></pre></td></tr></table></figure><p>这个技术点确实很牛逼, 之前都不了解, 后面在<code>kubernete</code>上的实现, 就顺理成章了, 只需要通过API接口将需要的<code>TARGET_ID</code>查询出来, 注入镜像即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前看这个<code>kubectl-debug</code>只是个人项目,  可能会有一些场景支持的并不好, 特别是各个云上的安全系统. 所以有需要可以拿过来改改代码试用一下, 如果偶尔使用的话, 直接在<code>kubernete</code>的节点上使用容器注入会更快和可控一点.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]Pipleline介绍</title>
      <link href="/2019/07/21/Kubeflow%E7%B3%BB%E5%88%97-Pipleline%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/07/21/Kubeflow%E7%B3%BB%E5%88%97-Pipleline%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p><code>Pipeline</code>在Kubeflow里面是一个最重要的能力, 因为需要它来串流整个机器学习任务的组件.<br>因此我们首先介绍一下<code>pipeline</code>的定义: 它是一个工作流平台，能够编译部署机器学习的工作流, 可以定义复杂的数据DAG流程, 并提供可视化的流程展示和结果展示.</p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p><code>pipelines</code>实现了一个工作流模型。所谓工作流，或者称之为流水线（pipeline），可以将其当做一个有向无环图（DAG）。其中的每一个节点，在<code>pipelines</code> 的语义下被称作组件（component）。组件在图中作为一个节点，其会处理真正的逻辑，比如预处理，数据清洗，模型训练等等。每一个组件负责的功能不同，但有一个共同点，即组件都是以 Docker 镜像的方式被打包，以容器的方式被运行的。这也是与 kubeflow 社区的 Run ML on Kubernetes 这一愿景相统一的。</p><p>实验（experiment）是一个工作空间，在其中可以针对流水线尝试不同的配置。运行（run）是流水线的一次执行，用户在执行的过程中可以看到每一步的输出文件，以及日志。步（step）是组件的一次运行，步与组件的关系就像是运行与流水线的关系一样。步输出工件（step output artifacts）是在组件的一次运行结束后输出的，能被系统的前端理解并渲染可视化的文件。</p><p><strong>Kubeflow Pipeline中概念列表</strong>:</p><ol><li>Pipeline: 定义一组操作的流水线，其中每一步都由component组成。 背后是一个Argo的模板配置</li><li>Component: 一个容器操作，可以通过pipeline的sdk 定义。每一个component 可以定义定义输出（output）和产物（artifact）， 输出可以通过设置下一步的环境变量，作为下一步的输入， artifact 是组件运行完成后写入一个约定格式文件，在界面上可以被渲染展示。</li><li>Graph: 就是上面看到流程图, 一个pipeline可以转化为一个DAG图</li><li>Experiment:  可以看做一个工作空间，管理一组运行任务</li><li>Run: 真实任务,  pipeline真正在K8s上实例化的产物, 会启动多个对应的Pod, 开始真正的计算</li><li>Recurring Run: 定时任务, 可以由<code>Run Trigger</code>触发</li><li>Step: Component对应实体, 就是上图之中的方框, 应该是一个Step会真实对应一个K8s的Pod</li></ol><h2 id="官方案例"><a href="#官方案例" class="headerlink" title="官方案例"></a>官方案例</h2><p>官方的<strong>XGBoost - Training with Confusion Matrix</strong>案例:</p><h3 id="流程可视化页面"><a href="#流程可视化页面" class="headerlink" title="流程可视化页面:"></a>流程可视化页面:</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/2.png" alt></p><h3 id="提交任务页面"><a href="#提交任务页面" class="headerlink" title="提交任务页面:"></a>提交任务页面:</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/6.png" alt></p><h3 id="Artifacts页面"><a href="#Artifacts页面" class="headerlink" title="Artifacts页面"></a>Artifacts页面</h3><p>运行起来的时候, 在Artifacts之中看到一些metric信息(这些信息需要在流程之中自定义)</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/3.png" alt>  <img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/4.png" alt></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/5.png" alt></p><h2 id="Pipeline的流程及架构"><a href="#Pipeline的流程及架构" class="headerlink" title="Pipeline的流程及架构"></a>Pipeline的流程及架构</h2><p>使用Pipeline一般会有以下的步骤:</p><ol><li>使用python代码编写pipeline定义代码</li><li>通过编译脚本编译生成压缩包</li><li>上传压缩文件, 创建pipeline</li><li>使用该pipeline, 创建任务, 并输入指定的参数值</li><li>查看任务完成状态及输出结果</li></ol><p>Python代码定义再放在下一章节介绍, 这个章节介绍2-5步骤后台实现的流程. </p><p>下面先介绍两个基础知识:</p><h3 id="Argo"><a href="#Argo" class="headerlink" title="Argo"></a>Argo</h3><p>我们在Pipeline的页面上可以看到一个<code>Source</code>的子页面, 里面的内容是这个流程yaml格式的定义文件:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/7.png" alt></p><p>可以看到叫做<a href="https://argoproj.github.io/" target="_blank" rel="noopener">argo</a>的组件,  它是一个开源的基于容器的工作流引擎，实现了一个K8S的CRD(用户自定义的资源):</p><ul><li>用容器实现工作流的每一个步骤</li><li>用DAG的形式描述多个任务之间的关系的依赖</li><li>支持机器学习和数据处理中的计算密集型任务</li></ul><p>实际上pipeline上传的就是argo的配置文件, 由这个配置文件来定义整个流程任务.</p><p>这个配置文件需要<code>pipeline sdk</code>对用户Python代码进行编译产生.</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/8.png" alt></p><p>pipeline的架构, 主要由以下模块组成:</p><ol><li>编译模块(SDK), 将Python代码转化为argo配置文件</li><li>页面模块(Web Server), 用于创建job以及监控任务运行结果</li><li>存储模块, 包含历史任务元数据信息, Artifact的数据缓存</li><li>服务模块, 一个由go编写的后端，提供kubernetes ApiServer 风格的Restful API。处理前端以及SDK发起的操作请求。 Pipeline/Experiment 之类的请求会直接存入元数据。和Run 相关的请求除了写入元数据以外还会通过APIServer 同步操作Argo实例。</li><li>控制模块:  Argo模块注册在K8s之中, 将argo的配置文件翻译为真正的K8s执行流程</li></ol><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><ol><li>客户编写python代码, 经过编译之后生产argo配置项</li><li>客户调用上传接口(可以是网页调用), 上传argo配置项到<code>Pipeline Service</code></li><li><code>Pipeline Service</code>将元数据信息存储到<code>Meta Service</code>之中</li><li>客户触发任务执行,  <code>Pipeline Service</code>调用请求到<code>K8s API Service</code></li><li><code>K8s API Service</code>根据资源类型(<code>argo</code>资源), 触发Argo的编排调度框架</li><li>框架完成任务启动之后, 并将部分Artifact写入到<code>Artifact Storage</code>之中</li><li>pipeline时刻监控K8s的任务信息, 并在更新任务状态</li></ol><h2 id="GATK最佳实践案例"><a href="#GATK最佳实践案例" class="headerlink" title="GATK最佳实践案例"></a>GATK最佳实践案例</h2><p>下面以GATK最佳实践为例, 看一下如何实现一个流程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> kfp <span class="keyword">import</span> dsl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">align_fastq</span><span class="params">(fastq1, fastq2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">'aligin_fastq_with_bwa'</span>,</span><br><span class="line">        image=<span class="string">'swr.cn-north-5.myhuaweicloud.com/kubeflow/hzw-bwa:1.0'</span>,</span><br><span class="line">        command=[<span class="string">'sh'</span>, <span class="string">'-c'</span>],</span><br><span class="line">        arguments=[<span class="string">"/bwa mem -t 4 -R '@RG\\tID:foo\\tLB:bar\\tPL:illumina\\tPU:illumina\\tSM:NA12878' /data/ref/hg19.fa $0 $1 &gt; /data/sample/test.sam;echo '/data/sample/test.sam' &gt; /output.txt"</span>, fastq1, fastq2],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">'output'</span>: <span class="string">'/output.txt'</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">'/data'</span>: dsl.PipelineVolume(pvc=<span class="string">"cce-sfs-notebook"</span>, name=<span class="string">"notebook-pvc"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conver_sam</span><span class="params">(samfile)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">'convert sam into bam'</span>,</span><br><span class="line">        image=<span class="string">'swr.cn-north-5.myhuaweicloud.com/kubeflow/samtools:latest'</span>,</span><br><span class="line">        command=[<span class="string">'sh'</span>, <span class="string">'-c'</span>],</span><br><span class="line">        arguments=[<span class="string">'samtools sort -@ 4  -O bam -o /data/sample/test.bam $0; echo "/data/sample/test.bam" &gt; /output.txt'</span>, samfile],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">'output'</span>: <span class="string">'/output.txt'</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">'/data'</span>: dsl.PipelineVolume(pvc=<span class="string">"cce-sfs-notebook"</span>, name=<span class="string">"notebook-pvc"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">markdup</span><span class="params">(bamfile)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">'mark dup'</span>,</span><br><span class="line">        image=<span class="string">'swr.cn-north-5.myhuaweicloud.com/kubeflow/gatk:latest'</span>,</span><br><span class="line">        command=[<span class="string">'sh'</span>, <span class="string">'-c'</span>],</span><br><span class="line">        arguments=[<span class="string">'gatk MarkDuplicates -I $0 -O /data/sample/test.markup.bam -M /data/sample/test.metrics.txt; echo "/data/sample/test.markup.bam" &gt; /output.txt'</span>, bamfile],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">'output'</span>: <span class="string">'/output.txt'</span></span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">'/data'</span>: dsl.PipelineVolume(pvc=<span class="string">"cce-sfs-notebook"</span>, name=<span class="string">"notebook-pvc"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bqsr</span><span class="params">(marddup)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">'bqsr'</span>,</span><br><span class="line">        image=<span class="string">'swr.cn-north-5.myhuaweicloud.com/kubeflow/gatk:latest'</span>,</span><br><span class="line">        command=[<span class="string">'sh'</span>, <span class="string">'-c'</span>],</span><br><span class="line">        arguments=[<span class="string">'gatk BaseRecalibrator -I $0 -O /data/sample/test.table -R /data/ref/hg19.fa.gz --known-sites /data/ref/1000g_omni2.5.hg19.sites.vcf.gz;'</span></span><br><span class="line">                   <span class="string">'gatk ApplyBQSR -R /data/ref/hg19.fa.gz -I $0 -O /data/sample/test.bqsr.bam --bqsr-recal-file /data/sample/test.table;'</span></span><br><span class="line">                   <span class="string">'echo "/data/sample/test.bqsr.bam" &gt; /output.txt'</span>, marddup],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">'data'</span>: <span class="string">'/output.txt'</span></span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">'/data'</span>: dsl.PipelineVolume(pvc=<span class="string">"cce-sfs-notebook"</span>, name=<span class="string">"notebook-pvc"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">haplotype</span><span class="params">(bqsr)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">'haplotype'</span>,</span><br><span class="line">        image=<span class="string">'swr.cn-north-5.myhuaweicloud.com/kubeflow/gatk:latest'</span>,</span><br><span class="line">        command=[<span class="string">'sh'</span>, <span class="string">'-c'</span>],</span><br><span class="line">        arguments=[<span class="string">'gatk HaplotypeCaller -I $0 -O /data/sample/test.vcf.gz -R /data/ref/hg19.fa.gz;'</span></span><br><span class="line">                   <span class="string">'echo "/data/sample/test.vcf.gz" &gt; /output.txt'</span>, bqsr],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">'data'</span>: <span class="string">'/output.txt'</span></span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">'/data'</span>: dsl.PipelineVolume(pvc=<span class="string">"cce-sfs-notebook"</span>, name=<span class="string">"notebook-pvc"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@dsl.pipeline(</span></span><br><span class="line">    name=<span class="string">"Gatk Basic pipeline"</span>,</span><br><span class="line">    description=<span class="string">"A Basic pipeline for gatk."</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipeline</span><span class="params">(fastq1=<span class="string">'/data/sample/200M_1_NA12878_clean_1.fastq.gz'</span>, fastq2=<span class="string">'/data/sample/200M_1_NA12878_clean_2.fastq.gz'</span>)</span>:</span></span><br><span class="line">    samfile = align_fastq(fastq1, fastq2)</span><br><span class="line">    bamfile = conver_sam(samfile.output)</span><br><span class="line">    markfile = markdup(bamfile.output)</span><br><span class="line">    bqsr_file = bqsr(markfile.output)</span><br><span class="line">    haplotype(bqsr_file.output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> kfp.compiler <span class="keyword">as</span> compiler</span><br><span class="line">    compiler.Compiler().compile(pipeline, __file__ + <span class="string">".tar.gz"</span>)r</span><br></pre></td></tr></table></figure><ol><li><code>dsl</code>就是pipeline sdk,  由<code>@dsl.pipeline</code>标识pipeline的定义: 下一个函数即为定义函数, 函数的参数列表翻译为argo的input字段</li><li><code>dsl.ContainerOp</code>定义了镜像操作, 在这个例子之中有5个<code>ContainerOp</code>就会对应五个step, 启动5个pod进行计算.</li><li><code>dsl.PipelineVolume</code>定义了磁盘挂载信息, 作为参数输入到<code>ContainerOp</code>之中, 说明将该名字的PVC挂载到镜像里之中, 挂载目录为<code>/data</code></li><li><code>bamfile = conver_sam(samfile.output)</code>定义了依赖关系, 说明<code>conver_sam</code>依赖<code>align_fastq</code>的输出, 这个例子是个串行任务,  pipeline支持并发支持, 实现如下所示<code>bamfile = conver_sam(samfile1.output, samfile2.output)</code></li></ol><p>最后, 执行<code>main</code>函数, 可以生成一个tar.gz包, 将这个包上传到流程页面上, 就能开始运行任务.</p><h2 id="Pipeline的SDK简要说明"><a href="#Pipeline的SDK简要说明" class="headerlink" title="Pipeline的SDK简要说明"></a>Pipeline的SDK简要说明</h2><p><a href="https://kubeflow-pipelines.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">API文档</a>之中详细写明了SDK目前的功能, 这儿就简单啰嗦一句.</p><p>目前SDK只支持如下几个模块的功能:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/18.png" alt="img"></p><ol><li>首先是<code>compiler</code>模块, 主要是将python代码转化为<code>argo</code>的yaml配置项</li><li><code>components</code>模型实现如何导入外部模块, 以及如何构建模块</li><li><code>dsl</code>是最重要的模块, 定义了<code>ContainerOp</code>以及<code>VolumeOp</code>等真正和K8s交互的模块</li><li><code>client</code>的模块主要是如何提交任务的模块</li><li><code>notebook</code>目前还是空的</li><li><code>extension</code>主要是云上的扩展模块, 目前主要支持谷歌亚马逊和微软的云</li></ol><p>整个SDK的代码量非常少, 可以直接看<a href="https://github.com/kubeflow/pipelines/tree/master/sdk/python/kfp" target="_blank" rel="noopener">源码</a>了解更多的内容.</p><h2 id="完结撒花"><a href="#完结撒花" class="headerlink" title="完结撒花"></a>完结撒花</h2><p>走马观花一样的看了一下<code>Pipleline</code>的能力, 接下来总结一下优点和缺点:</p><ol><li>整体上<code>pipeline</code>有好的编程入口, 比较适合程序员, 但缺乏页面定制能力, 无法面向非程序员群体</li><li>有简单的任务调度能力, 支持定时任务, 但是功能有限</li><li>整体架构简洁明了,  但微服务众多, 运维是个压力</li><li>整体来说, pipeline功能还不完善, 需要深度定制, 但K8s + AI是趋势, 未来肯定会越来越好, 越来越庞大.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>缓存一致性问题</title>
      <link href="/2019/07/14/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"/>
      <url>/2019/07/14/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缓存读"><a href="#缓存读" class="headerlink" title="缓存读"></a>缓存读</h2><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/cache_consistency/1.png" alt></div></p><h2 id="缓存更新"><a href="#缓存更新" class="headerlink" title="缓存更新"></a>缓存更新</h2><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/cache_consistency/2.png" alt></div></p><h2 id="缓存并发问题"><a href="#缓存并发问题" class="headerlink" title="缓存并发问题"></a>缓存并发问题</h2><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/cache_consistency/3.png" alt></div></p><p>缓存系统存在根本的问题是<strong>缓存的读写和数据库读写不是原子的</strong>, 因此它必然会出现并发的问题.</p><h2 id="缓存并发解决思路"><a href="#缓存并发解决思路" class="headerlink" title="缓存并发解决思路"></a>缓存并发解决思路</h2><h3 id="引入全局锁"><a href="#引入全局锁" class="headerlink" title="引入全局锁"></a>引入全局锁</h3><p>例如<code>Redis</code>就有锁机制, 通过<code>redis</code>实现全局锁(行锁),  更新的时候设置排它锁, 不允许读取操作即可.</p><p>这就和数据库之中的<code>悲观锁</code>是一个性质的, 这种系统最大的问题就是吞吐量限制, 每次必须查询锁的状态</p><h3 id="引入消息队列"><a href="#引入消息队列" class="headerlink" title="引入消息队列"></a>引入消息队列</h3><p>或者使用<code>乐观锁</code>的实现方式, 就是引入消息队列,  消息队列相对于全局锁的优势在于: </p><ol><li>可以合并一部分的更新操作, 例如2次更新之间没有任何读取, 就可以将更新合并</li><li>可以和流控系统结合, 消息队列可以用于请求的流控, 通过流控系统可以实现缓存读取的分布式化</li></ol><p>缺点就是, 系统架构越来越复杂了, 流控系统的可靠性成为系统的关键.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]总览介绍</title>
      <link href="/2019/07/14/Kubeflow%E7%B3%BB%E5%88%97-%E6%80%BB%E8%A7%88%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/07/14/Kubeflow%E7%B3%BB%E5%88%97-%E6%80%BB%E8%A7%88%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="什么是Kubeflow"><a href="#什么是Kubeflow" class="headerlink" title="什么是Kubeflow"></a>什么是Kubeflow</h2><p>对于<a href="https://www.kubeflow.org/" target="_blank" rel="noopener">官网</a>的定义:</p><blockquote><p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. </p></blockquote><p>我们可以看出<code>kubeflow</code>是在<code>Kubernetes</code>之上构建<code>Machine Learning</code>的项目,  他本身不会创建任何AI或者Kubernetes的服务,  而是为了<code>Machine Learning</code>项目构建的更加简单, 更加可扩展.</p><p>为了更好的学习Kubeflow, 我们需要有以下的基础知识:</p><ol><li>容器技术, 包含docker使用, 镜像制作, Kubernetes 任务提交等一系列的Paas能力</li><li>AI技术, 包括各种AI框架, 以及分布式AI模型等</li><li>Python语言, kubeflow的代码库大多以Python编写</li></ol><h2 id="Kubeflow有什么"><a href="#Kubeflow有什么" class="headerlink" title="Kubeflow有什么"></a>Kubeflow有什么</h2><p><code>Kubeflow</code>有很多的组件,  可以在官网文档的<a href="https://master.kubeflow.org/docs/components/" target="_blank" rel="noopener">Components</a>模块之中找到所有能力组件, 我把这些模块整理了一下, 画出脑图, 如下:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/1.png" alt></p><p><strong>左边的一列</strong>为kubeflow面向机器学习领域提供的能力, 包括模型的训练推理以及自动学习的超参调优组件, Kubeflow已经支持了市面上大多数的训练和推理框架, 其中最完善的就是google自己家的<code>TensorFlow</code>.</p><p>当然也是最重要的一个模块, 毕竟Kubeflow的名字有一般来自于它.</p><p><strong>右边的一列</strong>为Kubeflow的基础能力, 主要为pipeline, notebook, fairing以及一些公共的组件, 例如元数据管理等, 这个部分是围绕AI能力外围的基础组件, 主要关注于工程部分, 例如使用pipeline进行编排, 使用notebook进行开发等.</p><p>下面准备简单介绍一下这几个模块,  某些组件特别大, 我准备单独写个文章介绍, 这里就会一笔带过. </p><h3 id="TFJob-和-TF-Serving"><a href="#TFJob-和-TF-Serving" class="headerlink" title="TFJob 和 TF Serving"></a>TFJob 和 TF Serving</h3><p>模型训练是机器学习最主要的实践场景，尤其以使用机器学习框架TensorFlow进行模型训练最为流行，但是随着机器学习的平台由单机变成集群，这个问题变得复杂了。GPU的调度和绑定，涉及到分布式训练的编排和集群规约属性的配置（cluster spec）也成了数据科学家们巨大的负担。</p><p>为了解决这一问题，一个新的资源类型TFJob，即TensorFlow Job被定义出来了。通过这个资源类型，使用TensorFlow的数据科学家无需编写复杂的配置，只需要关注数据的输入，代码的运行和日志的输入输出。</p><p>下面看一个<strong>官方案例</strong>来看一下<code>TFJob</code>的定义, 除了<code>tfReplicaSpecs</code>定义为<code>PS</code> <code>Worker</code>等概念之外, 其实和没有K8s的POD定位没有太多的区别.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"kubeflow.org/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"TFJob"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"tf-smoke-gpu"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"kubeflow"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  tfReplicaSpecs:</span></span><br><span class="line"><span class="attr">    PS:</span></span><br><span class="line"><span class="attr">      replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      template:</span></span><br><span class="line"><span class="attr">        metadata:</span></span><br><span class="line"><span class="attr">          creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">        spec:</span></span><br><span class="line"><span class="attr">          containers:</span></span><br><span class="line"><span class="attr">          - args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">python</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">tf_cnn_benchmarks.py</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--batch_size=32</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--model=resnet50</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--variable_update=parameter_server</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--flush_stdout=true</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--num_gpus=1</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--local_parameter_device=cpu</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--device=cpu</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--data_format=NHWC</span></span><br><span class="line"><span class="attr">            image:</span> <span class="string">swr.cn-north-5.myhuaweicloud.com/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">tensorflow</span></span><br><span class="line"><span class="attr">            ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">2222</span></span><br><span class="line"><span class="attr">              name:</span> <span class="string">tfjob-port</span></span><br><span class="line"><span class="attr">            resources:</span></span><br><span class="line"><span class="attr">              limits:</span></span><br><span class="line"><span class="attr">                cpu:</span> <span class="string">'1'</span></span><br><span class="line"><span class="attr">            workingDir:</span> <span class="string">/opt/tf-benchmarks/scripts/tf_cnn_benchmarks</span></span><br><span class="line"><span class="attr">          restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line"><span class="attr">    Worker:</span></span><br><span class="line"><span class="attr">      replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      template:</span></span><br><span class="line"><span class="attr">        metadata:</span></span><br><span class="line"><span class="attr">          creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">        spec:</span></span><br><span class="line"><span class="attr">          containers:</span></span><br><span class="line"><span class="attr">          - args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">python</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">tf_cnn_benchmarks.py</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--batch_size=32</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--model=resnet50</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--variable_update=parameter_server</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--flush_stdout=true</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--num_gpus=1</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--local_parameter_device=cpu</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--device=gpu</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--data_format=NHWC</span></span><br><span class="line"><span class="attr">            image:</span> <span class="string">swr.cn-north-5.myhuaweicloud.com/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">tensorflow</span></span><br><span class="line"><span class="attr">            ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">2222</span></span><br><span class="line"><span class="attr">              name:</span> <span class="string">tfjob-port</span></span><br><span class="line"><span class="attr">            resources:</span></span><br><span class="line"><span class="attr">              limits:</span></span><br><span class="line">                <span class="string">nvidia.com/gpu:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            workingDir:</span> <span class="string">/opt/tf-benchmarks/scripts/tf_cnn_benchmarks</span></span><br><span class="line"><span class="attr">          restartPolicy:</span> <span class="string">OnFailure</span></span><br></pre></td></tr></table></figure><p><strong>TensorFlow Serving</strong>是Google开源的一个灵活的、高性能的机器学习模型服务系统，能够简化并加速从模型到生产应用的过程。它除了原生支持TensorFlow模型，还可以扩展支持其他类型的机器学习模型。</p><p>部署完成之后, 可以通过API访问TF Serving或者GRpc的方式来进行推理服务.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc mnist-service</span><br><span class="line">curl -X POST -d @input.json http://EXTERNAL_IP:8500/v1/models/mnist:predict</span><br></pre></td></tr></table></figure><blockquote><p>这个部分可以参考<a href="https://yq.aliyun.com/articles/601779?utm_content=m_1000003286" target="_blank" rel="noopener">阿里云的文章</a>, 后续再写详细介绍的文章</p></blockquote><p>其中有一点需要的注意的是: <strong>TFJob是Kubeflow在K8s上的CRD, 而TF-Serving是TensorFlow的内容, Kubeflow只不过在K8s上启动一个镜像实例而已</strong>, 所以你也可以来完成TF-Serving功能</p><h3 id="Katib超参数训练系统"><a href="#Katib超参数训练系统" class="headerlink" title="Katib超参数训练系统"></a>Katib超参数训练系统</h3><blockquote><p>Katib is a Kubernetes Native System for <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" target="_blank" rel="noopener">Hyperparameter Tuning</a> and <a href="https://en.wikipedia.org/wiki/Neural_architecture_search" target="_blank" rel="noopener">Neural Architecture Search</a>. The system is inspired by <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/bcb15507f4b52991a0783013df4222240e942381.pdf" target="_blank" rel="noopener">Google vizier</a> and supports multiple ML/DL frameworks (e.g. TensorFlow, MXNet, and PyTorch).</p></blockquote><p>Katib 也是对 Google Vizier 的开源实现，因此也遵循其中对问题的抽象模型：Study，Trial 和 Suggestion.<br>Trial 代表着一个由超参数的取值组成的列表，每个 Trial 都需要一次运行来得到这些超参数取值对应的结果。这里提到的运行就是一次训练的过程。Study 代表着在可行空间上运行的单个优化。每个 Study 都有一个配置，来描述可能取值的空间，超参数推荐算法等。此外，Study 包含一组 Trial，代表着算法在超参数集合中选取推荐值的多次尝试。如图所示，是创建一次 Study 并且进行 Trial 的验证的过程。</p><blockquote><p>Currently Katib supports the following exploration algorithms in v1alpha1:</p><ul><li>random search</li><li>grid search</li><li><a href="https://arxiv.org/pdf/1603.06560.pdf" target="_blank" rel="noopener">hyperband</a></li><li><a href="https://arxiv.org/pdf/1012.2599.pdf" target="_blank" rel="noopener">bayesian optimization</a></li><li><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1alpha1/NAS_Reinforcement_Learning" target="_blank" rel="noopener">NAS based on reinforcement learning</a></li><li><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1alpha1/NAS_Envelopenet" target="_blank" rel="noopener">NAS based on EnvelopeNets</a><br>And Katib supports the following exploration algorithms in v1alpha2:</li><li>random search</li></ul><p>这块非常不懂唉, 后续等恶补一下知识了.  现在摘要了这篇<a href="http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/katib" target="_blank" rel="noopener">博客文章</a>的内容</p></blockquote><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>这块的目的就是为了编排Kubeflow任务用的, 在<a href="https://saintbacchus.github.io/2019/07/21/Kubeflow-pipleline%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">这篇文章</a>之中介绍.</p><h3 id="Jupiter-Notebook"><a href="#Jupiter-Notebook" class="headerlink" title="Jupiter Notebook"></a>Jupiter Notebook</h3><p>Notebook是数据科学家比较喜欢的编程工具, kubeflow已经集成:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/9.png" alt></p><p>上图是notebook的创建页面,  可以看出来kubeflow的notebook有以下能力:</p><ol><li>自定义镜像</li><li>支持资源定义</li><li>支持各种数据盘挂载</li><li>支持GPU调度</li></ol><p><strong> 使用notebook提交pipeline</strong></p><p>以官方的<strong>Lightweight Python components - basics</strong>为例子</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/10.png" alt></p><ul><li>上边的cell定了流程算子</li><li>中间的cell编译流程算子, 生成<code>argo</code>的配置文件<code>pipeline.zip</code></li><li>下面的cell创建了<code>Pipeline Service</code>的客户端, 创建了<code>experiment</code>和<code>run</code>执行了任务</li></ul><h3 id="Fairing-SDK"><a href="#Fairing-SDK" class="headerlink" title="Fairing SDK"></a>Fairing SDK</h3><p>Kubeflow Fairing is <strong>a Python package</strong> that makes it easy to train and deploy ML models on <a href="https://www.kubeflow.org/docs/about/kubeflow/" target="_blank" rel="noopener">Kubeflow</a>. Kubeflow Fairing can also been extended to train or deploy on other platforms. Currently, Kubeflow Fairing has been extended to train on <a href="https://cloud.google.com/ml-engine/docs/" target="_blank" rel="noopener">Google AI Platform</a>.</p><p>Kubeflow Fairing <strong>packages your Jupyter notebook, Python function, or Python file as a Docker image</strong>, then <strong>deploys and runs the training job</strong> on Kubeflow or AI Platform. After your training job is complete, you can use Kubeflow Fairing to <strong>deploy your trained model as a prediction endpoint</strong> on Kubeflow.</p><p>官网上介绍Fairing的文字, fairing的定位就是机器学习的SDK, 但他最大的一个好处就是简化构建镜像的麻烦, 它能自动帮你构建镜像, 并将镜像部署在k8s上运行, 因为之前的开发模式是这样子的:</p> <div id="flowchart-0" class="flow-chart"></div><p>引入Kubeflow变成这样了:</p><div id="flowchart-1" class="flow-chart"></div><p>平白多了几部做镜像的步骤, 而且做镜像对数据科学家来说是个难活, 因此引入Fairing工程是十分必要的</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>上面已经把kubeflow最重要的内容将完, 但是k8s还有很多其他子项目, 虽然不然之前的模块那么重要, 但是也是必不可少的内容. 这些组件很多都不在k8s之中, 在k8s的其他项目里面.</p><h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><p>元数据模块主要是为了用户能更好的管理和追踪kubeflow的流程,  它后台使用的是Google的<a href="https://github.com/google/ml-metadata/blob/master/g3doc/get_started.md" target="_blank" rel="noopener">ML-Metadata</a>来管理, 而且暴露了<a href="https://github.com/kubeflow/metadata/blob/master/api/service.swagger.json" target="_blank" rel="noopener">REST API</a>来跟用户调用, 同时也有<a href="https://github.com/kubeflow/metadata/tree/master/sdk/python#python-client" target="_blank" rel="noopener">Python SDK</a>远程调用</p><h4 id="kustomize"><a href="#kustomize" class="headerlink" title="kustomize"></a>kustomize</h4><p><code>kustomize</code>并不是kubeflow的一个项目, 而是<a href="https://github.com/kubernetes-sigs/kustomize" target="_blank" rel="noopener">kubernetes-sigs</a>的一个子项目, 主要简化部署K8s服务的麻烦.</p><p>他的idea的是这样的: 之前部署k8s需要定义很多的YAML文件, 这些文件都要自己管理, 而kustomize可以将这些文件管理起来, 貌似还有版本管理的能力</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/1.jpg" alt></p><p>看一下这个图片示意, 基本上就了解.</p><p><a href="https://github.com/kubernetes-sigs/kustomize" target="_blank" rel="noopener">官网地址</a></p><h4 id="Nuclio-functions"><a href="#Nuclio-functions" class="headerlink" title="Nuclio functions"></a>Nuclio functions</h4><p>Nuclio 是开源的一个实时无服务器平台, 支持可插拔的事件源和数据源, 下面是它的整体架构.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/19.png" alt></p><blockquote><p>目前没看出来这个Serverless的框架和Kubeflow有太多的联系, 而且FaaS战场上Nuclio也不是真正的主流, <code>knative</code>是google推的Serverless解决方案, 所以这个部分不多介绍了.  </p></blockquote><h4 id="ksonnet"><a href="#ksonnet" class="headerlink" title="ksonnet"></a>ksonnet</h4><p>ksonnet 是一个基于jsonnet的快速简化kubernetes yaml 配置的工具，可以实现配置的复用 </p><p>同时也包含一个registry 的概念，可以实现可复用组件的分发，同时支持helm</p><p>Kubeflow里面主要使用<code>ksonnet</code>完成安装的时候参数的配置.</p><p>具体内容可以看<a href="https://ksonnet.io/" target="_blank" rel="noopener">官网文档</a></p><h4 id="Microk8s"><a href="#Microk8s" class="headerlink" title="Microk8s"></a>Microk8s</h4><p>单机K8s测试环境的库,  这个安装起来比<code>Minikube</code>和<code>MiniKF</code>更加方便</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/canonical-labs/kubernetes-tools</span><br><span class="line">sudo kubernetes-tools/setup-microk8s.sh</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/canonical-labs/kubeflow-tools</span><br><span class="line">kubeflow-tools/install-kubeflow.sh</span><br></pre></td></tr></table></figure><h4 id="Volcano’s-scheduler"><a href="#Volcano’s-scheduler" class="headerlink" title="Volcano’s scheduler"></a>Volcano’s scheduler</h4><p>Volcano调度是华为公司贡献给社区的批处理调度器,  支持AI和大数据计算</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/volcano-intro.png" alt></p><p>给K8s提供以下能力:</p><ol><li>Job management extensions and improvements, e.g:<ol><li>Multi-pod jobs</li><li>Lifecycle management extensions including suspend/resume and restart.</li><li>Improved error handling</li><li>Indexed jobs</li><li>Task dependencies</li></ol></li><li>Scheduling extensions, e.g:<ol><li>Co-scheduling</li><li>Fair-share scheduling</li><li>Queue scheduling</li><li>Preemption and reclaims</li><li>Reservations and backfills</li><li>Topology-based scheduling</li></ol></li><li>Runtime extensions, e.g:<ol><li>Support for specialized container runtimes like Singularity, with GPU accelerator extensions and enhanced security features.</li></ol></li><li>Other<ol><li>Data locality awareness and intelligent scheduling</li><li>Optimizations for data throughput, round-trip latency, etc.</li></ol></li></ol><h5 id="Kube-Batch"><a href="#Kube-Batch" class="headerlink" title="Kube-Batch"></a>Kube-Batch</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/kube-batch.png" alt></p><p><code>kube-batch</code>是一个为kubernetes实现批量任务调度的一个调度器，主要用于<code>机器学习</code>，<code>大数据</code>，<code>HPC</code>等场景.</p><p>而Volcano正是构建在Kube-Batch之上的.</p><h5 id="Gang-Scheduler"><a href="#Gang-Scheduler" class="headerlink" title="Gang Scheduler"></a>Gang Scheduler</h5><p><code>gang-schedule</code>是什么概念? 用用户提交一个 batch job, 这个batch job 包含100个任务，要不这100个任务全部调度成功，要么一个都调度不成功。这种<code>all or nothing</code>调度场景，就被称作:<code>gang-schedule</code>，通常用于集群资源不足的场景，比如 AI 场景下调用GPU资源</p><p>Gang Scheduler特别适合机器学习场景, 引入Vocalno和Kube-bath最终目的就是为了引入Gang-Scheduler</p><blockquote><p><a href="https://xigang.github.io/2019/02/17/gang-scheduler/" target="_blank" rel="noopener">参考文章</a></p></blockquote><h4 id="Dex"><a href="#Dex" class="headerlink" title="Dex"></a>Dex</h4><p><a href="https://github.com/coreos/dex" target="_blank" rel="noopener">dex</a> 是一个统一认证的服务，支持各种认证协议如Ouath2 ldap等，自己可以作为一个identity provider,也可以连到别的id provider(如github)上,dex作为一个中间代理.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/20.png" alt></p><p>安装官网给出的推荐案例, 使用dex进行租户认证</p><blockquote><p><a href="https://github.com/dexidp/dex" target="_blank" rel="noopener">Github首页</a></p></blockquote><h4 id="Istio"><a href="#Istio" class="headerlink" title="Istio"></a>Istio</h4><p>Istio是钟Service Mesh的实现, Service Mesh这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。</p><p>Istio 提供了一个完整的解决方案，通过为整个服务网格提供行为洞察和操作控制来满足微服务应用程序的多样化需求.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/arch.svg" alt></p><blockquote><p><a href="https://istio.io/" target="_blank" rel="noopener">官网首页</a>以及<a href="https://istio.io/zh/docs/concepts/what-is-istio/" target="_blank" rel="noopener">中文官网</a></p></blockquote><h2 id="完结撒花"><a href="#完结撒花" class="headerlink" title="完结撒花"></a>完结撒花</h2><p>今天把kubeflow的一些概念都整理了一下, 除了AI那块不是特别懂, 所以介绍内容比较少,  后面需要再调研一下, 再写介绍文章, 至于工程方面的都基本上算调研好了, 后续深入使用的时候再写文章介绍</p><p>后续至少需要隆重写文章介绍的有:</p><ol><li>Pipeline的文章</li><li>TFJob的文章</li><li>TF-Serving的文章</li><li>Katib的文章</li><li>Istio的文章<script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 写机器学习代码op=>operation: 执行训练过程, 并调试e=>end: 模型训练完毕, 进行部署st->op->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><textarea id="flowchart-1-code" style="display: none">st=>start: 写机器学习代码op1=>operation: 做镜像op2=>operation: 训练并调试op3=>operation: 继续做镜像e=>end: 进行部署st->op1->op2->op3->e</textarea><textarea id="flowchart-1-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-1", options);</script></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker in docker</title>
      <link href="/2019/07/13/Docker-in-docker/"/>
      <url>/2019/07/13/Docker-in-docker/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="HostPath挂载"><a href="#HostPath挂载" class="headerlink" title="HostPath挂载"></a>HostPath挂载</h2><p>容器启动时, 挂载<code>/var/run/docker.soc</code>k和<code>/usr/bin/docker</code>,即可在容器中执行docker命令。此时docker server仍运行在host机上，docker in docker 实际操作的是宿主机docker。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">docker-in-docker-mount</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">docker-in-docker-mount</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">docker-socket</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line"><span class="attr">            type:</span> <span class="string">''</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">docker-cmd</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/usr/bin/docker</span></span><br><span class="line"><span class="attr">            type:</span> <span class="string">''</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">container-0</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">'ubuntu-docker-in-docker:latest'</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">/bin/bash</span></span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">'-c'</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">'echo hello;docker image ls'</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">docker-socket</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">docker-cmd</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/usr/bin/docker</span></span><br></pre></td></tr></table></figure><h2 id="docker-dind"><a href="#docker-dind" class="headerlink" title="docker dind"></a>docker dind</h2><p>docker 官方提供了docker in docker镜像: <code>docker pull docker:18.09.7-dind</code>, 直接使用此镜像即可在容器中在运行容器. 但是此种方式运行权限需要为<code>特权容器</code>, 在K8s设置<code>securityContext.privileged</code>为true<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">dind-test10</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">dind-test10</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">dind-test10</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">container-0</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">'docker:18.09.7-dind'</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">/bin/sh</span></span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">'-c'</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">'echo hello;docker image ls'</span>  </span><br><span class="line"><span class="attr">          securityContext:</span></span><br><span class="line"><span class="attr">            privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">            procMount:</span> <span class="string">Default</span></span><br></pre></td></tr></table></figure></p><h2 id="两者比较"><a href="#两者比较" class="headerlink" title="两者比较"></a>两者比较</h2><div class="table-container"><table><thead><tr><th style="text-align:center">比较项</th><th style="text-align:center">HostPath</th><th style="text-align:center">Dind</th></tr></thead><tbody><tr><td style="text-align:center">安全性</td><td style="text-align:center">中, docker命令可以注入宿主机之中</td><td style="text-align:center">低, 特权容器所有权限开放, 不能有用户代码存在容器之中</td></tr><tr><td style="text-align:center">简便性</td><td style="text-align:center">中, 需要额外设置HostPath</td><td style="text-align:center">高, 配置简单, 一键使用</td></tr><tr><td style="text-align:center">并发性</td><td style="text-align:center">高, 可以用于任何容器</td><td style="text-align:center">低, 只能基于Dind基础镜像之上构做镜像</td></tr></tbody></table></div><p>结论: 能用<code>HostPath</code>模式尽量使用:</p><ul><li>如果需要在容器运行<code>UDF</code>, 这两种权限都不能使用, 相对来说<code>HostPath</code>的威胁小一点点</li><li>如果要自定义镜像, 使用<code>HostPath</code></li><li>如果只是调用docker命令, 可以使用<code>Dind</code>, 但还是推荐使用<code>HostPath</code>, 因为<strong>你也许知道权限问题, 但是他人接手你工作的时候, 可能并不清楚</strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]Kubeflow的部署</title>
      <link href="/2019/07/10/Kubeflow%E7%B3%BB%E5%88%97-Kubeflow%E7%9A%84%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/07/10/Kubeflow%E7%B3%BB%E5%88%97-Kubeflow%E7%9A%84%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先,  官网上有对应的安装文档, 地址在这里<a href="https://www.kubeflow.org/docs/started/getting-started-k8s/" target="_blank" rel="noopener">官方安装指南</a>.</p><p>但是在国内总是有一些中国特设的问题, 所以在这儿记录一下安装的流程.</p><blockquote><p>安装的环境是在华为云的云容器引擎服务<a href="https://www.huaweicloud.com/product/cce.html" target="_blank" rel="noopener">CCE</a>, 其他环境可能略有不同.</p><p>此外华为云上已有一个<a href="https://bbs.huaweicloud.com/blogs/413d1821c1a211e89fc57ca23e93a89f" target="_blank" rel="noopener">KubeFlow安装指南</a></p></blockquote><h2 id="依赖安装"><a href="#依赖安装" class="headerlink" title="依赖安装"></a>依赖安装</h2><p>首先安装官方文档, 安装如下的三个依赖:</p><ol><li><a href="https://support.huaweicloud.com/usermanual-cce/cce_01_0107.html" target="_blank" rel="noopener">kubectl</a></li><li><a href="https://github.com/ksonnet/ksonnet/releases/" target="_blank" rel="noopener">ks</a></li><li><a href="https://github.com/kubeflow/kubeflow/releases/" target="_blank" rel="noopener">kfctl</a></li></ol><p>将这三个依赖都放入到环境之中, <code>kubectl</code>需要设置好环境, <code>kubectl get pod</code>不能有一次</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv ks kubectl kfctl /usr/<span class="built_in">local</span>/bin/</span><br></pre></td></tr></table></figure><blockquote><p>我安装的是v0.5版本的kubeflow, 版本升级之后不保证成功</p></blockquote><h2 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h2><p>执行以下命令生成配置文件, <strong>这个步骤需要在联网环境下执行</strong>, 在华为云上虚拟机就需要绑定弹性IP.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/kubeflow</span><br><span class="line"><span class="built_in">cd</span> /root/kubeflow</span><br><span class="line"><span class="built_in">export</span> KFAPP=kubeflow</span><br><span class="line">kfctl init <span class="variable">$&#123;KFAPP&#125;</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KFAPP&#125;</span></span><br><span class="line">kfctl generate all -V</span><br></pre></td></tr></table></figure><p>完成后,  会在目录下生成如下文件:</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/1.png" alt></div></p><h2 id="替换镜像"><a href="#替换镜像" class="headerlink" title="替换镜像"></a>替换镜像</h2><p>默认生成的配置, 其中的镜像都是GoogleCloud的公开镜像, 在国内网络之中, 肯定无法访问, 需要将这些镜像想办法下载下来传入到华为云的<a href="https://www.huaweicloud.com/product/swr.html" target="_blank" rel="noopener">容器镜像服务</a>之内.</p><h3 id="镜像下载上传"><a href="#镜像下载上传" class="headerlink" title="镜像下载上传"></a>镜像下载上传</h3><p>我将我用到的镜像都列举出来, 配置代理之后, 再拉取到本地</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">docker pull  gcr.io/ml-pipeline/api-server:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/persistenceagent:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/scheduledworkflow:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/frontend:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/viewer-crd-controller:0.1.16</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/pytorch-operator:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/tf_operator:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/centraldashboard:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/jupyter-web-app:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/katib-ui:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-random:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  tensorflow/tensorflow:1.8.0</span><br><span class="line">docker pull  mysql:5.6</span><br><span class="line">docker pull  mysql:8.0.3</span><br><span class="line">docker pull  quay.io/datawire/ambassador:0.37.0</span><br><span class="line">docker pull  argoproj/argoui:v2.2.0</span><br><span class="line">docker pull  argoproj/workflow-controller:v2.2.0</span><br><span class="line">docker pull  metacontroller/metacontroller:v0.3.0</span><br><span class="line">docker pull  minio/minio:RELEASE.2018-02-09T22-40-05Z</span><br></pre></td></tr></table></figure><p>而后再加入SWR的tag</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">docker tag  gcr.io/ml-pipeline/api-server:0.1.16                                                              swr.cn-north-5.myhuaweicloud.com/kubeflow/api-server:0.1.16                                                          </span><br><span class="line">docker tag  gcr.io/ml-pipeline/persistenceagent:0.1.16                                                        swr.cn-north-5.myhuaweicloud.com/kubeflow/persistenceagent:0.1.16                                                    </span><br><span class="line">docker tag  gcr.io/ml-pipeline/scheduledworkflow:0.1.16                                                       swr.cn-north-5.myhuaweicloud.com/kubeflow/scheduledworkflow:0.1.16                                                   </span><br><span class="line">docker tag  gcr.io/ml-pipeline/frontend:0.1.16                                                                swr.cn-north-5.myhuaweicloud.com/kubeflow/frontend:0.1.16                                                            </span><br><span class="line">docker tag  gcr.io/ml-pipeline/viewer-crd-controller:0.1.16                                                   swr.cn-north-5.myhuaweicloud.com/kubeflow/viewer-crd-controller:0.1.16                                               </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4      swr.cn-north-5.myhuaweicloud.com/kubeflow/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4  </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/pytorch-operator:v0.5.0                                             swr.cn-north-5.myhuaweicloud.com/kubeflow/pytorch-operator:v0.5.0                                         </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd                 swr.cn-north-5.myhuaweicloud.com/kubeflow/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd             </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/tf_operator:v0.5.0                                                  swr.cn-north-5.myhuaweicloud.com/kubeflow/tf_operator:v0.5.0                                              </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd                         swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core:v0.1.2-alpha-156-g4ab3dbd                     </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                    swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/centraldashboard:v0.5.0                                             swr.cn-north-5.myhuaweicloud.com/kubeflow/centraldashboard:v0.5.0                                         </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/jupyter-web-app:v0.5.0                                              swr.cn-north-5.myhuaweicloud.com/kubeflow/jupyter-web-app:v0.5.0                                          </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/katib-ui:v0.1.2-alpha-156-g4ab3dbd                            swr.cn-north-5.myhuaweicloud.com/kubeflow/katib-ui:v0.1.2-alpha-156-g4ab3dbd                        </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd     swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                     swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                 </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd                swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd            </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-random:v0.1.2-alpha-156-g4ab3dbd                   swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-random:v0.1.2-alpha-156-g4ab3dbd               </span><br><span class="line">docker tag  tensorflow/tensorflow:1.8.0                                                                       swr.cn-north-5.myhuaweicloud.com/kubeflow/tensorflow:1.8.0                 </span><br><span class="line">docker tag  mysql:5.6                                                                                         swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:5.6                                   </span><br><span class="line">docker tag  mysql:8.0.3                                                                                       swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:8.0.3                                 </span><br><span class="line">docker tag  quay.io/datawire/ambassador:0.37.0                                                                swr.cn-north-5.myhuaweicloud.com/kubeflow/ambassador:0.37.0          </span><br><span class="line">docker tag  argoproj/argoui:v2.2.0                                                                            swr.cn-north-5.myhuaweicloud.com/kubeflow/argoui:v2.2.0                      </span><br><span class="line">docker tag  argoproj/workflow-controller:v2.2.0                                                               swr.cn-north-5.myhuaweicloud.com/kubeflow/workflow-controller:v2.2.0         </span><br><span class="line">docker tag  metacontroller/metacontroller:v0.3.0                                                              swr.cn-north-5.myhuaweicloud.com/kubeflow/metacontroller:v0.3.0        </span><br><span class="line">docker tag  minio/minio:RELEASE.2018-02-09T22-40-05Z                                                          swr.cn-north-5.myhuaweicloud.com/kubeflow/minio:RELEASE.2018-02-09T22-40-05Z</span><br></pre></td></tr></table></figure><p>再推到SWR之中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/api-server:0.1.16                                                          </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/persistenceagent:0.1.16                                                    </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/scheduledworkflow:0.1.16                                                   </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/frontend:0.1.16                                                            </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/viewer-crd-controller:0.1.16                                               </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4  </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/pytorch-operator:v0.5.0                                         </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd             </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/tf_operator:v0.5.0                                              </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core:v0.1.2-alpha-156-g4ab3dbd                     </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/centraldashboard:v0.5.0                                         </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/jupyter-web-app:v0.5.0                                          </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/katib-ui:v0.1.2-alpha-156-g4ab3dbd                        </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                 </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd            </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-random:v0.1.2-alpha-156-g4ab3dbd               </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/tensorflow:1.8.0                 </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:5.6                                   </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:8.0.3                                 </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/ambassador:0.37.0          </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/argoui:v2.2.0                      </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/workflow-controller:v2.2.0         </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/metacontroller:v0.3.0        </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/minio:RELEASE.2018-02-09T22-40-05Z</span><br></pre></td></tr></table></figure><blockquote><p>华为云有几个限制, 需要你注意:</p><ol><li>路径不能有多级, 即容器名字不能含有<code>/</code></li><li>路径不能含有点号<code>.</code>, 有部分镜像带有版本号, 需要修改原始名称, 但是tag里面又可以存在<code>.</code></li></ol><p>另外这部分镜像可能是不全的, 因为例如<code>model db</code>等组件是没有安装的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/api-server:0.1.16                                                          </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/persistenceagent:0.1.16                                                    </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/scheduledworkflow:0.1.16                                                   </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/frontend:0.1.16                                                            </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/viewer-crd-controller:0.1.16                                               </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4  </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/pytorch-operator:v0.5.0                                         </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd             </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/tf_operator:v0.5.0                                              </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/vizier-core:v0.1.2-alpha-156-g4ab3dbd                     </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/centraldashboard:v0.5.0                                         </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/jupyter-web-app:v0.5.0                                          </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/katib-ui:v0.1.2-alpha-156-g4ab3dbd                        </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                 </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd            </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-random:v0.1.2-alpha-156-g4ab3dbd               </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/tensorflow:1.8.0                 </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/mysql:5.6                                   </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/mysql:8.0.3                                 </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/ambassador:0.37.0          </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/argoui:v2.2.0                      </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/workflow-controller:v2.2.0         </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/metacontroller:v0.3.0        </span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/minio:RELEASE.2018-02-09T22-40-05Z</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/volume-nfs:0.8</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/argoexec:v2.2.0</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/metrics-collector:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-artifact-store:kubeflow</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-backend:kubeflow</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-backend-proxy:kubeflow</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/mysql:5.7</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-frontend:kubeflow</span><br><span class="line">&gt; docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/serving:1.11.1</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>这份长的整整有35个镜像之多</p></blockquote><h3 id="镜像替换"><a href="#镜像替换" class="headerlink" title="镜像替换"></a>镜像替换</h3><p>经过最终排查, 发现所有依赖镜像都在以下两个文件之中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ks_app/vendor/kubeflow/pipeline/pipeline.libsonnet</span><br><span class="line">vim ks_app/components/params.libsonnet</span><br></pre></td></tr></table></figure><p>使用vim打开这两个文件, 并搜索<code>/Image\c</code>, 忽略大小搜索所有Image, 并将后面的值修改为SWR上的镜像地址</p><h3 id="怎么找到哪些镜像是你需要替换的镜像"><a href="#怎么找到哪些镜像是你需要替换的镜像" class="headerlink" title="怎么找到哪些镜像是你需要替换的镜像"></a>怎么找到哪些镜像是你需要替换的镜像</h3><p>有两种方法: </p><ol><li>搜索上面那两个文件, 获取其中的镜像列表</li><li>先安装容器, 然后再看什么镜像拉取不下来</li></ol><p>实际上使用第一种会更快一点, 基本上也就包含了所有的镜像</p><h2 id="安装kubeflow组件"><a href="#安装kubeflow组件" class="headerlink" title="安装kubeflow组件"></a>安装kubeflow组件</h2><p>执行一下命令, 在k8s里部署<code>kubeflow</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kfctl apply all -V</span><br></pre></td></tr></table></figure><p>至此, 你以为任务已经完成, 但是一到k8s的页面一看, 发现Pod无法启动, 我们再一个一个解决问题.</p><blockquote><p>kubeflow大多数是deployment, 所以工作负载-&gt;无状态页面上可以看到组件状态</p></blockquote><h3 id="镜像拉取权限"><a href="#镜像拉取权限" class="headerlink" title="镜像拉取权限"></a>镜像拉取权限</h3><p>遇到的第一个问题, 发现Pod的镜像一直无法拉取, 查了半天资料才发现华为云还有这么一个坑爹的限制: <a href="https://support.huaweicloud.com/cce_faq/cce_faq_00015.html" target="_blank" rel="noopener">必须显式指定imagePullSecrets</a></p><p><strong>那就解决它吧</strong></p><p>首先执行以下命令获取全部的<code>serviceaccount</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get serviceaccount -n kubeflow</span><br></pre></td></tr></table></figure><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/2.png" alt></div></p><p>其次, 执行以下命令, 给每个<code>serviceaccount</code>添加默认的<code>imagePullSecrets</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kubeflow patch serviceaccount ambassador                             -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span>     </span><br><span class="line">kubectl -n kubeflow patch serviceaccount argo                                   -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span>  </span><br><span class="line">kubectl -n kubeflow patch serviceaccount argo-ui                                -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span>    </span><br><span class="line">kubectl -n kubeflow patch serviceaccount centraldashboard                       -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount default                                -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount jupyter-notebook                       -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount jupyter-web-app                        -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount katib-ui                               -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount meta-controller-service                -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount metrics-collector                      -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline                            -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-persistenceagent           -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-scheduledworkflow          -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-ui                         -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-viewer-crd-service-account -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount notebook-controller                    -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount pipeline-runner                        -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount pytorch-operator                       -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount studyjob-controller                    -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount tf-job-dashboard                       -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount tf-job-operator                        -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount vizier-core                            -p <span class="string">'&#123;"imagePullSecrets": [&#123;"name": "default-secret"&#125;]&#125;'</span></span><br></pre></td></tr></table></figure><p>删除失败Pod, 过段时间后, 大部分的都正常了, 还有部分有问题.</p><h3 id="创建数据库PVC"><a href="#创建数据库PVC" class="headerlink" title="创建数据库PVC"></a>创建数据库PVC</h3><p>这个时候, 你应该能发现, mysql的相关负载一直是黄, 估计是存储有问题, 用下面命令查看一下pvc状态, 果然如你所料</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pvc -n kubeflow</span><br><span class="line">NAME             STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">katib-mysql      Pending                                                      12h</span><br><span class="line">minio-pvc        Pending                                                      12h</span><br><span class="line">mysql-pv-claim   Pending                                                      12h</span><br></pre></td></tr></table></figure><p>那么主动在管理页面上创建三个对应的pvc(图中多了一个notebook的pvc请忽略)</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/3.png" alt></div></p><p>由于CCE的PVC必然是带有<code>cce-sfs-</code>这类的前缀, 因此实际上PVC的名字发生了改变, 需要修改对应的相应的Deployment的yaml配置项</p><p>这点在华为云的页面上直接可以操作: 点开对应的Deployment的配置项, 找到<code>编辑YAML</code>完成任务修改.</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/4.png" alt></div></p><p>重启Deployment之后, 这几个负载也变绿了.</p><h3 id="修改vizier-db配置项"><a href="#修改vizier-db配置项" class="headerlink" title="修改vizier-db配置项"></a>修改vizier-db配置项</h3><p>但是还有一个负载顽强跑不起来, 查看事件发现是, <code>readinessProbe</code>有出错, 发现<code>initialDelaySeconds</code>默认为1s, 启动时间太短, 修改为15秒后, 负载正常启动.<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">readinessProbe:</span></span><br><span class="line"><span class="attr">  exec:</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">/bin/bash</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">'-c'</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">'"""mysql -D $$MYSQL_DATABASE -p$$MYSQL_ROOT_PASSWORD -e '</span><span class="string">'SELECT 1'</span><span class="string">'"""'</span></span><br><span class="line"><span class="attr">  initialDelaySeconds:</span> <span class="string">**15**</span></span><br><span class="line"><span class="attr">  timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  periodSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  failureThreshold:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>至此,  所有Deployment都开始正常运行了</p><h2 id="完结撒花"><a href="#完结撒花" class="headerlink" title="完结撒花"></a>完结撒花</h2><p>最后放一张可视化的页面截图</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/5.png" alt></div></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[题型设计]复杂任务调度设计</title>
      <link href="/2019/06/29/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E8%AE%BE%E8%AE%A1/"/>
      <url>/2019/06/29/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近想优化一下业务代码之中的任务调度模块,  我把整体需求理了理, 准备重头开始设计一下.</p><p>请听需求:</p><p>在业务系统里面, 任务提交是一个非常常见的场景, 下面是我提炼的业务系统任务的几个特点:</p><ol><li>任务运行时间较长, 可能会超过一个小时, 肯定会有不同的状态</li><li>任务会有不同类型, 每个类型之间会有一定关联, 但是差别也很多</li><li>一个任务可能会有多个子任务: 子任务是不可再分的</li><li>每个子任务或者任务都可能会失败, 需要进行任务重试, 重试要遵循幂等设计</li><li>子任务会连接其他系统, 中间有可能会有超时异常, 要防止任务堆积</li><li>这里假设任务提交速率不高, 可以单机处理 </li></ol><p>问题如下, 请用伪代码描述:</p><ol><li>设计一个框架来实现该功能, 详细描述里面用到的设计模式</li><li>如果提交速率提高到100个请求/s, 需要分布式化, 请问如何设计</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TBD </tag>
            
            <tag> 面试题设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8s命令参数</title>
      <link href="/2019/06/29/K8s%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0/"/>
      <url>/2019/06/29/K8s%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="从Docker说起"><a href="#从Docker说起" class="headerlink" title="从Docker说起"></a>从Docker说起</h2><p>在Docker之中有两个命令行入口定义: <code>ENTRYPOINT</code>和<code>CMD</code>. 两者都是定义容器的启动命令, 两个定义方式也是相同的, 都是一个字符串数组, 类似于</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"sh"</span>, <span class="string">"/opt/entrypoint.sh"</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"sh"</span>, <span class="string">"/opt/entrypoint.sh"</span>]</span></span><br></pre></td></tr></table></figure><p>两者的区别在于, 执行<code>docker run</code>后面命令行的时候, <code>CMD</code>的命令直接被覆盖了, 而<code>ENTRYPOINT</code>还依然会被执行</p><blockquote><p>如果想覆盖<code>ENTRYPOINT</code>, 在docker run的时候需要显示指定<code>--entrypoint</code></p></blockquote><p>而且通常情况下, 我们会将<code>ENTRYPOINT</code>和<code>CMD</code>联合使用, 类似这样:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"sh"</span>, <span class="string">"/opt/entrypoint.sh"</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/home"</span>, <span class="string">"/opt"</span>]</span></span><br></pre></td></tr></table></figure><p>这种情况下, <code>DOCKERFILE</code>直接定义的入口脚本为<code>sh /opt/entrypoint.sh</code>, 但脚本的参数有<code>CMD</code>提供, 用户执行<code>docker run</code>的时候可以通过修改输入, 直接改变容器启动命令参数.</p><h2 id="K8s的命令参数"><a href="#K8s的命令参数" class="headerlink" title="K8s的命令参数"></a>K8s的命令参数</h2><p>在K8s之中, 是没有<code>ENTRYPOINT</code>或者<code>CMD</code>的,  来看一组K8s中典型的任务定义:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">command-demo</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    purpose:</span> <span class="string">demonstrate-command</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">command-demo-container</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">debian</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">["printenv"]</span></span><br><span class="line"><span class="attr">    args:</span> <span class="string">["HOSTNAME",</span> <span class="string">"KUBERNETES_PORT"</span><span class="string">]</span></span><br></pre></td></tr></table></figure></p><p>在K8s有两个关键的词: <code>command</code>和<code>args</code>, 这两个函数和DockerFile之中的<code>ENTRYPOINT</code>和<code>CMD</code>的关系如下:</p><div class="table-container"><table><thead><tr><th>分类</th><th>有args</th><th>无args</th></tr></thead><tbody><tr><td>有command</td><td>覆盖Docker内的ENTRYPOINT和CMD</td><td>覆盖Docker内的ENTRYPOINT和CMD</td></tr><tr><td>无command</td><td>使用Docker内的ENTRYPOINT, 覆盖CMD</td><td>使用Docker默认ENTRYPOINT和CMD</td></tr></tbody></table></div><p><strong>推荐使用方式: </strong>  K8s之中不定义<code>command</code>只定义<code>args</code>, 这样做的好处是, K8s层不需要来感知容器内部的入口, 真正的入口由<strong>容器制作者</strong>来确定, 而<code>args</code>则是比较变化的, 就由用户在K8s层输入即可.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo使用技巧记录</title>
      <link href="/2019/06/15/Hexo%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E8%AE%B0%E5%BD%95/"/>
      <url>/2019/06/15/Hexo%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="新环境初始化"><a href="#新环境初始化" class="headerlink" title="新环境初始化"></a>新环境初始化</h2><blockquote><p>用于新环境的安装, 例如刚申请了一个电脑, 假设你配置完毕git和下载安装完毕<a href="http://nodejs.cn/download/" target="_blank" rel="noopener">NodeJs</a></p></blockquote><p>执行下面的命令, 完成初始化命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:SaintBacchus/hexo.git</span><br><span class="line"><span class="built_in">cd</span> hexo</span><br><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure></p><h2 id="如何实现置顶功能"><a href="#如何实现置顶功能" class="headerlink" title="如何实现置顶功能"></a>如何实现置顶功能</h2><p>首先安装hexo插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure><p>然后在文章的Front-matter头部加入:    <code>top: true</code></p><blockquote><p>出自<a href="http://wangwlj.com/2018/01/09/blog_pin_post/" target="_blank" rel="noopener">博客</a></p></blockquote><h2 id="如何实现一个文章多个categories"><a href="#如何实现一个文章多个categories" class="headerlink" title="如何实现一个文章多个categories"></a>如何实现一个文章多个categories</h2><p>多个categories<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">[Sports]</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">[Baseball]</span></span><br></pre></td></tr></table></figure></p><p>多级categories<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">Sports</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">Baseball</span></span><br></pre></td></tr></table></figure></p><p>或者<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span> <span class="string">[Sports,Baseball</span></span><br></pre></td></tr></table></figure></p><p>组合使用:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">[Sports,Baseball]</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">[Play]</span></span><br></pre></td></tr></table></figure></p><blockquote><p>出自<a href="http://aiellochan.com/2018/02/13/hexo/Hexo-%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0%E5%A4%9A%E4%B8%AA-categories/" target="_blank" rel="noopener">博客</a></p></blockquote><h2 id="文章加密"><a href="#文章加密" class="headerlink" title="文章加密"></a>文章加密</h2><p>使用如下命令安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-blog-encrypt</span><br></pre></td></tr></table></figure><p>启动插件, 在根<code>_config.yaml</code>设置:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Security</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="attr">encrypt:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>在每篇文章的Format里面设置:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># password: 是该博客加密使用的密码</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">Mike</span></span><br><span class="line"><span class="comment"># abstract: 是该博客的摘要，会显示在博客的列表页</span></span><br><span class="line"><span class="attr">abstract:</span> <span class="string">Welcome</span> <span class="string">to</span> <span class="string">my</span> <span class="string">blog,</span> <span class="string">enter</span> <span class="string">password</span> <span class="string">to</span> <span class="string">read.</span></span><br><span class="line"><span class="comment"># message: 这个是博客查看时，密码输入框上面的描述性文字</span></span><br><span class="line"><span class="attr">message:</span> <span class="string">Welcome</span> <span class="string">to</span> <span class="string">my</span> <span class="string">blog,</span> <span class="string">enter</span> <span class="string">password</span> <span class="string">to</span> <span class="string">read.</span></span><br></pre></td></tr></table></figure><blockquote><p>出自<a href="https://github.com/MikeCoder/hexo-blog-encrypt/blob/master/ReadMe.zh.md" target="_blank" rel="noopener">Github</a></p></blockquote><h2 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h2><blockquote><p>出自<a href="https://www.jianshu.com/p/514c7792ad10" target="_blank" rel="noopener">简书</a></p></blockquote><h2 id="支持HTTPS认证"><a href="#支持HTTPS认证" class="headerlink" title="支持HTTPS认证"></a>支持HTTPS认证</h2><blockquote><p>出自<a href="https://molunerfinn.com/hexo-travisci-https/#%E5%8A%A0%E5%85%A5HSTS%E7%9A%84%E5%88%97%E8%A1%A8" target="_blank" rel="noopener">博客</a></p></blockquote><h2 id="升级Hexo客户端"><a href="#升级Hexo客户端" class="headerlink" title="升级Hexo客户端"></a>升级Hexo客户端</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo -v</span><br><span class="line">npm update</span><br></pre></td></tr></table></figure><h2 id="图片居中显示"><a href="#图片居中显示" class="headerlink" title="图片居中显示"></a>图片居中显示</h2><p>在图谱插入的前面加入HTML前缀<code>&lt;div align=center&gt;</code>, 使用方式如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div align=center&gt;![](https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/aboutme/1.png)</span><br></pre></td></tr></table></figure></p><h2 id="修改默认Front-matter"><a href="#修改默认Front-matter" class="headerlink" title="修改默认Front-matter"></a>修改默认Front-matter</h2><p>在根目录下有<code>/scaffolds/post.md</code>的文件用来定义默认的文章格式</p><p>在此键入以下模板</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">&#123;&#123;</span> <span class="string">title</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">date:</span> <span class="string">&#123;&#123;</span> <span class="string">date</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="attr">typora-root-url:</span> <span class="string">..</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[题型设计]字符串相似度匹配</title>
      <link href="/2019/06/15/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D/"/>
      <url>/2019/06/15/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>近段时间接触了基因比对算法, 业界最常使用的是BWA算法, 我现在还不了解具体如何实现的, 这里只是准备把算法用计算机的语言描述一下.</p><p>请听题:</p><p>假设有一长字符串, 有以下特征:</p><ol><li>字符串长度大约为30亿</li><li>所有字符由以下五种组成: <code>ATGCN</code>, 前面为4钟碱基, 最后一个为未知碱基</li><li>该字符串为确定字符串, 即所有位置都已经正确</li></ol><p>另外有一短字符串, 短字符串有如下特性:</p><ol><li>长度为100或者150左右的固定长度字符</li><li>字符串也有<code>ATGC</code>组成</li><li>由实验误差原因, 已知该字符串每个位置都可能出现<code>p</code>的概率误差, 称之为<code>测序误差</code></li><li>由于生物学的缘由,  会出现字符出错, 也有出现字符丢失, 或者字符增加, 这种误差被称之为<code>突变误差</code></li></ol><p>此外, 还有一点:</p><ol><li><p>短字符串比较多, 可能长字符串同一个位置会有段短字符串匹配, 也可能没一个匹配到的</p></li><li><p>短序列的起始和结束位置是不固定,</p></li><li><p>目前的高级的测序仪器, 平均所有短序列的覆盖深度为50X, 即总共的字符为30亿*50, 如果短字符长度为100的话, 就有30亿*50/100 = 15亿条.</p></li></ol><p>问题:</p><ol><li>设计一个概念模型来计算字符的相似度, 并说明如何确定匹配阈值, 即达到什么概论, 可以认为没有匹配上.</li><li>设计一个算法能够尽快的计算出所有短序列在长序列之中位置</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TBD </tag>
            
            <tag> 面试题设计 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行计算模式</title>
      <link href="/2019/06/14/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%BC%8F/"/>
      <url>/2019/06/14/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>之前一直在做Spark相关工作, 主要就是做分布式计算相关的内容, 经常会听到一些<code>BSP</code>或者<code>MPP</code>等分布式模式的术语, 每次看过文章有些了解之后, 但是经常会忘记, 因此写个文章记录这些概念.</p><h2 id="BSP-ASP-SSP"><a href="#BSP-ASP-SSP" class="headerlink" title="BSP/ASP/SSP"></a>BSP/ASP/SSP</h2><p>首先要明确的是, 这三个概念主要是为了处理<code>机器学习领域迭代计算</code>提出来的</p><p><strong>BSP</strong>是指迭代过程之中, 必须等待前一轮的迭代全部才能进行下一轮, 每轮之间的等待,被称之为<code>Barrier</code>,  所以才叫做<code>Barrier Synchronous Parallel</code></p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/002.jpg" alt><br>而<code>ASP: Asynchronous Synchronous Parallel</code>是另外一个极端的方式, 任何一轮迭代绝对不会等待前面的迭代结果, 这个当然能解决<code>BSP</code>模型里面<strong>慢节点</strong>的问题, 但是它存在的问题就是<strong>速度不一,导致最后的梯度不收敛</strong></div></p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/001.jpg" alt></div></p><p>那么<code>SSP: Stale Synchronous Parallel</code>就是这两者的折中, 他有一个超参<code>s</code>, 表示最快的迭代和最慢的迭代之间的代差要小于等于S.</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/003.jpg" alt></div></p><p>我们经常听说<code>Spark</code>或者<code>MapReduce</code>是一个BSP模型, 原因在于Spark的实际计算只有一轮迭代, 一轮迭代就需要直接出最终结果, 那么只有BSP才能正确计算完毕.</p><p>所以说, Spark是BSP的一种, 这种说明既对, 他确实有栅栏, 但也不对, 在于它主要描述分布式机器学习流程.</p><p>至于为什么机器学习那种多轮迭代为什么可以进行<code>SSP</code>或者<code>ASP</code>计算, 具体的数学原理可以查看<a href="https://www.zhihu.com/question/264189719" target="_blank" rel="noopener">如何理解随机梯度下降</a></p><blockquote><p>细节的内容可以看这篇<a href="https://zhuanlan.zhihu.com/p/29968773" target="_blank" rel="noopener">博客</a>, 写的不错, 当然最好直接看<a href="http://papers.nips.cc/paper/4894-more-effective-distributed-ml-via-a-stale-synchronous-parallel-parameter-server.pdf" target="_blank" rel="noopener">论文</a></p><h2 id="SMP-NUMA-MPP"><a href="#SMP-NUMA-MPP" class="headerlink" title="SMP/NUMA/MPP"></a>SMP/NUMA/MPP</h2></blockquote><p>这三个概念主要出现在<code>计算机体系结构</code>之中, 主要将多核CPU如何实现并行计算的.</p><ul><li><p>SMP：对称多处理器结构(Symmetric Multi-Processor)</p></li><li><p>NUMA：非一致存储访问结构(Non-Uniform Memory Access)</p></li></ul><p>这两个经常在课文里面见到, 大学里面也学过, 大致的体系结构如下图所示:</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/004.png" alt><br><code>SMP</code>和<code>NUMA</code>都是单机多核, <code>MPP</code>我理解就应该多台机器了</div></p><blockquote><p>MPP：和NUMA不同，MPP提供了另外一种进行系统扩展的方式，它由多个SMP服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个SMP服务器(每个SMP服务器称节点)通过节点互联网络连接而成，每个节点只访问自己的本地资源(内存、存储等)，是一种完全无共享(Share Nothing)结构，因而扩展能力最好，理论上其扩展无限制。</p></blockquote><p>可能的示意图如下所示:</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/005.png" alt></div></p><p>除了体系结构, 这三个概念还经常用于描述数据库的不同架构, 例如:</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/006.png" alt></div></p><p>SMP是Shared Everthting的方式</p><p>NUMA是Shared Storage的方式</p><p>MPP是Shared Nothing的方式</p><p>按照这类分法, Spark是数据Shared Storage的方式, 因此数据放在HDFS可以共享获取.</p><p><strong>MPP架构有如下特点：</strong></p><ul><li>Share Nothing、节点之间数据不共享，只有通过网络连接实现的协同</li><li>每个节点有独立的存储和内存</li><li>数据根据某种规则(如Hash)散布到各个节点</li><li>计算任务也是会发布到各个节点并行执行，最后再将结果聚合到整体返回</li><li>用户使用时会看做整体</li><li>MPP数据库（如GreePlum）往往优先考虑C一致性，然后是A可用性，最后考虑P分区容忍</li><li>MPP架构目前被并行数据库广泛采用，一般通过scan、sort和merge等操作符实时返回查询结果</li></ul><p><strong>MPP架构劣势</strong></p><ul><li>很难高可用 -&gt; 影响可用性和可靠性  因为数据按某种规则如HASH已经散布到了各个节点上。</li><li>节点数 =任务并行数 -&gt; 影响扩展性 一个作业提交时，每个节点都要执行相同任务。而不像MapReduce那样做了根据实际开销进行任务拆分后散发到有资源的几个节点上。这一点大大影响了MPP架构应用的可扩展性。</li><li>每个客户端同时连接所有节点通信 -&gt; 影响网络 MPP架构每个节点独立，所以客户端往往需要连接所有节点进行通信，这使得网络也成为瓶颈。</li><li>分区容错性差 前面提到过MPP主要考虑CA，最次才是P。那么一旦扩展节点太多后，元数据管理十分困难。</li></ul><p><strong>MPP 适用场景</strong></p><ul><li>集群规模100以内、并发小（50以下）</li><li>MPP架构目前被并行数据库广泛采用，一般通过scan、sort和merge等操作符实时返回查询结果</li></ul><p><strong>MPP的典型架构</strong></p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/007.png" alt></div></p><blockquote><p> 参考<a href="https://www.jianshu.com/p/720df3b85b3a" target="_blank" rel="noopener">博客1</a>, <a href="https://blog.csdn.net/baichoufei90/article/details/84328666" target="_blank" rel="noopener">博客2</a>, <a href="https://rainforc.iteye.com/blog/2217606" target="_blank" rel="noopener">博客3</a></p></blockquote><h2 id="MPP数据库-Hadoop数据库"><a href="#MPP数据库-Hadoop数据库" class="headerlink" title="MPP数据库/Hadoop数据库"></a>MPP数据库/Hadoop数据库</h2><p>这个<a href="https://mp.weixin.qq.com/s/scXNfkpjktCZxBg3pYEUUA?" target="_blank" rel="noopener">博客</a> 介绍了MPP和Spark数据库的差别, 并提出现在MPP和Batch类数据库的融合, 文章写的很流畅, 读就是了.</p><p>另外特别的要提一下<code>Impala</code>, 这个系统之前做Spark SQL的时候, 和它对标过: 之前团队想要使用Spark SQL替代Impala. 但效果肯定不理想, MPP数据库的时延确实比Spark好太多 了.</p><p>这篇介绍Impala的<a href="https://www.cnblogs.com/Rainbow-G/articles/4282444.html" target="_blank" rel="noopener">文章</a>不错, 可以好好看一下</p><p><strong>这个章节, 回头再理理, 需要详细写一下</strong></p><h2 id="Poll-Push模式"><a href="#Poll-Push模式" class="headerlink" title="Poll/Push模式"></a>Poll/Push模式</h2><p>这个是<code>Shuffle</code>处理的概念, 经常出现在流处理系统的概念表里面, 例如Kafka/Flink.<br>在Spark之中, Poll是指ReduceTask主动去拉Shuffle的数据, 这种模式容错比较好处理, 数据丢失之后主要重试计算就好了.<br>Push模式是指将MapTask主动将数据push到ReduceTask的节点上,但是我们实际上在MapTask时候比较难以估计ReduceTask位置, 尤其在节点丢失情况下, 所以要实现push模式, 编程量会很多.</p><p>而在Kafka之中, Push和Poll区别主要在于Consumer是主动获取数据, 还是被动接收数据.</p><p>这个概念还是比较容易理解的.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体的文章有点乱, 概念越看越模糊, 后续再优化吧.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TBD </tag>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>迭代器引导的序列化惨案</title>
      <link href="/2019/06/14/%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%BC%95%E5%AF%BC%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%83%A8%E6%A1%88/"/>
      <url>/2019/06/14/%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%BC%95%E5%AF%BC%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%83%A8%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>今天2月份左右GATK的4.1版本发布, 经过测试目前版本的准确率相对于单机版本也有版本提高, 具体数值见下表</p><center>单机版GATK准确率表</center><div class="table-container"><table><thead><tr><th>单机GATK</th><th>Precision</th><th>Sensitivity</th><th>F-measure</th></tr></thead><tbody><tr><td>SNP</td><td>98.90%</td><td>99.84%</td><td>99.37%</td></tr><tr><td>INDEL</td><td>97.20%</td><td>96.22%</td><td>96.71%</td></tr></tbody></table></div><center>分布式版GATK准确率表</center><div class="table-container"><table><thead><tr><th>分布式GATK</th><th>Precision</th><th>Sensitivity</th><th>F-measure</th></tr></thead><tbody><tr><td>SNP</td><td>98.91%</td><td>99.84%</td><td>99.37%</td></tr><tr><td>INDEL</td><td>97.32%</td><td>96.84%</td><td>96.84%</td></tr></tbody></table></div><p>从图表上看, 虽然目前分布式GATK还是beta特性, 但是准确率已经和单机版很接近, 并且有部分还超越了.</p><center>单机版和分布式性能比较表</center><div class="table-container"><table><thead><tr><th>对比项</th><th>单机版本</th><th>分布式版本</th></tr></thead><tbody><tr><td>耗时</td><td>48H</td><td>3.5H</td></tr></tbody></table></div><blockquote><p>测试数据来源: <a href="http://smash.cs.berkeley.edu/datasets.html" target="_blank" rel="noopener">http://smash.cs.berkeley.edu/datasets.html</a></p></blockquote><p>但是, 新版本的GAKT遇到一个严重的BUG, 使用<code>ReadsPipelineSpark</code>的时候, 如果使用<code>hg38</code>的Reference就必然出现<code>StackOverflowError</code></p><blockquote><p>Bug reported @ <a href="https://github.com/broadinstitute/gatk/issues/5869" target="_blank" rel="noopener">issues-5869</a></p></blockquote><h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>这个问题有一个麻烦点就是, 从异常栈只能看出是序列化的时候出问题了, 但无法定位哪一行出现的问题, 因此需要首先定界出问题的代码行.</p><h3 id="问题定界"><a href="#问题定界" class="headerlink" title="问题定界"></a>问题定界</h3><p>一般来说, 定界问题有两种:  Debug大法和Print大法. </p><p>Debug大法适合你已经对代码有一定的了解, 并有相应的测试用例支持, 这样做比较事半功倍. </p><p>而Print大法比较适合现在这种情况, 对GATK的源码不是很熟悉, 而且不知道如何构造简单用例复现问题的时候, 这个时候就在<code>ReadsPipelineSpark</code>的代码路径里面, 打满日志, 根据报错前的日志, 定界出错代码位置.</p><p>经过打印了解到, <code>StackOverflowError</code>发生在这个代码段之中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Broadcast&lt;Supplier&lt;AssemblyRegionEvaluator&gt;&gt; assemblyRegionEvaluatorSupplierBroadcast(</span><br><span class="line">        <span class="keyword">final</span> JavaSparkContext ctx,</span><br><span class="line">        <span class="keyword">final</span> HaplotypeCallerArgumentCollection hcArgs,</span><br><span class="line">        <span class="keyword">final</span> SAMFileHeader header,</span><br><span class="line">        <span class="keyword">final</span> String reference,</span><br><span class="line">        <span class="keyword">final</span> Collection&lt;Annotation&gt; annotations) &#123;</span><br><span class="line">    <span class="keyword">final</span> Path referencePath = IOUtils.getPath(reference);</span><br><span class="line">    <span class="keyword">final</span> String referenceFileName = referencePath.getFileName().toString();</span><br><span class="line">    <span class="keyword">final</span> ReferenceSequenceFile taskReferenceSequenceFile = taskReferenceSequenceFile(referenceFileName);</span><br><span class="line">    <span class="keyword">final</span> VariantAnnotatorEngine annotatorEngine = <span class="keyword">new</span> VariantAnnotatorEngine(annotations,  hcArgs.dbsnp.dbsnp, hcArgs.comps, hcArgs.emitReferenceConfidence != ReferenceConfidenceMode.NONE, <span class="keyword">false</span>);</span><br><span class="line">    <span class="keyword">return</span> assemblyRegionEvaluatorSupplierBroadcastFunction(ctx, hcArgs, header, taskReferenceSequenceFile, annotatorEngine);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的大致逻辑是比较清楚的, 就是<code>Driver</code>将一部分信息通过Spark的<code>广播机制</code>发布到<code>Executor</code>里面, 这个会有序列化的动作.</p><p>序列化的对象有: <code>hcArgs</code>,<code>header</code>,<code>taskReferenceSequenceFile</code>,<code>annotatorEngine</code>这四个, 具体是哪一个呢? </p><p>这时候祭出<code>Save-Load</code>大法, 将其中某个值设置null, 再一次次的尝试, 最后发现<code>taskReferenceSequenceFile</code>设置为null的时候, 代码能走过这段逻辑.</p><blockquote><p>当然SL大法在用的时候, 经常被自己的先验知识影响, 当时重点一直在怀疑<code>header</code>和<code>annotatorEngine</code>这两个字段, 一直没想到Reference会有问题, 绕了不少弯路, 因此SL一次的时间还是挺长的.</p></blockquote><h3 id="问题定位f"><a href="#问题定位f" class="headerlink" title="问题定位f"></a>问题定位f</h3><p>上面已经定位出来是<code>ReferenceSequenceFile</code>这个类导致的问题, 那么这个类的哪一部分出问题了呢? 这个时候就要用到Debug大法. </p><p>构造一次测试用例:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparkContext sc;</span><br><span class="line">ReferenceSequenceFile ref = <span class="keyword">new</span> UserDefinedReferenceSequenceFile()</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    sc.broadcast(ref)</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    t.printStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在<code>UserDefinedReferenceSequenceFile</code>里面不断将其中的字段加入进去, 最后发现以下代码片段导致整个<code>StackOverflowError</code>的问题:</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/iterator_serializable/001.png" alt></div></p><p>就是这个<code>Iterator</code>在序列化的时候, 会不断的递归遍历, 导致栈溢出.<br><code>Iterator</code>的实现为<code>LinkedHashIterator</code>, 里面<code>LinkedHashMap.Entry</code>为一个二叉树, 因此, 需要真实序列化的, 就会不断去遍历整个二叉树, 导致问题整个.</p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/iterator_serializable/002.png" alt></div></p><p><div align="center"><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/iterator_serializable/003.png" alt></div></p><h2 id="问题修复方案"><a href="#问题修复方案" class="headerlink" title="问题修复方案"></a>问题修复方案</h2><p>社区已经有解决的<a href="https://github.com/broadinstitute/gatk/pull/5950" target="_blank" rel="noopener">方案</a>了,  总体的方式就是不要在<code>Driver</code>加载Reference文件, 而是放在<code>Executor</code>, 这样就能免去序列化的步骤. </p><blockquote><p>社区的问题目的是为了解决内存问题, 但实际上这个是序列化的问题. 此外实际运行的时候, 还有一个内存OMM的问题, 这个并不能够解决.</p></blockquote><h3 id="为什么这个问题值得记录"><a href="#为什么这个问题值得记录" class="headerlink" title="为什么这个问题值得记录"></a>为什么这个问题值得记录</h3><ul><li>首先, 迭代器模式竟然会出现<code>StackOverFlowError</code>, 这个真的没想到.</li><li>其次, 对于陌生代码的定位方式记录一下.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[转]几道抛硬币问题</title>
      <link href="/2019/06/13/%E8%BD%AC-%E5%87%A0%E9%81%93%E6%8A%9B%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98/"/>
      <url>/2019/06/13/%E8%BD%AC-%E5%87%A0%E9%81%93%E6%8A%9B%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>首先说明转载, 本文转自<a href="https://www.raychase.net/3144" target="_blank" rel="noopener">这里</a></p></blockquote><h3 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h3><blockquote><p>1、平均需要抛掷多少次硬币，才会首次出现连续的两个正面？</p></blockquote><p>假设连续两个正面的期望是 E，那么，先看第一次抛硬币：</p><ol><li>如果抛到反面，那么还期望抛 E 次，因为抛到反面完全没用，总数就期望抛 E+1</li><li>如果抛到正面，那么要看下一次，如果下一次也是正面，那抛硬币就结束了，总数是 2；如果下一次是反面，那么相当于重头来过，总数就期望抛 E+2</li></ol><p>于是可以得到如下关系式：</p><blockquote><p>E = 0.5(E+1) + 0.25*2 + 0.25(E+2)</p></blockquote><p>得到所求期望 E=6</p><p>现在把题目拓展，不是说“连续两个正面”，而是“连续 n 个正面”呢？</p><p>这个问题 Matrix67 有非常有趣的解答 <a href="http://www.matrix67.com/blog/archives/3638" target="_blank" rel="noopener">《用数学解赌博问题不稀奇，用赌博解数学问题才牛 B》</a>，下面我简述一下：</p><p>假设有一个赌场，赌博的方式就是猜正反，每来一个玩家来的时候都只带了 1 元，每次都会全部下注，然后赌正面，庄家抛硬币，如果猜错就是全部输掉，如果赢了就得到下注的两倍，玩家会一直玩一直玩直到钱输光；而赌场老板会看，如果有人赢到 2^n 元，就下令关闭赌场。</p><p>于是直到 n 次正面朝上的情况发生，赌场关闭，只有最后那 n 个人才赚到了钱，最后一人得到了 2 元（没算成本价 1 元），倒数第二人是 4 元……倒数第 n 人是 2^n 元，所以，一共得到（等比数列求和）：</p><blockquote><p>2+4+8+…+2^n = 2*(1-2^n)/(1-2) = 2^(n+1) – 2</p></blockquote><p>赌场有多少钱流入，自然就有多少钱流出，所以到赌场倒闭，玩家赢得的钱的总数，就应该等于赌场期望的收入。而因为每个人来的时候都只带了 1 元，因此这个数正好等于期望的人数。于是这就是最终答案。</p><h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><blockquote><p>2、一堆硬币，每天都随便捡一枚抛，如果抛到正面，就把它翻过来；如果抛到反面，就再抛一下，问很长很长时间以后，硬币正面和反面的比例会趋近于多少？</p></blockquote><p>假设正面的比例是 x，那么反面就是 1-x，对于任意一次操作：</p><ul><li>如果抛到正面，那么得到的就一定是反面了；</li><li>如果抛到反面，那么得到正面的可能性为 0.5，反面的也为 0.5。</li></ul><p>所以得到正面的综合起来的概率为：</p><blockquote><p>x<em>0 + (1-x)</em>0.5 = x</p></blockquote><p>所以 x = 1/3，因此硬币正面和反面的比例会趋近于 x/(1-x) = 1/2</p><h3 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h3><blockquote><p>3、连续抛硬币，直到第一次出现连续两次正面为止，恰好抛了 N 次的概率是多少？</p></blockquote><p>考虑“恰好”抛 N 次硬币，到底有多少种情况可以得出最后两次是连续出现了正面，而之前没有出现过连续正面。</p><ul><li>假设 f(x) 表示第一次出现连续正面的时候，已经抛了 x 次，并且整个过程的第一次抛出的结果是反面；</li><li>假设 g(x) 表示第一次出现连续正面的时候，已经抛了 x 次，并且整个过程的第一次抛出的结果是正面。</li></ul><p>所以 f(1)=f(2)=0，g(1)=0，g(2)=1，而当 x&gt;2，</p><ul><li>求 f(x+1)，因为第一次是反面，所以这新添加的第一次不影响结果，因此 f(x+1)=f(x)+g(x)</li><li>求 g(x+1)，因为第一次是正面，必须要保证第二次不能为正，所以 g(x+1)=f(x)</li></ul><p>于是得到：</p><blockquote><p>f(x+2)=f(x+1)+g(x+1)=f(x+1)+f(x)</p><p>g(x+1)=f(x)</p></blockquote><p>其中，求 f(x) 的递推式可以看出 f(x) 是斐波那契数列，根据它的通项公式：</p><p><img src="https://www.raychase.net/wp-content/uploads/2015/07/coin-tossFibonacci_number.png" alt="Fibonacci_number"></p><p>得到 f(N)，也就得到了 g(N)，而总抛的可能性共有 2^N 次方，因此，概率为：</p><blockquote><p>(f(N)+g(N))/2^N</p></blockquote><h3 id="问题四"><a href="#问题四" class="headerlink" title="问题四"></a>问题四</h3><blockquote><p>4、抛硬币 N 次，出现连续 M 次正面的概率是多少？</p></blockquote><p>这个问题也很常见，但是做起来没那么容易，这里有一个 <a href="http://bbs.emath.ac.cn/thread-667-1-1.html" target="_blank" rel="noopener">非常详细的讨论过程（链接）</a>，我就不搬过来了。</p><p>结果公式为: $\frac  {F^M_N} {2^N}$</p><h3 id="问题五"><a href="#问题五" class="headerlink" title="问题五"></a>问题五</h3><blockquote><p>5、抛 N 次硬币，正反两面出现次数相同的概率是多少？</p></blockquote><p>其实就是从 N 个硬币的空位中，选出 N/2 个作为正面，余下 N/2 个作为反面，应用组合公式可得到：</p><blockquote><p>C(N,N/2)/2^N=N!/((N-N/2)!(N/2)!)/2^N</p></blockquote><p>继续，</p><blockquote><p>正面出现次数超过反面的概率？</p></blockquote><p>因为正反情况相同，因此正面次数超过反面的概率应当等于反面次数超过正面的概率，因此结果为 1 减去上面那一问的结果之后除以 2：</p><blockquote><p>(1-C(N,N/2)/2^N)/2</p></blockquote><h3 id="MathJax公式书写规范"><a href="#MathJax公式书写规范" class="headerlink" title="MathJax公式书写规范"></a>MathJax公式书写规范</h3><p>$\frac {1 -  \frac {C{^N_{N/2}}} {2^N}}  2 \text {，mathjax最后计算公式示例}$</p><p>参考这篇<a href="https://www.jianshu.com/p/16fbd768bfe7" target="_blank" rel="noopener">博客</a> 或者 <a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">这篇</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>加解密技术</title>
      <link href="/2019/06/05/%E5%8A%A0%E8%A7%A3%E5%AF%86%E6%8A%80%E6%9C%AF/"/>
      <url>/2019/06/05/%E5%8A%A0%E8%A7%A3%E5%AF%86%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>本周在向组内培训安全编程规范的原则, 其中遇到了不少加解密算法相关的内容, 之前大致都了解过, 但是经常忘记, 因此今天特地写个博客把其中的内容都记录一下. </p><p>本片博客内容, 参考了不少<a href="https://songlee24.github.io/2015/05/03/public-key-and-private-key/" target="_blank" rel="noopener">这篇文章</a>的内容,  有兴趣可以看原博客, 写的比我好.</p><h3 id="对称加密与非对称加密"><a href="#对称加密与非对称加密" class="headerlink" title="对称加密与非对称加密"></a>对称加密与非对称加密</h3><p><strong>对称加密</strong>使用同一个密钥进行加密和解密, 因此这个密钥是不能公开的, 那么密钥的分发就成了一个大问题, 如何在不受信任的网络之中分发密钥, 成为限制<strong>对称加密</strong>算法的痛点.</p><p>而<strong>非对称加密</strong>有两个密钥, 一个称之为<strong>公钥</strong>, 一个称之为<strong>私钥</strong>. <strong>公钥</strong>顾名思义可以在不受信任的网络之中分发, 无论攻击者是否获取了公钥都无法密文.</p><p><strong>非对称加密</strong>要实现这个能力, 需要有以下特点:</p><ol><li>公钥和私钥成对出现, 换而言之: <em>一把公钥有且只有一把私钥, 反之亦然</em></li><li>公钥加密的数据有且只能由对应的私钥解密, 反之亦然</li></ol><h3 id="非对称加密的传输过程"><a href="#非对称加密的传输过程" class="headerlink" title="非对称加密的传输过程"></a>非对称加密的传输过程</h3><ol><li><strong>接收方(解密者, 持有私钥)</strong> 生成一个密钥, 接收方将公钥广播给<strong>发送方(加密者, 持有公钥)</strong></li><li><strong>发送方</strong>将明文字符串安装某种字符串编码格式变成字节流</li><li><strong>发送方</strong>使用公钥对明文数据加密, 再将密文发送给<strong>接收方</strong></li><li><strong>接收方</strong>获得密文, 并使用私钥进行解密, 获得明文字节流</li><li><strong>接收方</strong>按照字符串编码格式加字节流解码为明文字符串</li></ol><p>整个步骤如下图所示(步骤一一对应): </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/encrypt/20150502122610368.jpg" alt></p><p>已知在网络公开的数据有两种: 公钥和密文, 由于非对称算法的特性, 公钥无法对密文进行解密,  这样就保证了数据传输的安全性.</p><h3 id="数字签名"><a href="#数字签名" class="headerlink" title="数字签名"></a>数字签名</h3><p>签名, 顾名思义就是告诉别人, 这就是我.  在刚才<strong>非对称加密</strong>传输的例子是公钥持有者发送数据给私钥持有者, 如果反过来, 私钥持有者将数据加密之后发送给公钥持有者, 公钥持有者使用公钥解密数据, 这里由于公钥和数据都是公开的, 那么明文数据是什么已经不重要了, 重要的是<strong>这个文件就是私钥持有者加密的</strong>, 这个就是数字签名. </p><p>它通过非对称加密的原理, 由公钥来验证私钥持有者的身份, 防止密文内容被串改.</p><blockquote><p>当然公钥的安全性是先验的, 也就是说验证者必须知道自己公钥是什么, 而且公钥内容不能被攻击者篡改, 不然给了假的公钥和假的密文, 就往验证者系统注入一个受信内容.</p></blockquote><h3 id="非对称加密算法的缺点"><a href="#非对称加密算法的缺点" class="headerlink" title="非对称加密算法的缺点"></a>非对称加密算法的缺点</h3><p><strong>非对称加密算法</strong>的执行效率比<strong>对称加密算法</strong>慢了2-3个数量级, 如果用非对称加密算法去加密数据性能不能接受, 因此在实际上, 这两者一般结合起来一起使用.</p><p><strong>对称加密算法</strong>的缺点就是<strong>无法安全的传输密钥</strong>, 那么就由<strong>非对称加密算法</strong>传输密钥就好了:</p><ol><li><strong>接收方(持有非对称算法的私钥, 解密者)</strong>通过公钥机制生成一对密钥，一个公钥，一个私钥, 并发送给<strong>发送方(持有对称算法的密钥和非对称算法的公钥, 加密者)</strong></li><li><strong>发送方</strong>将明文字符串安装某种字符串编码格式变成字节流</li><li><strong>发送方</strong>使用非对称算法公钥对对称算法的密钥加密, 获得<strong>加密后的对称密钥</strong></li><li><strong>发送方</strong>使用对称加密的密钥对字节流进行加密</li><li><strong>发送方</strong>将3和4步骤的数据集编码之后发送给<strong>接收方</strong></li><li><strong>接收方</strong>拆分掉这两部分数据: <strong>加密后的对称密钥</strong> 和 <strong>密文</strong></li><li><strong>接收方</strong>用私钥进行解密得到对称算法的密钥。</li><li><strong>接收方</strong>再在用对称加密的密钥对原始的密文进行解密</li><li><strong>接收方</strong>按照字符串编码格式加字节流解码为明文字符串</li></ol><p>整个步骤如下图所示(步骤一一对应): </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/encrypt/20150502122733376.jpg" alt></p><blockquote><p>这个图中, <strong>发送者</strong>和<strong>发送者</strong>只有两次交互, 这应该只是为了画图方便, 实际场景之中, 没必要将密钥和密文一起发送, 密文的传输应该就在其中握手过程完成.</p></blockquote><h3 id="常见的对称和非对称加密算法"><a href="#常见的对称和非对称加密算法" class="headerlink" title="常见的对称和非对称加密算法"></a>常见的对称和非对称加密算法</h3><p><strong>对称加密</strong>最常见的是AES和DES, 但是DES是一种弱加密算法, 不推荐使用<br><strong>不对称加密</strong>常见的有RSA/DSA/SHA256</p><p>秘钥的长度最低为:<br>AES: 128位<br>RSA: 2048位<br>DSA: 1024位<br>SHA: 256位</p><h2 id="非对称加密的数学原理"><a href="#非对称加密的数学原理" class="headerlink" title="非对称加密的数学原理"></a>非对称加密的数学原理</h2><iframe src="//player.bilibili.com/player.html?aid=26639065&cid=45813166&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" width="640" height="420" allowfullscreen="true"> </iframe><p>直接丟一个B站的视频, 李永乐老师对RSA算法讲解的非常通熟易懂, 一定要看一遍.</p><h2 id="HTTPS认证过程"><a href="#HTTPS认证过程" class="headerlink" title="HTTPS认证过程"></a>HTTPS认证过程</h2><blockquote><p>TBD</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
            <tag> TBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[2019-06-01]再看阿甘正传有感</title>
      <link href="/2019/06/01/2019-06-01-%E5%86%8D%E7%9C%8B%E9%98%BF%E7%94%98%E6%AD%A3%E4%BC%A0%E6%9C%89%E6%84%9F/"/>
      <url>/2019/06/01/2019-06-01-%E5%86%8D%E7%9C%8B%E9%98%BF%E7%94%98%E6%AD%A3%E4%BC%A0%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘由"><a href="#缘由" class="headerlink" title="缘由"></a>缘由</h2><p>前几天的时候, 看了&lt;阿甘正传&gt;的视频节目, 昨天晚上抽空又重温了一遍.  </p><p>说起来我已经有看过4遍的&lt;阿甘正传&gt;了:</p><ul><li>第一遍在高中时候看的, 那个时候觉得这个片子特别的爽, 毕竟是一个逆袭的故事嘛, 和大多数的网络小说的套路实际上类型. </li><li>第二遍是在大学的时候看的, 那个时候思绪关心政治历史哲学等内容, 特别关注于电影里面的历史进程, 觉得阿甘真的是历史见证者, 这也是另外一种意淫, 和大多数的穿越小说也类似.  </li><li>第三遍是在毕业之后看的, 那个时候关注电影的构建, 开始注意电影之中的人物塑造, 渐渐去了解了阿甘/珍妮/丹中尉/母亲等角色的性格</li><li>第四遍是昨天的看的, 发现了其中一个之前我一直没关注到的点: <strong>关于命运的思考</strong>. 在珍妮死去之后, 阿甘在墓前的那场戏, 实际上看着整部电影的文眼, 他让我思绪万千, 所以我就打算写个影评记录一下.</li></ul><h2 id="人物剖析"><a href="#人物剖析" class="headerlink" title="人物剖析"></a>人物剖析</h2><p>&lt;阿甘正传&gt;里面实际上描述的人物非常少,  因为整部电影都是以阿甘的视角展开的, 只有珍妮有一部分闪切的镜头来描述她的人生. 下面就依次描述一下对阿甘来说最重要的认.</p><h3 id="阿甘母亲"><a href="#阿甘母亲" class="headerlink" title="阿甘母亲"></a>阿甘母亲</h3><p>阿甘的口头禅是”妈妈曾经说过”, 说明他大部分的人生思考实际来自他妈妈,  她肯定是阿甘人生最重要的人.  她的形象应该是一个<strong>比较完美的乡下母亲</strong>, 当阿甘出生之后整个人生都围绕在阿甘身边:</p><ol><li><p>为让阿甘能上学而被潜规则</p></li><li><p>阿甘大学毕业之后, 以他为荣</p></li><li><p>尊重阿甘的人生选择, 并没有因为阿甘是低能儿而各种安排阿甘的人生.</p></li><li><p>耐心的教育阿甘, 特别是她死亡的时候那个场景, 能够那么平和的面对人生的结局, 并还在教育阿甘如何面对死亡/面对亲人的离去.  我自己有幻想今后如何教育自己的子女如何面对死亡, 因为这是一个无法逃避的问题, 我想阿甘母亲说的, 可能就是最好的注解了.</p></li></ol><p>阿甘的母亲在电影塑造上是非常 符号话的, 只有人物的形象, 并没有人物的血肉.</p><h3 id="丹中尉"><a href="#丹中尉" class="headerlink" title="丹中尉"></a>丹中尉</h3><p>这角色其实和阿甘母亲一样, 只是一个人物符号, 他是完全服务于电影另一个主题的表达: <strong>越战反思</strong>.<br>丹中尉出生一个电影的军人世家, 以为国牺牲为荣, 但是在越战之中受伤, 失去双腿, 身体残疾了; 国内反战严重, 不以军人为荣, 患上了PTSD, 精神也残疾了.<br>在现实之中, 他最大概率死于自杀, 但是由于&lt;阿甘正传&gt;属于正能量电影, 所以后面他正做起来了, 最后还娶了一个越南籍的老婆, 构成了对越战的反讽.</p><h3 id="珍妮"><a href="#珍妮" class="headerlink" title="珍妮"></a>珍妮</h3><p>珍妮是&lt;阿甘正传&gt;里面唯一有单独场景的人物(<em>其他人物都是和阿甘在一起的场景</em>) , 这个人物主要是为了与阿甘的人生做一个对比的, 象征着美国的两种完全不一样的思维方式.</p><p>阿甘是那种典型美国保守派的思路, 而珍妮是所谓的”垮掉的一代”的典型, 嬉皮士/反战/黑人运动, 哪儿有热闹就往哪儿钻. 她性解放/滥交/吸毒, 基本上所有传统道德反对的事情, 她都做.</p><p>但是最后阿甘和珍妮还是在一起了, 生下来了一个小阿甘, 因为阿甘和珍妮都是美国的一个象征, 一体两面. 最后以传统思维的胜利而结束, 因此珍妮就必须死去了, 但是留下了小阿甘, 代表了两种思潮合并后的现代美国, 继续生活下去了.</p><h3 id="阿甘"><a href="#阿甘" class="headerlink" title="阿甘"></a>阿甘</h3><p>和珍妮不同, 阿甘是所有美国保守派对于美好道德的憧憬: <strong>尽管人有缺陷, 但是道德是完美的</strong></p><p>阿甘在电影虽然戏份太多, 但是人物形象基本上很平, 首先是他是一个低能儿, 因此就不可能展现太多的思考, 其次电影太多的让阿甘的人生轨迹贴近历史了, 实际上对于他自身的心里描述很少. </p><p>前面的三个角色都是阿甘人生里面不可或缺的角色: </p><ol><li>母亲是阿甘人生的导师</li><li>丹中尉是阿甘的经济来源, 阿甘捕虾事业实际上由丹中尉再管理</li><li>珍妮是阿甘人生的追求和比对.</li></ol><h2 id="再论珍妮"><a href="#再论珍妮" class="headerlink" title="再论珍妮"></a>再论珍妮</h2><p>上面介绍珍妮的时候，完全是按照电影角色的角度去思考的，虽然我觉得编剧应该就是这么构想的，因为&lt;阿甘正传&gt;算商业电影，这么设计符合受众的口味．但是我决定还是<strong>过度解读</strong>一下, 以人的角度来描述. 珍妮就是珍妮, 阿甘就是阿甘, 他们是在那个时代阿拉巴马州的两个小镇孩子的缩影.</p><h3 id="少年期"><a href="#少年期" class="headerlink" title="少年期"></a>少年期</h3><p>阿甘是个笨蛋, 腿上又有点残疾, 但有一个非常爱他的母亲, 人虽然傻, 好在对世界的认识都很美好.</p><p>而珍妮是个正常人, 但出生在一个不正常家庭, 父亲小时候性侵她, 她最后报警搬到祖母的房车上居住, 她最大希望是逃离这个乡下, 逃离这个童年.</p><h3 id="青年期"><a href="#青年期" class="headerlink" title="青年期"></a>青年期</h3><p>阿甘凭借打橄榄球读了大学了, 并顺利毕业, 毕业后不知道去哪儿就去当兵, 然后去了越南打战.</p><p>珍妮在大学里开始就解放了天性, 梦想当一个歌手, 站在一个瞩目的位置, 但是命运弄人, 只能靠肉体在杂志里出位.</p><h3 id="越战期间"><a href="#越战期间" class="headerlink" title="越战期间"></a>越战期间</h3><p>阿甘就一路开挂, 而珍妮则在参加嬉皮士运动, 反战等, 这段期间可以珍妮已经放弃了人生, 随波逐流, 跟随那些进步青年的口号, 走上了国内战场.</p><h3 id="越战归来"><a href="#越战归来" class="headerlink" title="越战归来"></a>越战归来</h3><p>阿甘开始当上的捕虾船长, 而这个珍妮就真的堕落了, 开始吸毒当妓女, 生活一片黑暗, 甚至想寻死. 珍妮想跳楼的那场戏是塑造人物最重要的点, 让观众了解珍妮不只是享乐主义的人, 她有她自己痛苦, 而且这场戏也是她人生的一个转折点.</p><h3 id="母亲过世"><a href="#母亲过世" class="headerlink" title="母亲过世"></a>母亲过世</h3><p>阿甘母亲过世之后, 阿甘就又回归到小镇生活了, 而珍妮也厌倦了之前的生活, 回到阿甘身边生活了一段时间. 最后阿甘向珍妮求婚之后, 珍妮再一次离别.</p><h3 id="奔跑吧"><a href="#奔跑吧" class="headerlink" title="奔跑吧"></a>奔跑吧</h3><p>之后, 阿甘就开启了长跑之路, 而珍妮也开始回归正常生活, 当起来了单身母亲</p><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>最后珍妮得病, 把小阿甘托付之后, 和阿甘结婚, 马上就过世了. 阿甘就独立带孩子, 完成新的一个轮循环.</p><h3 id="三次离别"><a href="#三次离别" class="headerlink" title="三次离别"></a>三次离别</h3><p>珍妮有三次和阿甘分道扬镳, 一次在越战之前, 一次在越战之后, 最后一次在小镇生活过一段时间之后.</p><p>第一次的时候, 两个人的追求不同, 珍妮希望当明星, 而阿甘没有追求</p><p>第二次的时候, 两个人的人生不同, 珍妮显然看不上阿甘甘于寂寞的人生.</p><p>第三次的时候, 两个人的经历不同, 珍妮有点看不起自己, 另外不想住在greenblow</p><h3 id="为什么要奔跑"><a href="#为什么要奔跑" class="headerlink" title="为什么要奔跑"></a>为什么要奔跑</h3><p>阿甘跑遍美国的时候, 一直解释说自己没有目的, 他确实很没有目的, 但又有目的: 阿甘母亲过世之前, 她一直是阿甘的精神支柱; 她过世之后, 珍妮就成了他人生的唯一意义, 当珍妮来到greenblow的时候, 阿甘觉得是这辈子最幸福的时候, 但当珍妮再次离开的时候, 他觉得整个人生的意义就不存在了. </p><p>其中有一场戏, 阿甘看着空空荡荡的房间, 看见珍妮留在房间的总统勋章, 演技是真的很好: 有一种心死了的感觉. 换一个正常的人, 可能会想着寻死觅活, 而阿甘想到就是珍妮的那句话, 跑.</p><h3 id="珍妮的悲剧"><a href="#珍妮的悲剧" class="headerlink" title="珍妮的悲剧"></a>珍妮的悲剧</h3><p>珍妮和阿甘的悲剧最大的原因在于, 珍妮是比较了解阿甘的, 但是阿甘是完全不了解珍妮的.</p><p>珍妮知道阿甘的痛苦: 当在电视里面看到阿甘在跑的时候, 珍妮就知道阿甘非常痛苦, 她知道阿甘为什么痛苦, 所以准备回到阿甘身边(<em>写信</em>), 也开始关注阿甘的动态(<em>剪报</em>) .</p><p>而阿甘不知道珍妮的痛苦: 珍妮朝老房子扔石头的时候, 阿甘是不知道为什么的, 到最后只知道珍妮讨厌老房子, 就把她推平了</p><h3 id="珍妮的救赎"><a href="#珍妮的救赎" class="headerlink" title="珍妮的救赎"></a>珍妮的救赎</h3><p>珍妮的人生悲剧在于那个悲惨的童年, 她一直想逃离, 但一直都无法逃离. 最后一个离开阿甘的时候, 说道”我没有逃”. 但是我觉得她确实不想逃了, 但是并没有完全放下: 她并不想住在小时候的地方, 而阿甘却一直想住在童年的地方. 所以她第三次的时候, 又逃离了.</p><p>而真正让珍妮放下的是, 她有了下一代, 当人有下一代的时候, 人的责任心就不一样了.</p><p>之前只想着自己的生活, 之后大多数的考虑都围绕在孩子在周围.</p><p><strong>有了下一代的含义, 就是这一代结束了</strong></p><p>当珍妮在剪报的时候, 她已经完成了救赎, 她结束了自己的使命.</p><h2 id="关于命运"><a href="#关于命运" class="headerlink" title="关于命运"></a>关于命运</h2><p>之前说过, 阿甘在珍妮的墓前说:</p><blockquote><p>这一切是命中注定的, 还是命运无常, 他觉得两者都有.</p></blockquote><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gum/Selection_018.png" alt></p><p>这里主语不清楚, 我觉得他应该说的是他和珍妮的人生, 命中注定说的是结局, 命运无常说的是过程.</p><p>但我想说的是, 阿甘是命中注定, 而珍妮是命运无常.</p><p>阿甘虽然脑子不太好使, 但是有一个特别能跑的优点, 又有主角光环, 关键还有一个珍妮值得它去爱,  这三个优点无论去掉哪个, 阿甘的人生都暗淡的.</p><p>而珍妮虽然人长得漂亮, 但是出生于一个恶劣的家庭环境, 个人又没有明显的特长, 人生旅途之中又没有一个真正能拯救她的人, 唯一一个爱他的阿甘, 又根本无法了解她的内心, 可以说她一直都是孤独的.</p><p>站在珍妮的角度, 能死在阿甘的怀里, 可能已经是她最幸福的结局了. 她确实是一个可怜的人.</p><p><strong>命运是一种看不见摸不着的东西, 但你想摆脱它, 那又比登天一样难</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&lt;阿甘正传&gt;并非一部非常经得起分析的电影, 特别是关于人物方面的, 但是我还是推荐你们多去看几遍, 因为我知道大多数人是不会去看这种贴近历史的严肃文学的, 例如我看过的&lt;白鹿原&gt;, 也是以历史为背景介绍一个村子人的变化, 虽然着眼点很小, 但是里面的人物确实觉得是真实的人.</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GATK4.0 Spark性能分析</title>
      <link href="/2018/09/08/GATK4-0-Spark%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
      <url>/2018/09/08/GATK4-0-Spark%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇文章讲记录GATK4.0 Spark流程之中性能点的分析.<br>目前我们团队已经把整个参数调优已经做到了极致, 目前已经识别的这些性能优化点只能优化GATK代码方式去调优了, 这也是我们团队接下来的一个重要工作.</p><p>因此, 这篇文章讲包括:</p><ol><li>GATK步骤的主要瓶颈点</li><li>以及造成该瓶颈的原因</li><li>优化建议</li></ol><p>这篇文章不包括:</p><ol><li>具体的GATK调优的参数</li><li>具体的Spark调优的参数</li><li>GATK算法相关瓶颈点(目前能力不够, 以后有机会再分析)</li><li>GATK源码级调优</li></ol><h2 id="流程剖析"><a href="#流程剖析" class="headerlink" title="流程剖析"></a>流程剖析</h2><p>之前已经有介绍<a href="https://saintbacchus.github.io/2018/08/19/%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8CWGS%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">GATK4.0 Spark</a>的测序流程, 这里就按照里面的步骤一个一个分析吧.</p><p>分析的样本为NA12878的fastq.gz(98GB)文件, Ref使用HG38(3G), KnownSites使用GATK官网文件, 总大小为8G<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1000g_omni2.5.hg38.vcf</span><br><span class="line">1000g_phase1.snps.high_confidence.hg38.vcf</span><br><span class="line">dbsnp_144.hg38.vcf</span><br><span class="line">hapmap_3.3.hg38.vcf</span><br><span class="line">homo_sapiens_assembly38.known_indels.vcf</span><br><span class="line">mills_and_1000g_gold_standard.indels.hg38.vcf</span><br></pre></td></tr></table></figure></p><h3 id="FastqToSam"><a href="#FastqToSam" class="headerlink" title="FastqToSam"></a>FastqToSam</h3><p>由于FastQ文件是GZ格式的, 而Gzip格式在HDFS上是无法切分的, 因此无法用Spark加速, 所以实际上<code>FastqToSam</code>工具是单机的.<br>这个步骤主要耗时是花费在<code>fastq.gz</code>解压缩之中, 目前我们处理这个步骤需要花费90分钟, 而使用linux的命令解压缩单个文件大约就需要1个小时以上, 因此基本上只要解压缩完就能处理完毕</p><p><strong>现状瓶颈:</strong> fastq.gz的文件解压缩</p><p><strong>瓶颈原因:</strong> <code>FastqToSam</code>调用的是Java的GZip库进行解压的, 因此整个性能和linux gzip命令的性能一样. 而gzip解压缩最大的问题在于是单线程解压的, 因此性能一直不高, 而且机器的CPU利用率也非常低.</p><p><strong>优化建议:</strong> 实际上<code>fastq.gz</code>格式不是linux上那种通用的gzip格式, 而且专门为生物信息领域设计的gzip格式, 因此它实际上可以多线程解压的. 经过测试使用<a href="http://www.htslib.org/download/" target="_blank" rel="noopener">bgzip</a>多线程解压, 性能可以从原有的1个小时, 提高到半个小时左右(4线程). 但是目前这个工具似乎只有C语言版本, <code>FastqToSam</code>是用java写的, 如果想利用这个步骤, 可能要用到JNI的技术来实现C语言的加载.</p><h3 id="BwaSpark"><a href="#BwaSpark" class="headerlink" title="BwaSpark"></a>BwaSpark</h3><p>首先看一样,<code>BwaSpark</code>的Spark的任务图<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/BwaSparkJobs.png" alt></p><p>总共有2个长时间的Job组成, 一个是<code>collect</code>,一个是<code>save</code></p><p>分析collect过程, 发现每个Task都很小, 绝大部分的时间加载Ref文件, 由于每个Executor都需要拉取5G左右的Ref文件, 当Executor达到50个以上的时候, 将近有250GB的数据需要在集群交互.</p><p>save过程, 是用BWA软件将每个小的Bam文件进行比对, 这个步骤目前无法优化.</p><p><strong>现状瓶颈:</strong> 获取HDFS数据慢</p><p><strong>瓶颈原因:</strong> 每个Executor都需要拉取5G左右的Ref, HDFS无法承受如此高的资源.</p><p><strong>优化建议:</strong> 提前加载Ref的Image的数据, 让整个流程快速开始计算.</p><h3 id="ReadsPipelineSpark"><a href="#ReadsPipelineSpark" class="headerlink" title="ReadsPipelineSpark"></a>ReadsPipelineSpark</h3><p>看一下, <code>ReadsPipelineSpark</code>的Spark的任务图<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/ReadsPipelineSpark_Jobs.png" alt></p><p>这个时间需要消耗在<code>BaseRecalibratorSpark</code>和<code>VariantsSparkSink</code>,前者是BQSR的计算流程, 后者是<code>HaplotypeCallerSpark</code>的流程.</p><p><code>HaplotypeCallerSpark</code>只消耗了32min符合预期, 但是BQSR流程耗时1.2小时, 时间慢的不正常.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/BQSR_Jobs.png" alt><br>打开最慢的那个步骤的日志, 发现加载KnownSites文件将近耗费了25分钟, 整个流程被阻塞近半个小时.<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/BQSR_Logs.png" alt></p><p><strong>现状瓶颈:</strong> KnownSites文件加载过慢</p><p><strong>瓶颈原因:</strong> 每个Exeuctor都要计算并加载整个KnownSites</p><p><strong>优化建议:</strong> 提前加载KnownSites的数据, 让整个流程快速开始计算.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前对于Spark WGS流程的优化点, 有以下两个:</p><ol><li>使用bgzip加速FastQ转化Bam</li><li>将整个GATK作为Service, 提前加载好所有的资源文件, 客户端提交工具请求, 服务端直接开始计算. 作为完全的Serviceless的服务.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GATK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Spark进行WGS分析</title>
      <link href="/2018/08/19/%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8CWGS%E5%88%86%E6%9E%90/"/>
      <url>/2018/08/19/%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8CWGS%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="GATK简介"><a href="#GATK简介" class="headerlink" title="GATK简介"></a>GATK简介</h2><p>GATK全称叫做: Genome Analysis Toolkit. 是Broad Institute开发的用于二代重测序数据分析的一款软件.<br>目前主要用于人类的<strong>WGS</strong>以及<strong>WES</strong>基因测试流程, 具体流程介绍可以看官网的<a href="https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145" target="_blank" rel="noopener">最佳实践</a>  </p><p>GATK3版本之前, 一直都是单机版本, 性能一直是瓶颈点, 做完一个WGS的流程大约需要3天时间. 因此在GATK4以后的版本之中, 引入Spark做分布式性能优化, GATK4.0版本可以讲整个WGS测序流程的时间压缩在半天之内, 性能提高将近10倍有余.</p><p>但是, 目前所有标注有Spark加速的工具都是<code>BETA Tool</code>, 虽然就我们测试来看敏感度和准确性都和单机版本没有太大区别, 但是由于整理功能开发阶段, 工具接口可能会调整, 因此如果想应用到生产系统上的话, 也请慎重选择.</p><blockquote><p>WGS: Whole Genome Sequencing 全基因组测序</p><p>WES: Whole Exome Sequencing  全外显子测测序</p><h2 id="WGS流程简介"><a href="#WGS流程简介" class="headerlink" title="WGS流程简介"></a>WGS流程简介</h2><p>在GATK的最佳实践里面, 有流程介绍, 也有<a href="https://github.com/gatk-workflows/gatk4-germline-snps-indels" target="_blank" rel="noopener">样例程序</a>供大家参考</p></blockquote><p>但是如果大家之前没有接触过WGS的话, 看官网的介绍还是有点晕. 推荐看一下<strong>碱基矿工</strong>的<a href="http://www.huangshujia.me/2018/02/20/2018-02-20-WGS-Best-Practics.html" target="_blank" rel="noopener">GATK4.0和全基因组数据分析实践</a></p><p>好了, 言归正传, 我在这儿简单总结一下WGS的流程:  </p><ol><li>获取数据 — 脱机数据转化成FastQ格式</li><li>数据质控 — 使用<code>Fastqc</code>工具过滤掉低质量的数据</li><li>比对排序 — 使用<code>Bwa + samtools</code>工具对FastQ进行比对排序, 并将格式转化为Bam格式</li><li>碱基去重 — 使用GATK的<code>MarkDuplicates</code>工具完成该步骤</li><li>碱基矫正 — 使用GATK的<code>BQSR</code>工具完成该步骤</li><li>变异检测 — 使用GATK的<code>HaplotypeCaller</code>工具完成该步骤</li><li>变异控制 — 使用GATK的<code>VQSR</code>工具完成该步骤</li></ol><p>最后我们实现的一个功能是将原始的FastQ个数的数据, 转化为VCF格式的数据, 完成整个WGS的流程.</p><blockquote><p>VCF(Variant Call Format): 是种文本格式变异数据的格式, 可以直接用文本编辑器查看里面的字段及数据, 字段含义<a href="http://samtools.github.io/hts-specs/VCFv4.2.pdf" target="_blank" rel="noopener">如文所示</a></p></blockquote><h2 id="使用Spark进行WGS分析流程"><a href="#使用Spark进行WGS分析流程" class="headerlink" title="使用Spark进行WGS分析流程"></a>使用Spark进行WGS分析流程</h2><h3 id="流程介绍"><a href="#流程介绍" class="headerlink" title="流程介绍"></a>流程介绍</h3><p>使用Spark进行WGS的流程如下图所示: </p><ol><li>首先FastQ格式的原始数据,通过<code>FastqToSam</code>工具转化为<code>UBam</code>格式</li><li>接着使用<code>BwaSpark</code>方法进行比对, 输出经过比对的<code>Bam</code>格式数据</li><li>最后通过<code>ReadsPipelineSpark</code>进行变异检测,并将变异点输出为<code>VCF</code>格式</li></ol><div id="flowchart-0" class="flow-chart"></div><p><strong>下面讲详细介绍每个工具的用法及命令</strong></p><h3 id="FastqToSam"><a href="#FastqToSam" class="headerlink" title="FastqToSam"></a>FastqToSam</h3><p><em>官网简介</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Converts a FASTQ file to an unaligned BAM or SAM file.</span><br><span class="line"></span><br><span class="line">Output read records will contain the original base calls and quality scores will be translated depending on the base quality score encoding: FastqSanger, FastqSolexa and FastqIllumina.</span><br><span class="line"></span><br><span class="line">There are also arguments to provide values for SAM header and read attributes that are not present in FASTQ (e.g see RG or SM below).</span><br></pre></td></tr></table></figure></p><p>这个工具的作用就是做好格式转化, 并对BAM格式进行排序. <strong>这个工具是单机的, 无法使用Spark加速. 官方工具转化一个NA1278 30X的样本大致需要3-4个小时. 我们团队这个工具进行了优化, 使得转化时间提高到1.5小时,大大降低了样本转化的时间</strong></p><p><em>执行命令</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/gatk/gatk FastqToSam -F1 NA12878_1.fastq.gz -F2 NA12878_2.fastq.gz -O NA12878_unaligned.bam -SM SM1 -PL illumina -R hg19.fa</span><br></pre></td></tr></table></figure></p><p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_FastqToSam.php" target="_blank" rel="noopener">FastqToSam文档</a></p><h3 id="BwaSpark"><a href="#BwaSpark" class="headerlink" title="BwaSpark"></a>BwaSpark</h3><p>简介<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Align reads to a given reference using BWA on Spark</span><br></pre></td></tr></table></figure></p><p>这个工具本质上是使用HDFS分片的能力, 让Spark对BWA软件分布化. Spark的每个Task都比对一个<em>块大小</em>的<code>uBam</code>. 每个块大小由参数<code>--bam-partition-size</code>指定, 默认值是使用Hadoop默认的块大小.<br><strong>通过Spark的分布式话, 原有的比对时间从4小时,可以降低到1个小时左右.</strong>  </p><p><em>执行命令</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/gatk/gatk BwaSpark -I hdfs://hacluster/gatk/NA12878_unaligned.bam -O hdfs://hacluster/gatk/NA12878_aligned.bam -R hdfs://hacluster/gatk/hg19.2bit --spark-runner SPARK --spark-master yarn-cluster</span><br></pre></td></tr></table></figure></p><p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_bwa_BwaSpark.php" target="_blank" rel="noopener">BwaSpark文档</a></p><h3 id="ReadsPipelineSpark"><a href="#ReadsPipelineSpark" class="headerlink" title="ReadsPipelineSpark"></a>ReadsPipelineSpark</h3><p>简介<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Takes unaligned or aligned reads and runs BWA (if specified), MarkDuplicates, BQSR, and HaplotypeCaller to generate a VCF file of variants</span><br></pre></td></tr></table></figure></p><p>在之前的GATK版本之中, 每个命令都是单独的, 只能通过自己编写脚本的方式讲这些工具集串行起来.<br>而在GATK4.0之中, 想用<code>ReadsPipelineSpark</code>这一个工具, 将整个变异流程全部统一起来, 未来变异检测只要执行这个工具即可.   </p><blockquote><p>实际上<code>ReadsPipelineSpark</code>的流程能够包括<code>BwaSpark</code>的步骤, 但为什么还要特别分开呢? 原因在于: 目前<code>ReadsPipelineSpark</code>的实现之中没有缓存必要的RDD, 导致重计算, 整个性能没有分开计算好. 因此目前分成了两个步骤, 等社区解决了这个问题之后, 可以讲这两个步骤合并. 这也符合社区对于这个工具的定位.</p></blockquote><p><em>执行命令</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/gatk/gatk ReadsPipelineSpark -I hdfs://hacluster/gatk/NA12878_aligned.bam -O hdfs://hacluster/gatk/NA12878.vcf -R hdfs://hacluster/gatk/hg19.2bit --known-sites hdfs://hacluster/gatk_ref/dbsnp.vcf --spark-runner SPARK --spark-master yarn-cluster</span><br></pre></td></tr></table></figure></p><p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pipelines_ReadsPipelineSpark.php" target="_blank" rel="noopener">ReadsPipelineSpark文档</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>GATK4.0 在今年初引入了Spark进行分布式化性能优化, 整个WGS流程的性能由原有的<strong>天</strong>级别降低到<strong>小时</strong>级别, 性能得到极大优化.  </p><p>后续将从这个流程输出,分析一下目前使用Spark之中的性能瓶颈点以及源码级的分析</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h3><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>输入的数据可以有两种, 一种是<a href="http://www.huangshujia.me/2017/08/12/2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html" target="_blank" rel="noopener">FASTQ</a>格式, 另外一种是<a href="http://www.huangshujia.me/2017/11/27/2017-11-27-Begining-WGS-Data-Analysis-BAM-CRAM-And-SAM.html" target="_blank" rel="noopener">BAM</a>格式</p><blockquote><p>NA12878是人类基因测试里面最常用的实验数据</p><h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.gz</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.fai</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.64.alt</span><br></pre></td></tr></table></figure></blockquote><h4 id="knownsites"><a href="#knownsites" class="headerlink" title="knownsites"></a>knownsites</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 获取SNP的Knownsites: 1000G_phase1.snps.high_confidence</span></span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取Indel的Knownsites: Mills_and_1000G_gold_standard.indels</span></span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi</span><br></pre></td></tr></table></figure><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Input(FastQ)e=>end: Output(VCF)op1=>operation: FastqToSamop2=>operation: BwaSparkop3=>operation: ReadsPipelineSparkst->op1->op2->op3->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GATK </tag>
            
            <tag> SPARK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming Indroduce</title>
      <link href="/2015/10/30/Spark-Streaming-Indroduce/"/>
      <url>/2015/10/30/Spark-Streaming-Indroduce/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="面向人群"><a href="#面向人群" class="headerlink" title="面向人群"></a>面向人群</h3><ul><li>精通Java/Scala编程</li><li>Spark相关使用及编程经验</li><li>了解流应用架构</li></ul><hr><h3 id="简明介绍"><a href="#简明介绍" class="headerlink" title="简明介绍"></a>简明介绍</h3><p>Spark Streaming是一种基于微批量(<em>micro-batch</em>)方式计算和处理实时流数据执行框架。<br>Streaming依托于于Spark执行框架，将连续输入数据按批次切分，通过DStream(Discretized stream)来表征，然后按批次组装为Spark任务，放入Spark任务池中执行。<br><img src="http://spark.apache.org/docs/latest/img/streaming-flow.png" alt></p><center>图1 Streaming与Spark关系</center><p>因此基于<strong>micro-batch</strong>的Streaming必然会带有以下特征:</p><ul><li>高扩展性</li><li>高吞吐量</li><li>高可靠性</li><li>高延时性</li></ul><p>目前Streaming已经内置以下多种数据源和输出源的适配器，其中数据源使用比较多的是Kafka和HDFS，输出源一般都为HDFS。<br><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Streming输入输出"></p><center>图2 Streming输入输出</center><p>Streaming目前使用的案例并不是特别多，<a href="http://www.infoq.com/cn/news/2014/04/spark-streaming-bidding" target="_blank" rel="noopener">Sharethrough</a>和<a href="http://www.infoq.com/cn/news/2015/04/pinterest-memsql-spark-streaming" target="_blank" rel="noopener">Pinterest</a>是比较明确Streaming的使用者:).</p><h3 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h3><!-- 架构层设计 --><h4 id="micro-batch"><a href="#micro-batch" class="headerlink" title="micro-batch"></a>micro-batch</h4><p><img src="http://image.slidesharecdn.com/apachestormvs-140811162542-phpapp01/95/apache-storm-vs-spark-streaming-11-638.jpg?cb=1425908462" alt></p><center>图3 micro-batch与流与批的关系</center><p>Wiki上并没有关于<code>micro-batch</code>的介绍，甚至在Streaming的paper中也没有提出这个名词，只能从网上别人总结的图来说明。<br>用这个概念能很好的总结Spark Streaming的执行流程，也能很直观的与类似Storm这类<code>Record By Record</code>类型的流系统区别开来。所谓的<code>micro-batch</code>，它的本质是批处理，因此Streaming的执行层是Spark——一个批处理系统：</p><blockquote><p>Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.</p></blockquote><p>因此，Streaming会天然继承Spark的优点：<em>高扩展性</em>、<em>高吞吐量</em>和<em>高可靠性</em>。<br>但是Streaming能够处理处理流式数据的原因在与它的batch相对于正常的Spark应用是非常小的，而且是严格按照时间切分批次。<br><strong>定义：</strong></p><blockquote><p>Streaming将流式数据分为多个<strong>固定时间窗口</strong>中的<strong>单批数据</strong>，并根据<strong>处理逻辑</strong>封装成Spark任务<strong>持续不断</strong>放入Spark中执行。  </p></blockquote><h4 id="JobGenerator"><a href="#JobGenerator" class="headerlink" title="JobGenerator"></a>JobGenerator</h4><p>Streaming需要提供按照<strong>固定时间</strong>产生的<strong>持续不断</strong>的Spark任务，因此在它的实现里，必然会有一个定时器，该定时器每隔<strong>固定时间</strong>生成一个处理该时间窗口数据的Spark任务，我们称这个组件为JobGenerator。<br>由于一些Streaming任务前后数据之间没有关联性，因此在JobGenerator中必须提供一个Spark的任务池，用于多线程的执行Spark的任务。</p><h4 id="Receiver"><a href="#Receiver" class="headerlink" title="Receiver"></a>Receiver</h4><p>Streaming还需要负责接收<strong>固定时间</strong>产生的流式数据，并将这种数据封装为JobGenerator产生Spark任务的输入数据，我们称之为Receiver。<br>由于<em>micro-batch</em>方式，Streaming可以同时处理批数据和流式数据，因此也会存在两种组织形式的Receiver。</p><ul><li>流式数据<br>以SocketReceiver为代表，通过单节点接受流式数据，将数据按批组装为任务输入数据，包括KafkaReceiver、FlumeReceiver等接收型数据。</li><li>批数据<br>以HDFS接口为代表，本身底层数据系统即是分布式数据，数据不需要组装，可以直接被RDD表征，在Streaming主要包括HDFS文件以及DirectKafkaAPI。</li></ul><h4 id="DStreamGraph"><a href="#DStreamGraph" class="headerlink" title="DStreamGraph"></a>DStreamGraph</h4><p>Streaming的API设计与<code>RDD</code>接口相似，RDD通过<code>dependencies_</code>存储自己的处理逻辑，并通过<code>DAGScheduler</code>分解出RDD整个Spark执行的逻辑，而相对应的<code>DStream</code>需要将自己的逻辑<em>翻译</em>为RDD原语，这个翻译过程被称为<code>DStreamGraph</code>。</p><p><img src="http://images.cnitblog.com/blog/287057/201404/231432212018837.jpg" alt></p><center>图4 DStream转化RDD</center><!-- API层设计 --><h4 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Iter</span></span><br><span class="line"><span class="type">Source</span>.fromFile(<span class="string">""</span>).getLines().flatMap(_.split(<span class="string">","</span>)).map(x =&gt; (x, <span class="number">1</span>)).foreach(println)</span><br><span class="line"><span class="comment">// RDD</span></span><br><span class="line"><span class="type">SpoarkContext</span>.textFile(<span class="string">""</span>).flatMap(_.split(<span class="string">","</span>)).map(x =&gt; (x, <span class="number">1</span>)).foreach(println)</span><br><span class="line"><span class="comment">// DStream</span></span><br><span class="line"><span class="type">StreamingContext</span>.textFileStream(<span class="string">""</span>).flatMap(_.split(<span class="string">","</span>)).map(x =&gt; (x, <span class="number">1</span>)).foreach(println)</span><br></pre></td></tr></table></figure><p>从上诉代码上看，DStream和RDD的接口都参考Scala的集合API设计，我们可以将迭代器理解为单机上表征数据以及数据转化方式的对象，那么从RDD的定义和实现来看，RDD是在分布式维度上表征数据及数据转化方式的对象。<br><strong>RDD定义：</strong></p><blockquote><p>Resilient Distributed Datasets (RDDs) are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.</p></blockquote><p>而DStream可以理解为在时间维度上表征分布式数据及数据转化方式的对象：</p><blockquote><p>A discretized stream or D-Stream groups together <strong>a series of RDDs</strong> and lets the user manipulate them to through various operators. D-Streams provide both stateless operators, such as map, which act independently on each time interval, and stateful operators, such as aggregation over a sliding window, which operate on multiple intervals and may produce intermediate RDDs as state.   </p></blockquote><p>当然，DStream除了上述和scala集合操作对应的API，DStream还包括一些流应用特有的操作，例如</p><ul><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#updatestatebykey-operation" target="_blank" rel="noopener">updateStateByKey</a></li><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations" target="_blank" rel="noopener">Window Operations</a></li><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#transform-operation" target="_blank" rel="noopener">transform operation</a></li></ul><h3 id="实现浅析"><a href="#实现浅析" class="headerlink" title="实现浅析"></a>实现浅析</h3><h3 id="流式语义"><a href="#流式语义" class="headerlink" title="流式语义"></a>流式语义</h3><p>Streaming的语义在<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#definitions" target="_blank" rel="noopener">官网</a>已经有所介绍，总结而言：</p><ul><li>数据接收阶段，对于批数据已经实现<code>Exactly once</code>语义，对于流式数据在Spark-1.2以后引入<code>WAL</code>技术，可以保证<code>at-least once</code>语义</li><li>数据转化阶段，依托RDD的可靠性保证，Streaming能保证<code>Exactly once</code>语义</li><li>数据输出阶段，默认的语义只为<code>at-least once</code>，需要用户自己实现<code>Exactly once</code>语义</li></ul><h3 id="简单案例"><a href="#简单案例" class="headerlink" title="简单案例"></a>简单案例</h3><h4 id="HdfsWordCount"><a href="#HdfsWordCount" class="headerlink" title="HdfsWordCount"></a>HdfsWordCount</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"HdfsWordCount"</span>)</span><br><span class="line"><span class="comment">// Create the context</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the FileInputDStream on the directory and use the</span></span><br><span class="line"><span class="comment">// stream to count words in new files created</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.textFileStream(args(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><h4 id="DirectKafkaWordCount"><a href="#DirectKafkaWordCount" class="headerlink" title="DirectKafkaWordCount"></a>DirectKafkaWordCount</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"DirectKafkaWordCount"</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create direct kafka stream with brokers and topics</span></span><br><span class="line"><span class="keyword">val</span> topicsSet = topics.split(<span class="string">","</span>).toSet</span><br><span class="line"><span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"metadata.broker.list"</span> -&gt; brokers)</span><br><span class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](</span><br><span class="line">  ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get the lines, split them into words, count the words and print</span></span><br><span class="line"><span class="keyword">val</span> lines = messages.map(_._2)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>L)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Start the computation</span></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><p>Streaming应用一般有以下步骤：</p><ol><li>根据不同数据源接口获取<code>InputDStream</code>，对于Kafka即<code>KafkaUtils.createDirectStream</code>，对于HDFS即：<code>ssc.textFileStream</code></li><li>通过<code>DStream</code>接口根据业务对<code>InputDStream</code>进行转化：<code>flatMap(_.split(&quot; &quot;)).map(x =&gt; (x, 1L)).reduceByKey(_ + _)</code></li><li><code>DStream</code>输出结果，<code>print</code>和<code>saveAsTextFiles</code>等，如果需要自定义的输出结果，可以使用<code>foreachRDD</code>算子</li><li>调用<code>ssc.start()</code>和<code>ssc.awaitTermination()</code>，启动Streaming的计算</li></ol><p><strong>任务提交</strong><br>使用常规的Spark任务提交应用，例如<code>HdfsWordCount</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/run-example streaming.HdfsWordCount /streaming</span><br></pre></td></tr></table></figure></p><p>启动任务后，可以在本地上传文本文件到HDFS指定目录下，这时在日志界面上就能看到统计出来的单词条数<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop fs -put textFie /streaming</span><br></pre></td></tr></table></figure></p><h3 id="实际案例"><a href="#实际案例" class="headerlink" title="实际案例"></a>实际案例</h3><p>公司案例</p><h3 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h3><ol><li>首先仔细浏览官网上<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">Streaming编程指南</a>，学习Streaming相关概念</li><li>通过官方<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming" target="_blank" rel="noopener">样例工程</a>熟悉API接口，并编写编写简单的应用，尝试在集群中运行。</li><li>学有余力的可以开始阅读Streaming源代码，并通过对比代码尝试定位运行过程中出现的问题。</li><li>学习<a href="http://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>相关概念与架构以及其他流式组件</li><li>学习<a href="http://storm.apache.org/" target="_blank" rel="noopener">其他流式</a>组件，集思广益</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol><li><a href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank" rel="noopener">Spark Paper</a></li><li><a href="https://people.csail.mit.edu/matei/papers/2012/hotcloud_spark_streaming.pdf" target="_blank" rel="noopener">Spark Streaming Paper</a></li><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">Streaming Programming Guide</a></li><li><a href="http://stanford.edu/~rezab/sparkclass/slides/td_streaming.pdf" target="_blank" rel="noopener">Indroduce By Tathagata</a></li><li><a href="http://www.csdn.net/article/2014-01-27/2818282-Spark-Streaming-big-data" target="_blank" rel="noopener">大规模流式数据处理的新贵</a></li><li><a href="http://www.slideshare.net/ptgoetz/apache-storm-vs-spark-streaming" target="_blank" rel="noopener">Storm与Spark Streaming比较</a></li><li><a href="http://www.cnblogs.com/shenh062326/p/3530092.html" target="_blank" rel="noopener">Spark Streaming实时计算框架介绍</a></li><li><a href="http://blog.csdn.net/anzhsoft/article/details/38168025" target="_blank" rel="noopener">从Storm和Spark 学习流式实时分布式计算的设计</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPARK </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
